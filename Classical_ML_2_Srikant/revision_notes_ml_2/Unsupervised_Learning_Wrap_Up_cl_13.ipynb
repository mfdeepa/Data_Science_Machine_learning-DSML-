{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on August 24, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/34674/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "Scenario based QnA sessions for following topics:\n",
        "1.   Unsupervised Learning.\n",
        "2.   High Dimensional Data Vizualizations.\n",
        "3.   Recommender Systems.\n",
        "4.   Time Series Analysis and Forecasting.\n",
        "\n"
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to revise topics?"
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To revise any topics in general, one can adopt the following strategy:\n",
        "- Understand '***How***' it works internally.\n",
        "- '***Why***' it works?\n",
        "- ***Pros*** and ***Cons***.\n",
        "<img src='https://drive.google.com/uc?id=1OKFy2La8s8QO2yv1EyEmOSMIdSex-BEO'>\n"
      ],
      "metadata": {
        "id": "6lClAUcPf6Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How it works Internally:***\n",
        "\n",
        "- Assume you are learning a concept for e.g. K-means. Mostly in interviews, candidates are asked to write code on how K-means work. Doing so, interviewers can understand if a candidate can code, handle boundary cases and knows the algorithm in question.\n",
        "- Knowing ***How*** will certainly help during in-depth ML rounds.\n"
      ],
      "metadata": {
        "id": "PLQHxkhxgdxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Why?:***\n",
        "\n",
        "Try asking yourself questions like:\n",
        "- Why $L1$ regularization creates sparsity?\n",
        "- How do you give $95\\%$ confidence interval on $\\hat{y}$ in Linear/Logistic Regression?\n",
        "- Why do you use Sigmoid in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "ign6N7iJhOsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pros and Cons:***\n",
        "- Knowing ***Cons*** will help to understand why a technique will not work and when not to use it."
      ],
      "metadata": {
        "id": "kJBFqzJzh2wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tips:***\n",
        "\n",
        "- Work out all maths on a piece of paper from first principles. (Feynman's Technique).\n",
        "- Explore StackOverflow/Quora/Wikipedia to get help in understanding Pros and Cons."
      ],
      "metadata": {
        "id": "TLqbbdPziHtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick recap of ideas:"
      ],
      "metadata": {
        "id": "Q0E2Xgv_ioTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=116PKNea-31eOq3fc_xAyoVR6SaZY6RMp'>\n"
      ],
      "metadata": {
        "id": "CDB6TVVPixD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1zA-blDM8VnvBUS1eqLBRMHCNqGStEd_s'>"
      ],
      "metadata": {
        "id": "-7U-kSlWjD6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scenarios:"
      ],
      "metadata": {
        "id": "diG9ifdQjT1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1:"
      ],
      "metadata": {
        "id": "YWOzydYRjXju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we have 10 million e-commerce products and we have to cluster them? Let the products be represented as d-dimensional item vector.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1LqL5QnZKK5lEq-Bq9cSHGBrYtHNZwRHj'>\n"
      ],
      "metadata": {
        "id": "sdBQsVNajT1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "LEZbT49ykAUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hierarchical clustering:\n",
        "  - Hierarchical clustering would make sense here because e-commerce have product hierarchies to begin with. This occurs naturally in e-commerce space.\n",
        "  - For e.g. Shirts could be categorized as T-shirts and Full-shirts.\n",
        "  - We won't have ground truth to begin with. But clustering will partially if not perfectly determine and create this sort of hierarchy.\n",
        "- Multi-level clustering based on Category:\n",
        "  - This would not work as we dont have categories to begin with.\n",
        "- K-means Clustering:\n",
        "  - K-means for clustering is computationally expensive.\n",
        "  - In K-means we need to decide on ***k***, which may not work in e-commerce.\n",
        "  - If ***k*** is less then we will have very high level of e-commerce products.\n",
        "  - If ***k*** is very high, then we will gets very low level of products but not the hierarchy.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "\n",
        "We need to think if we can make hierarchical clustering work for 10 Million e-commerce products with 40 dimensional vector representation of products.\n",
        "    "
      ],
      "metadata": {
        "id": "NgdZX5elkHzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How can we speed-up hierarchical clustering?\n",
        "    <img src='https://drive.google.com/uc?id=1R0iFeg7Foukfwz-Zse7HV8sSGbApemmd'>"
      ],
      "metadata": {
        "id": "6up3cB9YjT1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dimensionality reduction.\n",
        "  - $d$ is not the problem here. $N$ is.\n",
        "- Sampling:\n",
        "  - We sample $n'$ products such that $n' << N$ and do hierarchical clustering on $n'$ products.\n",
        "  - There are $n - n'$ products remaining. We can use K-NN to determine which category do these $n - n'$ products belong.\n",
        "  - Let $P_j$ and $P_k$ be 2 datapoints from sampled space. If we find $P_i \\sim P_j$, where $p_j \\in n'-n$ space, then we assign $P_j$ to cluster that holds $P_j$ and $P_k$.\n",
        "- Multi-core Processing:\n",
        " - In multi-core environment, we can run hierarchical clustering on separate core and speed-up the process."
      ],
      "metadata": {
        "id": "AhZBh7jGqtuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2:"
      ],
      "metadata": {
        "id": "tRmBdONYtx8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are trying to fing k-Nearest Neighbours and you want to build Faster KNN. Can you use some of the techniques from clustering to facilitate this?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10FPe-o-umZeardzGPP5Q-9WZTKdMTqBR'>\n"
      ],
      "metadata": {
        "id": "oFOZ0jCetx80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "j9jkvST2tx81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DBSCAN structure $R^* tree$ might be helpful:\n",
        "  - Issue is DBSCAN is density based amd not nearest based.\n",
        "- Locality Sensitive Hashing:\n",
        "  - Could be used here.\n",
        "- Divisive or Hierachical Clustering:\n",
        "  - Suppose we already have a Pre-cluster computed on disc and we have a querypoint $x_q$. Then how to we decide which cluster it would belongs to?\n",
        "- We can have Centroids for each clusters and then may be use Cosine Similarity.\n",
        "  - Here we compare centroids for each cluster with querypoint $x_q$.\n",
        "  - If a querypoint is closer to a cluster then we go down that path in tree.\n",
        "  - This has $log(N)$ time complexity as depth of tree is $log(N)$.\n",
        "  - So we keep going down the tree till we have cluster of size $k$ in our Faster k-NN algorithm.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "\n",
        "- So here, we are using K-NN as an application to solve hierarchal clustering.\n",
        "- There is a problem with this approach as well:\n",
        "  - The resulting binary tree becomes highly unbalanced - as some products have few/no subcategories and some might have too many subcategories.\n",
        "  \n",
        "    <img src='https://drive.google.com/uc?id=1DupF0toi2aWa9qHVJ6JvBQbP-_82iNxl'>\n",
        "  - Time complexity to traverse a tree would still be $log_b(N)$. But here base $b$ would change and have significant impact.\n",
        "  - There are some advanced techniques where we can put a constraint that the tree should be fairly balanced.\n"
      ],
      "metadata": {
        "id": "kbT1ZW2itx82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3:"
      ],
      "metadata": {
        "id": "V4oXeP90ygF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let say we have $100k$ resumes. What kind of clustering techniques should we use to cluster them?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ShqF4hIif1wTFPA7jw51d_nuwmgv56BY'>\n",
        "\n"
      ],
      "metadata": {
        "id": "NVcWVJWsygF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "MSXODX22ygF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let resumes be represented as BoW and then do clustering:\n",
        "  - If BoW is used, then dimensionality becomes very large. Hierachical clustering could be used on this, but is there a better technique?\n",
        "- Matrix Factorization and clustering:\n",
        "  - Word-document matrix could be decomposed and then document representation can be used to compute similarity.\n",
        "  - MF gives rich representations of each document.\n",
        "  - We can take this document representations and then do hierarchical clustering.\n",
        "- Use Non-Negative Matrix Factorization (NMF):\n",
        "  - NMF as it would do Matrix Factorization and does clustering as well.\n",
        "\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "\n",
        "NMF is a better strategy."
      ],
      "metadata": {
        "id": "UbgTNjcYygF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 4:"
      ],
      "metadata": {
        "id": "R3d3So_S03Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have latitudes and longitudes of Source and Destinations of all deliveries for a fast delivery company like Zepto. Company decides to place dark stores in the region.\n",
        "\n",
        "We want to cluster destinations so that we can decide where to place the stores. How do we do it?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1dxbaDKm77uKXthO3q-3-NhEJBkmux0fH'>\n",
        "\n"
      ],
      "metadata": {
        "id": "VHfV2wjG03Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "ckOPQGhy03Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DBSCAN:\n",
        "  - DBSCAN has 2 parameters $(\\epsilon, minpts)$. DBSCAN is sensitive to density.\n",
        "  - Some regions might have higher density but for upcoming areas/localities density would be less.\n",
        "  - DBSCAN will have issues where there is non-uniform density.\n",
        "- K-means to cluster and we can place dark stores at centroids:\n",
        "  - This could work very well when dimensionality is small.\n",
        "  - K-means creates approximately equal sized clusters. This would help in a way such that rather than placing one dark store in a big cluster, it will break down a big cluster into uniformly size cluster and allowing us to place one dark store per cluster.\n",
        "- Use of Gaussion Mixture models:\n",
        "  - Since it's low dimensional data (d=2) can we check if it's following normal or multi-modal distribution.\n",
        "  - If yes. we use GMM as it's high interpretable and dark stores can be placed in mean/median place.\n",
        "\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "\n",
        "- K-means has certain problems as well:\n",
        "  - Size is same but the distance can we challenge. For e.g. if distance is large traffic in a bigger cluster would be less. In a densely packed cluster, traffic would also be large.\n",
        "- Traffic needs to be considered.\n",
        "- Duration of the day when orders peak."
      ],
      "metadata": {
        "id": "BZn8o0Xo03Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5:"
      ],
      "metadata": {
        "id": "1QBqNM4At39d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Depth round:\n",
        "- Candidates are asked to pick a technique and write pseudo code for this.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=18tJor9ZOlBs8apJM4AdP7BBJU3gJA0_Y'>\n",
        "- For e.g. given a function $f(x)$, implement $Integral(f,a,b)$:\n",
        "$$\\int_{-10}^{10} \\ f(x)$$\n",
        "- Here we would have to write code from stratch. Cannot use integral function from Scipy.\n",
        "- Objective is to understand how things work under the hood.\n",
        "\n"
      ],
      "metadata": {
        "id": "aXap9tuit397"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Would you suggest choosing a simple method like linear regression in interview depth round?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "It would be tricker to do that as this topic has much more depth. For e.g. interviewer would ask:\n",
        "- Why Squared loss is used?\n",
        "- How can you give $95\\%$ Confidence Interval on $\\hat{y}$.\n",
        "- $95\\%$ Confidence Interval on each of the weights.\n",
        "- Why Sigmoid?\n",
        "- Why does $L1$ regularization creates more sparsity than $L2$.\n",
        "- Can you build a Linear regression model on a dataset with 1000 features with latency < $1ms$.\n",
        "- Write pseudo code for Stochastic Gradient Descent.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1_E_1sHj_VKkot6vtcyndvPbG9UFaEw-y'>"
      ],
      "metadata": {
        "id": "abtl3KojTu8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 6:"
      ],
      "metadata": {
        "id": "WPRdiJSDVcOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does t-SNE not preserve global structure?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1QkrswfKGA_7B_QfO7n3-Y0OvDKIl4lsY'>\n"
      ],
      "metadata": {
        "id": "hIoRkHXMZ3JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "sKFXx4A_VcOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Crowding problem.\n",
        "- Stochasticity: Different runs giving different solutions.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "\n",
        "- Above options are correct to an extent. But any other reasons?\n",
        "\n",
        "*Hint:* Objective function in optimization problem."
      ],
      "metadata": {
        "id": "4BfUKcgjVcOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- t-SNE uses $KL Divergence(P,Q)$. There are 2 set of probabilities for every datapoint.\n",
        "- $P_{ij} \\to$ probability that $x_i$ and $x_j$ are neighbours in $d$ dimensions. It is computed using Gaussian Distribution.\n",
        "- $Q_{ij} \\to$ probability that $x_i$ and $x_j$ are neighbours in $d'$ dimensions. It is computed using t-distribution.\n",
        "- $x_i's \\to$ higher dimensions and $y_i's \\to$ lower dimensions.\n",
        "- As distance between points increases, probability value falls."
      ],
      "metadata": {
        "id": "dI9gW7VwWnV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- t-SNE uses $KL Divergence(P,Q)$. There are 2 set of probabilities for every datapoint.\n",
        "- $P_{ij} \\to$ probability that $x_i$ and $x_j$ are neighbours in $d$ dimensions. It is computed using Gaussian Distribution.\n",
        "- $Q_{ij} \\to$ probability that $x_i$ and $x_j$ are neighbours in $d'$ dimensions. It is computed using t-distribution.\n",
        "- $x_i's \\to$ higher dimensions and $y_i's \\to$ lower dimensions.\n",
        "- As distance between points increases, probability value falls.\n",
        "  <img src='https://drive.google.com/uc?id=1RTXhVjtKga63LjSR-p76Gz4v7mW038TT'>\n",
        "- For e.g. if the distance is more than 3 standard deviation, then probability value is smaller and the point is pushed much much farther away. Because it uses t-distribution, this distance of slightly farther away points is no longer preserved."
      ],
      "metadata": {
        "id": "AFBzrKtRVcOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 7:"
      ],
      "metadata": {
        "id": "OsKbVx-iZqdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you are working for Amazon.in and we have Times Series of order placed per minute. Which forecasting techniques to use and why?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1CXy3zTwEF9TmBxM_z2sqcRx5wu-xd2FX'>\n"
      ],
      "metadata": {
        "id": "MWg_mfKIafo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "SjdHHQS2Zqdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Amazon sales are quite affected by festivals and product launches, so exogenous variables are important to consider.\n",
        "- There will be a daily pattern of orders considering the fact that customers would be placing order in the morning or after office hours.\n",
        "- $y_t$ will be very close to $y_{t-1}$.\n",
        "- We would need seasonality. We could have 24 hours seasonality, as we have a daily pattern. Infact there are multiple seasonalities (weekly, daily, festive, holidays).\n",
        "- We could use SARIMAX model.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "- Use of SARIMAX makes sense. But why SARIMAX? We could just use SARMAX. Why the integrating part?\n",
        "\n",
        "Integrating part is used to de-trend the data. Since we have seen a growing trend at Amazon for the past 8 years, we want the trend to be accounted for.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "- But data still have multiple seasonality. How can you account for this?\n",
        "\n",
        "We can use FB-Prophet.\n"
      ],
      "metadata": {
        "id": "ONo31knDZqdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 8:"
      ],
      "metadata": {
        "id": "mOR0KDgyesdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we an implementation of SARIMAX and we want to add multiple quadratic features additionally. How can we do this?\n",
        "<img src='https://drive.google.com/uc?id=1hz9NzGutZj7E92tWxt9Uauwz_GEF3FmV'>\n"
      ],
      "metadata": {
        "id": "zUuxKlc3esdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "c5Yan9QWesdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can create the new quadratic feature and pass it as exogenous feature to the model.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "- Yes, this can work.\n"
      ],
      "metadata": {
        "id": "VxOIKySresdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 9:"
      ],
      "metadata": {
        "id": "gmpHyMU4fdar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement pseudo code of PACF:\n",
        "<img src='https://drive.google.com/uc?id=1XmyMxKXts4DpLDeWztRwgfrF3FOomr5N'>\n"
      ],
      "metadata": {
        "id": "kG3wSkiMfdbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "hOwRBSPtfdbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suppose we want to compute Auto correlation between $y_t$ and $y_{t-2}$.\n",
        "- To begin with, we need to predicts $y_t$ using $y_{t-1}$. $f_{AR}(y_{t-1}) \\to y_t$.\n",
        "- It could be a simple AR model. We would compute residual $\\delta_{t}$ as $y_t - f_{AR}(y_{t-1})$.\n",
        "- Then we could find out simple correlation coefficient between this residual and $y_{t-2}$.\n",
        "- Here we are trying to answer what is the correlation between original time step $y_{t-1}$ and Time Series shifted by 2 units, while we have accounted for Time series in the previous step i.e. $y_{t-1}$.\n",
        "\n",
        "***Discussion Outcome:***    \n",
        "How do we conclude that it is accounted for? We build a simple AR model for this."
      ],
      "metadata": {
        "id": "FbUpLyGdfdbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Key Lessons:"
      ],
      "metadata": {
        "id": "karJTYzat6CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Understand the advantages and dis-advantages of each technique.\n",
        "- Each technique works well under some constraints and has specific properties that facilitates its working.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1bOTIVN6tNFL4FzrH3J_K2D6mclMkRKeJ'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Pk2m7RgyuR8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d//view?usp=sharing"
      ],
      "metadata": {
        "id": "SbQKFJTAt8-r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCldZBJk03Dt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}