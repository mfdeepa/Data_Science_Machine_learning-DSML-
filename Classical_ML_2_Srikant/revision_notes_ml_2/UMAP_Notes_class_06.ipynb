{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on August 5, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/32970/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "1.   t-SNE practical aspects.\n",
        "2.   UMAP: Algorithmic and optimization perspective + practical aspects.\n",
        "3.   Content Based Recommender Systems.\n",
        "4.   Collaborative Based Recommender Systems.\n",
        "5.   Matrix Factorization."
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE practical aspects:\n",
        "\n"
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- T-SNE has a much better separation than PCA. If we don't have labels, then we can draw better insights or have better understanding about the data. For e.g. humans tend to write 1's and 9's in similar fashion. Same would be the case with 1's and 2's.\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1PhbR5gFVHEiUVkmUtrNQv38TaHs2MsSr'>\n",
        "\n",
        "- t-SNE and UMAP are widely used to vizualise high dimensional data.\n",
        "- It solves an optimization problem to minimise KL_div(P,Q) of probability distributions of P and Q.\n",
        "- It tries to push dis-similar points away from each other and similar points into one region.\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1B1bAW659j-rrrSyV5nYfPICYIo_ncWin'>\n",
        "\n",
        "- *Source:* https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/animation-94a2c1ff.gif"
      ],
      "metadata": {
        "id": "_lOIuKdyWmfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to use T-SNE effectively.\n",
        "\n",
        "- The most important parameter in t-SNE is perplexity. So, what does it mean?\n",
        "- Perplexity is approximately equal to number of neighbors whose distance we want to preserve.\n",
        "- As perplexity $\\big\\uparrow$, we are trying to preserve more of the global shape. When perplexity $\\big\\downarrow$, we are trying to preserve local structure."
      ],
      "metadata": {
        "id": "-XvqLg50lSuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question:*** Why can't we always preserve global structure?\n",
        "\n",
        "*Answer:* Due to crowding problem. It is not always possible to preserve the distance in all neighborhood(N)"
      ],
      "metadata": {
        "id": "uSLUvwNd5CNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to read/interpret output of t-SNE:\n",
        "\n",
        "- Hyperparameters really matter:\n",
        "    - As perplexity increases to very high value, then there is no separation. (Crowding problem kicks in).\n",
        "    - Try different values of perplexity and then draw conclusions.\n",
        "    -*Note:* avoid perplexity = n (# datapoints)\n",
        "<img src='https://drive.google.com/uc?id=1d6AWGtESvw-yRmRESCej9GcStbERE2CX'>\n",
        "\n",
        "\n",
        "- Cluster sizes in a t-SNE plot mean nothing:\n",
        "    - Size of clusters in original data and those in vizualizations may not be of same size.\n",
        "    <img src='https://drive.google.com/uc?id=1pt_xw4379p0NY40LoEz79nXDHnAln0ky'>\n",
        "\n",
        "- Distances between clusters might not mean anything:\n",
        "    - Distances between clusters in original data and vizualization may not be preserved.\n",
        "    <img src='https://drive.google.com/uc?id=1rDY-KTP7z9dRgMQjtovp0LRBbG6w7ZN4'>\n",
        "\n",
        "- Random noise may not always look random:\n",
        "    - Randomness in original data and vizualization may not always be same. At times, vizualization may look ot have some pattern.\n",
        "    <img src='https://drive.google.com/uc?id=1RKxCo8aFII8nkZKIuFlvektemDm5541A'>\n",
        "\n",
        "- Sometimes, you can see shapes that makes sense:\n",
        "    - Very rarely the original data is perfectly symmetric. For high enough perplexity values, the shapes are well preserved.\n",
        "    <img src='https://drive.google.com/uc?id=1-OsRS4ZdvzT0ZAg0XWzescnq8L6VWktF'>\n",
        "\n",
        "- For topology, you may need more than one plot:\n",
        "    - At times, you can read topological information from a t-SNE plot. But we need to try it for multiple values of perplexity.\n",
        "    <img src='https://drive.google.com/uc?id=1HsTExTkJwTMO2fWJSEcK89o7aBJqm8Nz'>\n",
        "\n",
        "Source: https://distill.pub/2016/misread-tsne/\n",
        "\n",
        "***Further Reads:***\n",
        "- There is a very good blog of distill that has explained t-SNE very well without diving deep into mathematics part of it.\n",
        "https://distill.pub/2016/misread-tsne/"
      ],
      "metadata": {
        "id": "vYm87Fn25wQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question:*** Where do we use t-SNE in a real world scenario?\n",
        "\n",
        "*Answer:*\n",
        "- Let's say we have dataset: ${D}$ = {$({x_i}, {y_i})_{i=1}^n$} and a classification model that gives us predictions ${\\hat{y_i}}$ with a loss function (${L_i}$) = diff(${y_i}$, ${\\hat{y_i}}$)\n",
        "    \n",
        "    <img src='https://drive.google.com/uc?id=1nEeRTADr9ubl3JFFdlajYk5FcrzSv55J'>\n",
        "\n",
        "- Let's say we project the data ${x_i}\\in \\mathbb{R}^d$ to ${x_i}\\in \\mathbb{R}^2$ and color code these points in the plot with red for higher loss.\n",
        "    \n",
        "    <img src='https://drive.google.com/uc?id=1liG3tIDOOiq1LWQTrzRwxynwBn7fZBIm'>\n",
        "- We may see bunch of points that are grouped together and have very high loss. These points could be investigated further and we could generate new features"
      ],
      "metadata": {
        "id": "tEv552t2SNQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UMAP: Uniform Manifold Approximation and Projection"
      ],
      "metadata": {
        "id": "YTxzQyFNWdSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Underlying Mathematical assumption:**\n",
        "\n",
        "- Mathematics used in UMAP comes from algebriac topology (study of shapes). Manifolds are like shapes in complex high dimensional space.\n",
        "\n",
        "- In the high dimensional space, there exists a manifold(topological structure) or a surface/shape on which all our points lie uniformly.\n"
      ],
      "metadata": {
        "id": "KgeGqZLvXDHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Aspects**\n",
        "\n",
        "1.   Find nearest neighbors.\n",
        "    - For every points, finds its nearest neighbors.\n",
        "    - Let n_neighbors = 5. Then for every point, it looks at 5 nearest neighbors.\n",
        "    \n",
        "    <img src='https://drive.google.com/uc?id=1N_GnkMGET8bI9sZuS_C3WuIlTF0jFsuG'>\n",
        "\n",
        "2.   Build a Graph: (${G^d}$ in d-dimensional space)\n",
        "    - Build a graph such that, each edge will be assigned a weight based on the distance.\n",
        "    - Closer the points, higher the weights i.e ${w_{ij}} \\propto \\frac{1}{dist({x_i},{x_j})}$\n",
        "    - Hence, it is also referred to as weighted graph.\n",
        "\n",
        "3.   For each ${x_i}$ (d-dimensions) $\\xrightarrow{Project}$ ${y_i}$ (2-dimensions) such that ${G^d}$(graph in d-dimensional space) $ \\approx {G^2}$(graph in 2-dimensional space)\n",
        "\n"
      ],
      "metadata": {
        "id": "nBpXmE5HXuwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization in UMAP:**\n",
        "- Weights in high dimensions ${W_h}(e)$ and low dimensions ${W_l}(e)$ are kept close.\n",
        "<img src='https://drive.google.com/uc?id=13faB1QDYXnDXFnoN0PC7afQ2Ka5MPKNl'>\n",
        "- If attractive force is large, then the 1st component in the above figure matters. It ensures that edge weights in high dimensions and low dimensions are very close to each other.\n",
        "- If the high dimensional weight is small, then we still want high dimensional weight and low dimensional weight to be close to each other. This is ensured by the 2nd component in the above figure.\n",
        "- Thus UMAP tries to preserve the global structure. This is the core objective and it does much better than t-SNE.\n",
        "- UMAP is much faster than T-SNE as:\n",
        "    - Global structure is preserved.\n",
        "    - Speed is faster.\n"
      ],
      "metadata": {
        "id": "Qghqt9pAddNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental parameters of UMAP**\n",
        "- n_neighbors:\n",
        "    - As n_neighbors increases, global structure is preserved.\n",
        "- min_dist:\n",
        "    - It is the desired distance between closest pairs of points in d-dimensions to be preserved in vizualization.\n",
        "    - For e.g. if min_dist = 0.25, then it implies that closest point in d-dimension are to be placed at distance of 0.25 in d-dimension.\n",
        "- target dimension:\n",
        "    - Dimension for vizualization.\n",
        "\n",
        "**Wooly Mammoth:**\n",
        "    <img src='https://drive.google.com/uc?id=1iAkIKysGx6j-WxX1bhoYdDmEIdm8ySMb'>\n",
        "- There is an interesting article by folks working at Google PAIR. It explains with a example of how parameters influence vizualizations.\n",
        "https://pair-code.github.io/understanding-umap/"
      ],
      "metadata": {
        "id": "xpOjT6qhk69D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to (mis)read UMAP:**\n",
        "\n",
        "- Hyperparameters really matter:\n",
        "    - Advantage of UMAP is its speed. So we can try variety of hyperparameters to get a  sense of how the projection is affected by its parameters.\n",
        "\n",
        "- Cluster sizes in a UMAP plot mean nothing:\n",
        "    - Size of clusters in original data and those in vizualizations may not be of same size.\n",
        "    \n",
        "- Distances between clusters might not mean anything:\n",
        "    - Distances between clusters in original data and vizualization may not be preserved.\n",
        "    \n",
        "- Random noise may not always look random:\n",
        "    - Randomness in original data and vizualization may not always be same. At times, vizualization may look ot have some pattern.\n",
        "   \n",
        "- You may need more than one plot:\n",
        "    - UMAP algorithm is based on Stochastic approach. Hence, different runs with same hyperparameters may give different results."
      ],
      "metadata": {
        "id": "IsktS5rUmCsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key-takeaways for high dimensional data vizualizations"
      ],
      "metadata": {
        "id": "ZhQQ-D75Y2aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If you want to vizualize high dimensional data, go to UMAP.\n",
        "- Becareful and do not over-read output.\n",
        "- Based on structures that we see, comeup with good ideas.\n",
        "- Verify these ideas with proofs."
      ],
      "metadata": {
        "id": "ccNWy3LmY_gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare t-SNE Vs UMAP"
      ],
      "metadata": {
        "id": "LaZ6hu9SbBR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- t-SNE tries to preserve large distances, whereas UMAP preserves small and large distances as best as possible.\n",
        "- In t-SNE $\\forall_{ij}$ we compute $dist({x_i},{x_j})$. In UMAP, we compute $dist({x_i},{x_j})$ only for ${x_j} \\in N({x_i})$."
      ],
      "metadata": {
        "id": "PsUIwaNDbJ3q"
      }
    }
  ]
}