{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Time-Series:"
      ],
      "metadata": {
        "id": "cPAjRbzTly-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concept of Time-Series:"
      ],
      "metadata": {
        "id": "znIjxPL0SG9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's plot number of items sold on Amazon over time.\n",
        "- Here $Y axis \\to$ # items sold and $X axis \\to$ time which could be weeks/months/years or even in the granular form like minutes/hours.\n",
        "<img src='https://drive.google.com/uc?id=1cDRfNQ6JuN-ug-KItoTSycozZ0ZUTf_5'>\n",
        "- The pattern could be repeating over time. It could hourly, weekly, seasonal, weather or yearly pattern.\n",
        "<img src='https://drive.google.com/uc?id=1-Y9SDKDMzbK2RIk_Jsii-aEIYMg0Dp9K'>\n"
      ],
      "metadata": {
        "id": "ORYL8UhOl8j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forecasting:"
      ],
      "metadata": {
        "id": "rSi9nHbC2ZOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most interesting problem in Time-Series is Forecasting.\n"
      ],
      "metadata": {
        "id": "gSzmedLp2e13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Task:"
      ],
      "metadata": {
        "id": "RykS1r6D6QbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given historical data $x_1, x_2, .. x_{t-1}, x_t$ of # items sold at Amazon till time ($t$), predict $x_{t+1}, \\ x_{t+2},..$\n",
        "<img src='https://drive.google.com/uc?id=1IZRPQbWYUaj0vQE9zMJuZ3kLuFOXARfC'>\n",
        "\n",
        "So, if we are given historical hourly sales data for Amazon, can we predict how many sales will happen in the future.\n"
      ],
      "metadata": {
        "id": "9QpQG2I06KRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How do we predict future data using historical data?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1iH-mXVEvUhOHguTQfoqZT2yCRxXcvQFb'>"
      ],
      "metadata": {
        "id": "w0xLRAD8sPqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Approaches:"
      ],
      "metadata": {
        "id": "bf2Fc3Wr54Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Average of last 3 months:"
      ],
      "metadata": {
        "id": "9Ac2fmzQ6oBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This may not work. For e.g. if we compute $x_{t+1}$ as last 3 month's average, then it will fall sharply.\n",
        "<img src='https://drive.google.com/uc?id=15dmpPRI8ywn0VRT51j6M1xdpW6FBN0mt'>\n"
      ],
      "metadata": {
        "id": "0WisIPsd5_m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Regression/Curve Fitting:"
      ],
      "metadata": {
        "id": "7nrdLPkC7EL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This could be a valid approach.\n",
        "- We can have features like: Weather, Holiday, time(hrs), Day of Week (DOW), Month of the year (MOY) and target as Sales per hour.\n",
        "<img src='https://drive.google.com/uc?id=1ev9oKYwlh_GmTlqKtsKAcvvnBndUXNW6'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxbkudCC7EL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Flaw:***\n",
        "- However, there is a flaw with this approach. It may not capture trend over years.\n",
        "- For e.g. In a growing startup, sales on a Chilly Saturday in October during holiday season might be increasing for the same criteria over the years.\n",
        "<img src='https://drive.google.com/uc?id=1iZSmjlNPGUV_M_m7YTVgE7RaAyZ_rukc'>\n",
        "\n",
        "***Solution:***\n",
        "- Create a feature $\\to$ Year of Operation. Let say, the company was started in 2012. Then feature values will be: 0 $\\to$ 2012, 1 $\\to$ 2013, 2 $\\to$ 2014 and so on."
      ],
      "metadata": {
        "id": "T83WvldQ8O9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Weighted Average:"
      ],
      "metadata": {
        "id": "tFy4-Xx19M6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In weighted moving average, we take weighted average of last k observations.\n",
        "<img src='https://drive.google.com/uc?id=1lrQRk6RWt7T0RzWcBVNjQQORz9h34xJ7'>\n",
        "\n",
        "- $\\hat{x}_{t+1}$ is computed as $\\alpha_0x_t + \\alpha_1x_{t_1} + \\alpha_2x_{t_2} + ... + \\alpha_kx_{t-k}$ divided by $\\alpha_0 + \\alpha_1 + \\alpha_2 + .. + \\alpha_k$.\n",
        "- Mathematically, it can be written as:\n",
        "$$\\hat{x}_{t+1} = \\frac{\\overset{k}{\\underset{i=0}{\\Sigma}} \\ \\alpha_i * x_{t-i}}{\\overset{k}{\\underset{i=0}{\\Sigma}} \\alpha_i}$$\n",
        "<img src='https://drive.google.com/uc?id=1gXZN13RzH5H-wSIVnHXXY05189MZDZD9'>\n",
        "- Note, we are using observations in the window of $k$ and for each of them we give weights. Most recent observations have higher weights and lesser weights to older observations.\n",
        "- So, $\\alpha_0 > \\alpha_1 > \\alpha_2 > .. > \\alpha_k$.\n",
        "- Using data from $x_{t-k} ... x_t$ we can predic $\\hat{x}_{t+1}$. Similarly, using $x_{t-k+1} ... \\hat{x}_{t+1}$ we can predict $\\hat{x}_{t+2}$.\n",
        "- Thus, we can keep predicting into future.\n"
      ],
      "metadata": {
        "id": "5OkHIekb9M6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weighted moving average (WMA) is a type of regression with manual weights.\n",
        "<img src='https://drive.google.com/uc?id=1ioOsYvo6hO8NkXhlxz57RjGGlHacIHou'>\n",
        "- We can also write WMA with computed weights i.e. we estimate these weights: $\\alpha_i$.\n",
        "\n"
      ],
      "metadata": {
        "id": "iF2083CZIWs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train/Test Split in Time-Series:"
      ],
      "metadata": {
        "id": "I22EnILCf9Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How do we split train/test data? Till now, we used random train-test split. Will this approach work in Time-Series as well?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- No, random sampling will fail in Time-Series.\n",
        "    <img src='https://drive.google.com/uc?id=1k8QmK7KTQiapezyOYKpdgTC-i4BJHqbf'>\n",
        "- It might happen than training data (during random) split might contain future data and the test sample contains older data. So, training data has already seen the future and model may give excellent predictions that are too good to be true.\n",
        "\n",
        "***Solution:***\n",
        "\n",
        "- We sequentially break the data into Train, Validation and Test split.\n",
        "    <img src='https://drive.google.com/uc?id=1T14QoZsKHEFlNjNxdBjWxaSeiIUxNn9O'>\n",
        "\n"
      ],
      "metadata": {
        "id": "cMEFShUmOXlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resampling:"
      ],
      "metadata": {
        "id": "Np8kzrmUQIiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hourly $\\to$ Daily:"
      ],
      "metadata": {
        "id": "g63a-DscQRai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are given Hourly time-series data, then how do we convert it to daily time-series?\n",
        "\n",
        "***Answer:***\n",
        "- We sum-up the data in 24 hours window. So now 1 point is 1 day.\n",
        "<img src='https://drive.google.com/uc?id=1z4CO-Lc_qxHya-VoZE1XaO4Z5q5i1-NU'>\n"
      ],
      "metadata": {
        "id": "c5WOK2llQIit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hourly $\\to$ Minutely:"
      ],
      "metadata": {
        "id": "pUM719qcRVeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we are given Hourly time-series data, then how do we get a granular time-series?\n",
        "\n",
        "***Answer:***\n",
        "- Assuming a uniform spread, we divide hourly data by 60 and assign it to each minutes in an hour.\n",
        "<img src='https://drive.google.com/uc?id=1Uwgo37_FfNYxW-xhLrFQTNkkf75tg-Fr'>\n"
      ],
      "metadata": {
        "id": "MHmVTrz0RVeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing values treatment:"
      ],
      "metadata": {
        "id": "GInVt2rjSV0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we having some missing data. Then, how do we impute?\n",
        "\n",
        "***Possible Approaches:***\n",
        "- Drop values? We cannot drop missing values in time-series data.\n",
        "- One option could be to calculate missing value from previous data point i.e. $x_t = x_{t-1}$.\n",
        "- We could calculate the missing value as average of previous and next data point i.e.\n",
        "$$x_t = \\frac{x_{t-1} + x_{t+1}}{2}$$\n",
        "<img src='https://drive.google.com/uc?id=1fLNlowuK8RMgnUStTXUJYYmvBbeA-bFU'>\n",
        "- There is another option known as Centered Moving Average (CMA).\n",
        "<img src='https://drive.google.com/uc?id=1_m71-rte8O09QlDxW7aRRugWTPygGx16'>\n",
        "- This CMA is a generalization of looking back and forward for missing value imputation.\n",
        "- Sometimes, people also use a concept called Interpolation.\n",
        "<img src='https://drive.google.com/uc?id=11a-4m9xpdnG-Of2G8E7dmX0EN0ZsgW-O'>\n",
        "- Imagine we have 2 observations $t-1$ and $t+1$ with $t^{th}$ observation missing. Then:\n",
        "$$x_t = \\frac{x_{t-1} + x_{t+1}}{2}$$\n",
        "- This is could be thought of CMA with $m = 1$."
      ],
      "metadata": {
        "id": "AMJpO2dXT1ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outliers/Anamolies:"
      ],
      "metadata": {
        "id": "4BQB-BqwVi-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the below graph, you can see there are sudden spikes or clips in the graph for # items sold on hourly basis.\n",
        "<img src='https://drive.google.com/uc?id=1WdFEij9hNQRjDFF66UINQ5AGXnGtiajV'>\n",
        "- A sudden spike could be because of celebrities endorsing a product that leads to buying surge whereas a sudden clip could be due to servers that are down/website not working correctly. This results in loss of revenue.\n",
        "\n",
        "So how to be find such sudden changes? Let's discuss some of the approaches:"
      ],
      "metadata": {
        "id": "z-Ov7PoQVi-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Percentage Change:"
      ],
      "metadata": {
        "id": "GFmulsH7W9rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Between every $x_t$ and $x_{t+1}$, we get a percentage change as follows:\n",
        "$$ \\frac{x_t - x_{t-1}}{x_{t-1}} = \\Delta_t$$\n",
        "<img src='https://drive.google.com/uc?id=1phnlnQvud2FhRoa9xhLgHP0-UGa6N_iN'>\n",
        "- For each $\\Delta_1,\\Delta_2,\\Delta_3,...,\\Delta_t$, we compute IQR."
      ],
      "metadata": {
        "id": "fFRTJZ5lXDK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Isolation Forest/ OneClass SVM/ DBSCAN:"
      ],
      "metadata": {
        "id": "2rWt-15-YCZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- iForest/OneClass SVM/DBSCAN works on tabular data.\n",
        "<img src='https://drive.google.com/uc?id=1BCe9bar5Ta9ncBDYtb9JkZnZKYTC-Kbt'>\n",
        "- Note that Time-Series data is sequential data. Hence, the above approaches would not work here."
      ],
      "metadata": {
        "id": "asby2jAiYCZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Regression:"
      ],
      "metadata": {
        "id": "lehOu9CMYveJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For every time $t$, we will predict $\\hat{x}_{t+1}$ using Regression/WMA.\n",
        "<img src='https://drive.google.com/uc?id=1PdAvdJh-4DYgGcQnF5sTlKZPCZeQQlGR'>\n",
        "- Now we will have observed $x_{t+1}$ and predicted values $\\hat{x}_{t+1}$.\n",
        "<img src='https://drive.google.com/uc?id=1YgSCEKz6r5QCZgGsr1zzX9IqvkWZpJeu'>\n",
        "- If ratio of ($x_{t+1}$ - $\\hat{x}_{t+1}$) and $\\hat{x}_{t+1}$ is very high or very low (a threshold is set for this), then we consider this point as outlier.\n",
        "\n",
        "***Assumptions:***\n",
        "\n",
        "- Historical data has no outliers.\n",
        "- There is some pattern in the data.\n",
        "- If we identify a point as outlier, then we don't use $x_{t+1}$ for future calculations but use $\\hat{x}_{t+1}$ for further predictions.\n",
        "- This works very good as long as we keep on removing historical outliers.\n"
      ],
      "metadata": {
        "id": "9Ih2GMgZYveS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is DL approach or classical ARIMA better for time series?\n",
        "\n",
        "***Answer:***\n",
        "- DL-based approach like LSTM /GRU's are much powerful in comparison to Classical Time-Series models like ARIMA.\n",
        "<img src='https://drive.google.com/uc?id=1xyaOluZrJnYWadTCv_3kQU5qPkTpDcUj'>\n",
        "- If the dataset is simple, ARIMA models are preferred as they are computationally less complex."
      ],
      "metadata": {
        "id": "Wkuei6knng-L"
      }
    }
  ]
}