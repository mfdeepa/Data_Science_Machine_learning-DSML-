{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on August 26, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/35891/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "Scenario based QnA sessions for following topics:\n",
        "1.   Linear Regression.\n",
        "2.   Logistic Regression.\n",
        "3.   Decision Trees, Random Forests, Gradient Boost Decision Trees.\n",
        "4.   Naive Bayes.\n",
        "5.   Support Vector Machines.\n",
        "6.   Bagging/Boosting.\n",
        "7.   RANSAC, SMOTE.\n",
        "8.   MLE, MAP, Regularization.\n",
        "9.   Calibration.\n",
        "10.  Loss Functions.\n",
        "11.  Data Drift.\n"
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of topics under Supervised Learning:"
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To revise Supervised Learning concepts, first list down all the topics:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1kx0dRhajGukn3mGg_xsFPugqdc2I8J4c'>\n"
      ],
      "metadata": {
        "id": "6lClAUcPf6Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each model, we could think of how could we handle each of these cases.\n",
        "<img src='https://drive.google.com/uc?id=1gAE-B7l-rq0OXVQgAVx97VWFjwZNwjQw'>\n"
      ],
      "metadata": {
        "id": "xliUW9syuVEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression:"
      ],
      "metadata": {
        "id": "Q0E2Xgv_ioTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1e0O0OLEQcj4UG_y21EZ3VQxw5UAiGJLQ'>"
      ],
      "metadata": {
        "id": "oKbd6fii0Cyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss Function:"
      ],
      "metadata": {
        "id": "z6KNzR2qz8VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Squared loss/Mean Squared Error is the loss function used in Linear Regression.\n",
        "- As per Maximum Likelihood estimation (MLE), $y_i$ belongs to $w^Tx_i + b$ with Gaussion Noise $\\epsilon_{i}$ and $\\epsilon_{i}$ has Gaussian noise with $0$ mean and variance $\\sigma^2$."
      ],
      "metadata": {
        "id": "CDB6TVVPixD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bias Variance Trade-off:\n",
        "- $\\lambda$ is the hyper-parameter that could be used for Bias-Variance Trade-off."
      ],
      "metadata": {
        "id": "Ep1fvlja0Hlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What happens when $\\lambda$ increases or decreases?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- There are broadly 2 terms in Optimization function for Linear Regression: $Squared \\ loss + \\lambda (Reg)$.\n",
        "- If you give more weightage to regularization term, then squared-loss is less focused upon. So we won't overfit for sure. Hence as $\\lambda$ increases, we have a underfitted model and as $\\lambda$ decreases, we have a overfitted model.\n",
        "\n",
        "*Note:* $\\lambda$ is a function of $\\sigma^2$.\n"
      ],
      "metadata": {
        "id": "61trY-Lnypx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outliers:\n",
        "- Linear Regression is influenced by outliers."
      ],
      "metadata": {
        "id": "4v3frPTK8njm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How to fix impact of outliers in Linear Regression?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- Regularization will not always work here.\n",
        "- We could use RANSAC or change of loss function (Mean Absolute Error).\n"
      ],
      "metadata": {
        "id": "EOHAWCnZ8njo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we remove outliers using Box-plot?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- It depends upon which dimension you plot it on.\n",
        "- For e.g. in the plot below, plotting it on X-axis may not work, but plotting it on Y-axis makes sense.\n",
        "<img src='https://drive.google.com/uc?id=1hFu4JFMRQxBQ-5ux6cbmwsRWd0nPMGD5'>\n",
        "- Also, we should decide if a datapoint is an outlier only if it is outlier for the model. A datapoint is an outlier for the model only if it is far away from the Hyperplane. Hence, RANSAC is a better option."
      ],
      "metadata": {
        "id": "mYz2B-dJ9tbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we change the loss function?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- We can change the loss function to Huber Loss.\n",
        "- Huber loss is a specialized loss function to deal with outliers.\n",
        "- It is quadratic in some region and then linear in other region."
      ],
      "metadata": {
        "id": "vHXjZMxw-sNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is it a good practice to do anomaly detection before modelling?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- If you are using it to detect outliers, RANSAC would make sense.\n",
        "- For e.g. the far away point does not impact the model.\n",
        "  <img src='https://drive.google.com/uc?id=1hFu4JFMRQxBQ-5ux6cbmwsRWd0nPMGD5'>\n",
        "- Univariate analysis on $f_1$ and $f_2$ might remove these far away datapoints.\n",
        "- Using Anomaly detection we could remove these points. But there should be detailed analysis on why this point is an anomaly.\n",
        "- Hence RANSAC based approach is more useful."
      ],
      "metadata": {
        "id": "0YMnlk41AVBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Function:"
      ],
      "metadata": {
        "id": "MZvDKqCp8njo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decision function is a plane $\\pi^d$. If we have a quadratic/higher order polynomial, then we change it to curve or surface in $d$ dimensions.\n",
        "<img src='https://drive.google.com/uc?id=18wrST1NUkn9tI7NcTvgS_JmZHDsui6z0'>\n"
      ],
      "metadata": {
        "id": "sdBQsVNajT1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $95\\%$ Confidence Interval on $\\hat{y}_i$:"
      ],
      "metadata": {
        "id": "tA9TgYZfB0sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How can get $95\\%$ confidence interval on $\\hat{y}_i$?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- We can determine $95\\%$ either by using bootstrapping or distribution of errors.\n",
        "<img src='https://drive.google.com/uc?id=1fomlB8CCzu6iWNsN5AmD7V_bz4vmfPqF'>\n",
        "- In bootstrapping, we take sample of the dataset with replacement and build multiple datasets. Each of this sampled dataset is passed to individual model and confidence interval is generated from it.\n",
        "- Disadvantage of this approach is that evaluation time is larger. This is because the query point $x_q$ needs to be be passed through $M_1,M_2,... M_k$ to get the output.\n",
        "- In Linear Regression, we assume that $y_i \\sim N(w^Tx_i+b, \\epsilon)$ and $\\epsilon \\sim N(0,\\sigma^2)$. Here $\\hat{\\sigma}^2$ is estimated from MSE of training data ($MSE_{Train})$.\n",
        "- For of a datapoint $x_q$, we can compute $y_q$ as $w^Tx_q + b \\pm 1.96 \\hat{\\sigma}$.\n",
        "\n",
        "*Note:*\n",
        "- Both options work here, but if low latency during evaluation time is required, then distribution of errors makes more sense.\n",
        "- In Bootstrapping, we are evaluating $K$ models instead of one model."
      ],
      "metadata": {
        "id": "8uIxb5A0B0sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Estimation time aside, will error based method be more accurate or Bootstrapping?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "If all statistical assumptions hold then distribution of errors would be more accurate ($y_q = w^Tx_q + b \\pm 1.96 \\hat{\\sigma}$)."
      ],
      "metadata": {
        "id": "dkpRojBrE8ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretability:"
      ],
      "metadata": {
        "id": "qdLhtW9EFe9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $|w_j| \\to$ Absolute value of weights on standardized dataset is helpful for interpretability.\n",
        "<img src='https://drive.google.com/uc?id=1VTaA9spvd4lpfRVwWVAHJC8Q7OAaX9Bj'>"
      ],
      "metadata": {
        "id": "giGm23v3Fe9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Where would this break? When can you not interpret weights easily?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- This breaks when we have collinear features. Hence Statisticians want to remove correlated features using VIF.\n",
        "- Suppose we have features $f_i, f_j, f_i*f_j, log(f_i)$. Here we will have non-zero correlation.\n",
        "- Using VIF will remove correlated features, but model performance will drop as well.\n"
      ],
      "metadata": {
        "id": "OjV0USm0GG-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, $L_1$ regularization is used as it will zeroout lesser important features."
      ],
      "metadata": {
        "id": "36g0UtfZHabo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Alternate Option:***\n",
        "\n",
        "- We keep removing features and measure delta validation loss. This will certainly be expensive, as we need to train $'n'$ models for $'n'$ features."
      ],
      "metadata": {
        "id": "0sky540HHls3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is there any way to estimate such features instead of selecting all?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- There are techniques like LIME and SHAP. These will be covered in Deep Learning Sessions."
      ],
      "metadata": {
        "id": "liqmV-A9Fe9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-Processing/Post-Processing:"
      ],
      "metadata": {
        "id": "mwMpMHdwInOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We need to standardized dataset before model processing and is needed for interpretability of $w_j's$ and it simplifies optimization as well.\n",
        "  \n",
        "  <img src='https://drive.google.com/uc?id=1s7PXs_eKIbN13ey1A3jJ-SWT_2sQpIGl'>\n",
        "- Once we predict $\\hat{y}_i$, we should check if $\\epsilon$ are normal or not and check if the Confidence interval are trustworthy."
      ],
      "metadata": {
        "id": "bUd8QOZ6InOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** VIF uses linear function internally, so is it able to model non-linear relationships well?\n",
        "\n",
        "***Answer:***\n",
        "- There is some ability to predict it.\n",
        "  <img src='https://drive.google.com/uc?id=12K_xLFu0rqtlhfIU5SROlekdTsYoO5Ey'>\n",
        "- For e.g. let say we have features, $...f_i..f_j.. f_i*f_j...$. During VIF, $...f_i..f_j... \\to x_i$ and $f_i*f_j \\to y_i$.\n",
        "- VIF might insist to drop $f_i$ and $f_j$. But the model performance may still increase due to this polynomial component.\n",
        "- This is where Statistians and Applied Scientists disagree. A Statistician would say that fundamental assumptions of Linear Regression are breaking down. Whereas an Applied Scientist would say as long as model performance is increasing, it is acceptable.\n",
        "- If the assumptions, weight interpretability, error margins are breaking down it could be addressed using alternate solution like SHAP, bootstrapping.\n",
        "\n",
        "*Note:* You can use Rank Correlation for Non-linear relations.\n"
      ],
      "metadata": {
        "id": "1Fne_w5xInOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Encoding Techniques:"
      ],
      "metadata": {
        "id": "_b2QdwZUMIfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What are some of the good Feature encoding techniques?\n",
        "\n",
        "***Answer:***\n",
        "- For categorical features, we could use Target Encoding/One-Hot encoding.\n",
        "- For numerical features, we could use standardization.\n",
        "  <img src='https://drive.google.com/uc?id=1CfYcPG628nMggYuLewPpWsA4QNaYQ6Jg'>\n"
      ],
      "metadata": {
        "id": "qc_QO16KMH2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why Target Encoding?\n",
        "\n",
        "***Answer:***\n",
        "- A feature with a large number of categories can be troublesome to encode and one-hot encoding would generate too many features."
      ],
      "metadata": {
        "id": "EhbVlnS-M248"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why One-Hot Encoding? It might cause curse of dimensionality.\n",
        "\n",
        "***Answer:***\n",
        "- Increasing dimensions means more room or flexibility to fit a model."
      ],
      "metadata": {
        "id": "LEboY0TUSj6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Time:"
      ],
      "metadata": {
        "id": "oB5CKEkdTL5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test time latency is $O(d)$. In fact it is actually $O(d')$, where $d' \\to$ number of non-zero weights."
      ],
      "metadata": {
        "id": "zgAZiAs8TL5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression:"
      ],
      "metadata": {
        "id": "zCn_At-vThN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1hkVjFMQBvCModybuDum3KpUWK2zBHAHr'>\n"
      ],
      "metadata": {
        "id": "0GHz91wxUNw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imbalanced Data:"
      ],
      "metadata": {
        "id": "VEuBdHLZTntm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How to address Imbalanced Dataset in Logistic Regression?\n",
        "\n",
        "***Answer:***\n",
        "- We can use general purpose techniques like Up-sampling, Down-sampling, SMOTE.\n",
        "- There as also model specific approaches like class weights.\n"
      ],
      "metadata": {
        "id": "Jcvvi19qThN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-class classification:"
      ],
      "metadata": {
        "id": "JLO5-m6qUcPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How to address Multi-class classification in Logistic Regression?\n",
        "\n",
        "***Answer:***\n",
        "- One versus the rest.\n",
        "- Softmax classifier used in Deep Learning."
      ],
      "metadata": {
        "id": "53dwvfhmUcPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss Function:"
      ],
      "metadata": {
        "id": "kvaU3PaPVFSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Loss functions used in Logistic Regression?\n",
        "\n",
        "***Answer:***\n",
        "- Log-loss (Binary Cross Entropy)."
      ],
      "metadata": {
        "id": "I4_JaF-IVFST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probabilistic Class labels:"
      ],
      "metadata": {
        "id": "5ggVswJZVW0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1VYMsLrhRDILGl_r5Oz-llF9jSjohgyX4'>\n"
      ],
      "metadata": {
        "id": "w8JtxB-pYYMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is probabilistic class labels possible in Logistic Regression?\n",
        "\n",
        "***Answer:***\n",
        "- Yes it is possible to determine them using $\\hat{y} = P(y_i=1\\bigg| x_i,w)$."
      ],
      "metadata": {
        "id": "Kc_c9snqVW0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Failure cases for probabilistic class labels in Logistic Regression?\n",
        "\n",
        "***Answer:***\n",
        "- High class imbalance will cause it to fail.\n",
        "- For e.g. if $P(y=1) = 0.001$. Due to class imbalance, $\\hat{y}$ are not well calibrated."
      ],
      "metadata": {
        "id": "2PYxfZ9wV3jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretability/Feature Encoding/Outliers:"
      ],
      "metadata": {
        "id": "Jrl5CW5wXd72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as Linear Regression.\n",
        "<img src='https://drive.google.com/uc?id=1XunWjvnNxObSKjaRbhan9C-OpQ0yTwQM'>\n"
      ],
      "metadata": {
        "id": "VH20-KGGXn_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers on the correct side will not be an issue in the Logistic Regression. Those on the wrong side certainly are.\n",
        "<img src='https://drive.google.com/uc?id=1OTu8X2areS09qRBVsah2gheIRU9Iy0FO'>\n"
      ],
      "metadata": {
        "id": "WcoYdtpWZsHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Time:"
      ],
      "metadata": {
        "id": "ayQY-W32YlHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test time latency is $O(d')$, where $d' \\to$ number of non-zero weights."
      ],
      "metadata": {
        "id": "bb-3l7RgYlHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Post-Processing:"
      ],
      "metadata": {
        "id": "NNHuCkFVWo36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As a rule of thumb, always plot Calibration plots.\n",
        "- Calibration plots are useful when we have high imbalanced data.\n",
        "- Plot Confusion Matrix, AUC/ROC curves."
      ],
      "metadata": {
        "id": "1KmGmah-W9-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cons of Linear Models:"
      ],
      "metadata": {
        "id": "S_EAXbjOYyqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear Models cannot automatically detect non-linear relationships.\n",
        "- We could add manually non-linear relationships through quadratic features."
      ],
      "metadata": {
        "id": "QB9lzCT7Y1zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Support Vector Machines (SVM):"
      ],
      "metadata": {
        "id": "bnIECWBwaE_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is very similar to Linear models.\n",
        "- It uses Hinge-loss and $L2$ Regularization.\n",
        "<img src='https://drive.google.com/uc?id=10lRGSacP3BX8jArBHn2xQ7nqFjj8b8fd'>\n",
        "- They are interpretable as we can get $w_j's$ by solving optimzation problem using SGD.\n",
        "- Probability class labels can be determined as $P(y_i=1\\bigg|w,x_i)$. For every $x_i$, we compute $d_i$ which is the signed distance and it could be calibrated.\n",
        "- Feature encoding and Outliers is same as linear models.\n"
      ],
      "metadata": {
        "id": "yQRV928EaMw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kernel Support Vector Machines (SVM):"
      ],
      "metadata": {
        "id": "FSg5dUTSbkYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In SVM, the kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.\n",
        "<img src='https://drive.google.com/uc?id=1RZN-4pukARc8Z7DHBJ8HfksyOMscaiQU'>\n"
      ],
      "metadata": {
        "id": "kn9UsyjSbkY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disadvantages:"
      ],
      "metadata": {
        "id": "QWpSKNAedMRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In SVM, there is no interpretability.\n",
        "- Time complexity is much more as it depends upon number of support vectors.\n",
        "<img src='https://drive.google.com/uc?id=1H-Bn_KjmcAhtl7hcLH8AQEOA1Rh5Zk_b'>\n",
        "- Custom domain specific are needed for good performance."
      ],
      "metadata": {
        "id": "oV8GnTsSdP_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees:"
      ],
      "metadata": {
        "id": "BPxYCL5Jd88b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No standardization is required in Decsion Trees as we are not solving core tree building using optimization, but greedy based tree expansion using entropy.\n",
        "- For Regression using DT, MSE is used.\n",
        "  <img src='https://drive.google.com/uc?id=1e4IHpn6LCp_gvAhwjeV2RXMG2wqukfdk'>\n",
        "- For feature importance, we have weighted/normalized information gain.\n",
        "- DT are not much affected by outliers.\n",
        "- For feature encoding, we can use Target encoding. OHE is not preferred as fewer dimensions is fundamentally good. Checks for number of splits reduces and are much more sensible.\n",
        "- We need not encode numerical features. We could also try binning.\n",
        "\n"
      ],
      "metadata": {
        "id": "LqOonZ79eBGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Trees can handle categorical features. Why to encode them?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- Depends upon which dimension you plot it on.\n",
        "- Let there be a feature $f_i$ with categories as $c_1, c_2,...c_{20}$.\n",
        "- So we would have to check for 20 possible splits and certainly complicate the process.\n",
        "  <img src='https://drive.google.com/uc?id=1kMfWTCtWlWqZi-8JiWBnmTKQw-86nS_w'>\n",
        "- Target encoding is computed as $P(y_i\\bigg| f_i=C_1)$. If these categories are more or less same in number, then it will use simple axis parallel hyperplane to group them.\n",
        "- But if more groups are created, then complexity explodes.\n",
        "- Because of entropy based methods, features with more categories are biased towards in splits."
      ],
      "metadata": {
        "id": "yXgvjRDejpu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Confidence Interval is determined using bootstrapping.\n",
        "- Probability class labels can be determined using impurity in each leaf node. But it is slightly tricky and often doesn't work well.\n",
        "<img src='https://drive.google.com/uc?id=1HfXegnXthqZb9lelBKJGyOLJnuYhQVtD'>\n"
      ],
      "metadata": {
        "id": "J2ODxRlKeqWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes:"
      ],
      "metadata": {
        "id": "7zrkmV8FfPY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Naive Bayes, Laplace smoothing is most important concept.\n",
        "- Important assumption in Naive Bayes is that features are **conditionally** independent.\n",
        "<img src='https://drive.google.com/uc?id=1XUuAtr-tKPpVAGp_CmcOl-CO_HzTOIoK'>"
      ],
      "metadata": {
        "id": "rObysxNYfTXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boost Decision Trees:"
      ],
      "metadata": {
        "id": "MoAYRPxpfqG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You can train a GBDT for any differentiable loss function.\n",
        "- Trickiest part is to how to compute weights on $m^{th}$ learner.\n",
        "<img src='https://drive.google.com/uc?id=1Idu5l7qaAhWWu4HiSwIJtAVTkytYj0KN'>\n",
        "- Concept of pseudo residuals is important.\n",
        "- XgBoost and LightGBM are go to strategy for tabular data."
      ],
      "metadata": {
        "id": "lMvo-E_6fqG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Misc. Questions:**"
      ],
      "metadata": {
        "id": "PBdyxyopoyKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we use PCA for feature importance in Linear Regression?\n",
        "\n",
        "***Answer:***\n",
        "- No, VIF can easily handle this.\n",
        "- In PCA, we map features in $d$ dimensions to $d'$ dimensions.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YwCFOOqr3KmrMhdZxZhg9WSS_Z-IoZTX'>"
      ],
      "metadata": {
        "id": "qoYywzuigTLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How do we decide between standardization and normalization? can we use it interchangeably ?\n",
        "\n",
        "***Answer:***\n",
        "- Standardization means mean centering and variance scaling whereas normalization means min-max scaling.\n",
        "- In Machine learning, standardization is mostly preferred.\n",
        "<img src='https://drive.google.com/uc?id=1gX98rwGcg003tbP6ty8rGp3zRTsDqY1S'>\n",
        "- Min-max scaling in used in Computer vision where images are represented using 0-255 value of RGB.\n"
      ],
      "metadata": {
        "id": "X4jXA-xUhAsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is there a fundamental difference between anomaly and outlier? or is anomaly a subset of some kind of outliers?\n",
        "\n",
        "***Answer:***\n",
        "- They both mean the same.\n",
        "- As rule of thumb it is done when we know there is a strongly possibility of outliers or there are some concerns with how data was collected.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1wH1wzk5bwM0cebDaMyRBrFD4fbJrIZ0Q'>\n",
        "- There are two ways to detect anamolies. Model Dependent and Model Independent.\n",
        "- In model independent, we can do EDA + domain expert to detect anomalies.\n",
        "- If the model is robust to outliers, then the model itself handles outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "vbv-oRhOhOFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What are generative models vs. discriminative models?\n",
        "\n",
        "***Answer:***\n",
        "- Generative model are trying to understand how data is generated in the first place. For e.g. Naive Bayes, GMM. Another example would be GANs and will be covered in Deep Learning.\n",
        "- In Discriminative models we are trying to discriminate data and not understand how the data was generated. For e.g. Decision Tree.\n",
        "<img src='https://drive.google.com/uc?id=1QjbLdP9mlg6a0taPFDbwN_AKR0DjaqUp'>"
      ],
      "metadata": {
        "id": "aWNseDgShZ2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Drift:**"
      ],
      "metadata": {
        "id": "J3iqF6WNh2zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine, we have a model built using data of last 100 days. Let today is $'t'$.\n",
        "- Let say the model performance starts detiorating after some days (10 days). Reason could be change in test dataset.\n",
        "<img src='https://drive.google.com/uc?id=1BqsXBdZOVwZTkQbcImlLLE7hYEPU1fXl'>\n",
        "- For e.g. $Test^1$ dataset on which the model was trained/evaluated before productionalization is different than $Test^2$.\n",
        "- So we can say that dataset drifts away from training data.\n",
        "- We can track this live in production on a daily or weekly basis.\n",
        "  <img src='https://drive.google.com/uc?id=18UFsQrJRDij6E_Uu3PVNCsgd4fwHw-A4'>\n",
        "- We take the recent data and give it higher weightage."
      ],
      "metadata": {
        "id": "oZ_-3hQ7hlZs"
      }
    }
  ]
}