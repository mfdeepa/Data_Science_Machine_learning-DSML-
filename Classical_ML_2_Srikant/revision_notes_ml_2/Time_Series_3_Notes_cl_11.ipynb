{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on August 19, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/34668/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "1.   Quick Recap of Auto Correlation Function ACF.\n",
        "2.   PACF.\n",
        "3.   Train-Test split in Time Series.\n",
        "4.   Metrics used in Time Series.\n",
        "5.   Model Building:\n",
        "        - Mean Model.\n",
        "        - Naive Forecast.\n",
        "        - Seasonal Naive Forecast.\n",
        "        - Simple Drift Model.\n",
        "        - Moving Average Forecast.\n",
        "        - Simple Exponential Model (SES).\n",
        "        - Dual Exponential Model (DES).\n",
        "        - Triple Exponential Model (TES).\n",
        "6.    Mixture of Additive and Multiplicative Model.\n",
        "7.    Complex Models:\n",
        "        - Auto Regressive model AR(p).\n",
        "        - Moving Average model MA(q).\n",
        "        - Auto Regressive Moving Average ARMA(p,q).\n",
        "        - Auto Regressive Integrated Moving Average ARIMA (p,q,δ).\n",
        "        - Seasonal-ARIMA (p,q,δ,s,P,Q,D).\n",
        "        - SARIMAX.\n"
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Recap of ACF:\n"
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let say, we have a Time Series $x_t$ wave. We shift it by some $m$ units and compute the correlation coefficient of Time Series $x_t$ and new Time Series $x_{t+m}$ that we shifted by some $m$ units.\n",
        "<img src='https://drive.google.com/uc?id=1U3l2pjuZogox5jJVx_4LJ2Z-_oOKrklo'>\n",
        "- If the correlation is very high of these 2 Time Series, we can say that Time Series has seasonality of $m$."
      ],
      "metadata": {
        "id": "xT2g-rkZSUxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ACF plot of de-trended Mobile Sales data:"
      ],
      "metadata": {
        "id": "aiHiqjF5hfM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We got it by differencing it with previous timestamp.\n",
        "<img src='https://drive.google.com/uc?id=1Obp8qpDbnEof9GsqGPxfhJVIU2tKpoiY'>\n",
        "- There is an interesting part of this de-trended ACF plot.\n",
        "    - At 0, we have very high correlation as we are doing correlation of Time Series with itself.\n",
        "    - There is high correlation at 12 units and there is also a good correlation at 6 units.\n",
        "    - This shows us that there is seasonality.\n",
        "    - There is also negative correlation at 1 unit. This happens due to the fact that sales usually dips after peak season.\n",
        "\n"
      ],
      "metadata": {
        "id": "j4T5YMl05H2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** While learning linear regression we discussed there should be no correlation as one of assumption. Would this be the same case in Time Series.\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "In Linear Regression, we had features $f_1,f_2,...f_d$ and we checked if features are correlated using VIF. In Time Series, there will always be some amount of correlation.\n",
        "\n",
        "Suppose, we are predicting $\\hat{x}_t$ using $x_{t-1},x_{t-2},....$. Here, value at time $t$ is dependent on $x_{t-1}, x_{t-2},...$"
      ],
      "metadata": {
        "id": "HFd3mOeR63bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 95% Confidence Interval for Correlation:\n"
      ],
      "metadata": {
        "id": "LAuubnezrs2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Blue line in the below graph shows 95% Confidence Interval (CI). Suppose at 18 units, we have $Corr(x^d_t, x^d_{t+18}) \\approx 0.1$.\n",
        "    <img src='https://drive.google.com/uc?id=1xFvBITIzzfsv6wXlcY6uYdM_NU5x_pey'>\n",
        "- So the Blue line indicates 95% CI on $Corr(x^d_t, x^d_{t+18}) = 0.1 \\pm 0.1$ = [0, 0.2].\n",
        "- So the fact that correlation = 0, it means that there is no correlation. So in ACF plot, we only take those values that are outside this Confidence Interval.\n"
      ],
      "metadata": {
        "id": "zog5brKJ8Vqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Auto Correlation Function (PACF):"
      ],
      "metadata": {
        "id": "_6ir3eMRYiuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concept of PACF:"
      ],
      "metadata": {
        "id": "Enh7hYD_Bksg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In ACF, we wanted to compute correlation coefficient between a Time Series and itself which is shifted by $m$ units.\n",
        "- In PACF, we compute correlation coefficient between a Time Series and itself which is shifted by $m$ units after all other intermediate correlations have been accounted for or removed. It means that how much can I attribute to seasonality of 12 months, after removing average effect at $m = 1,2,....11$.\n",
        "    <img src='https://drive.google.com/uc?id=10LoXMoeSnNvp3wku_UvNtp9_4WUfLLq6'>\n",
        "- This is also called as Conditional Correlations in Statistics."
      ],
      "metadata": {
        "id": "nsRV68feUQra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For e.g. if we take raw mobile sales Time Series data and plot simple auto correlation plot, we have high correlation at $0$, slightly lesser correlation at $1$ and $2$ as well.\n",
        "    <img src='https://drive.google.com/uc?id=1rPhgyl6wv3pR0ZmdYgydywFUAaAet65x'>\n",
        "- So when we compute correlation at $12$, we want it to account for all other correlations or remove all impact because of timestamp $1$ to $11$ and then see how useful is timestamp $12$.\n"
      ],
      "metadata": {
        "id": "wugllQf7_cYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PACF on Raw Data:"
      ],
      "metadata": {
        "id": "KXi2TA-NBr9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- At timestamp $0$, we have higher correlation. So is the case at timestamp $1$.\n",
        "- It tells us that if we have $x_t$, we can predict $x_{t+1}$ very easily.\n",
        "    <img src='https://drive.google.com/uc?id=1Ow8ne5pXyD5DhPOXFc_1Jz_gzb16BamR'>\n",
        "- There is also high value at timestamp $12$ which shows seasonality.\n",
        "- In PACF correlation coefficient at $12$ shows that even if you account for timestamp 1-11 using simple AR model to predict $x_t$ using $x_{t-1} \\to x_{t-11}$, there is lot of information at $x_{12}$.\n",
        "    <img src='https://drive.google.com/uc?id=1KkIVRcXnX2UWXCXqTbH1mLfBbGQIXLMD'>\n",
        "- PACF is also known as Conditional Auto Correlation and is used by Statisticians.\n",
        "- It tells how much is the correlation between $x_t$ and $x_{t+m}$ conditioned on the fact that we have accounted for all $x_{t-1},.... x_{t-m-1}$\n",
        "$$Conditional \\ Auto \\ Correlation = Corr(x_t, x_{t+m}|x_{t-1}... x_{t-m-1})$$\n",
        "- Simply put, it says let us build a model using $11$ timestamps to predict $x_t$. Even after doing that there will be error term and that error term correlates very highly with $x_{t-12}$.\n",
        "    <img src='https://drive.google.com/uc?id=1DMrHzW52KWVup6gUdQZogJfPt_NevzOx'>\n"
      ],
      "metadata": {
        "id": "tTKU-fciBNuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Splits:"
      ],
      "metadata": {
        "id": "jN259W5zYqJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- During Classification and Regression problems, we did random splits for training and test data.\n",
        "- For Time Series, we do a time based split.\n",
        "    <img src='https://drive.google.com/uc?id=1boUNeHOhyWZf7Rw4wCuFjpqNYzqkuFZ3'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_s-637WcWUHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics for Time Series:"
      ],
      "metadata": {
        "id": "Mjs2kCA9aYwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Which metrics could be used for Time Series that we have used in the past?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- Mean Squared Error (MSE).\n",
        "- Mean Absolute Error (MAE).\n",
        "- Mean Absolute Percentage Error (MAPE).\n",
        "\n",
        "\n",
        "Of these MAPE is preferred a lot more. What could be the reason?"
      ],
      "metadata": {
        "id": "cUMa3cFaatX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MAPE is a relative measure.\n",
        "- Let say, $MSE = 20.6$, then what does it mean? If a Time Series is in $5000-7000$ range then $20.6$ is low. But if Time Series value is in $20-40$ range, then this MSE is terrible.\n",
        "- MAPE is computed as:\n",
        "$$ MAPE \\ = \\ \\frac{|x_t - \\hat{x_t}|}{x_t}$$\n",
        "- In MAPE, we determine how different is predicted value with respect to original value. So if MAPE is $2\\%$, then we know that we are off by $2\\%$.\n",
        "    \n",
        "    <img src='https://drive.google.com/uc?id=1mgIuNOJviWwVsvV0eCbukKa4PhP7ENzd'>\n",
        "\n",
        "*Note:* Offcourse, MAPE is robust to Outliers.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hRzxWB0-aYwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Foundational Time Series Models:"
      ],
      "metadata": {
        "id": "ZLzE3eJ4Yz9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base Model:"
      ],
      "metadata": {
        "id": "wIxWibNdc66j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For a given training data, we calculate mean/average and this average value is used as predected value in the test data.\n",
        "    <img src='https://drive.google.com/uc?id=1LS7gydRiCTEKQq2T8zJZ8t-eskAzVzZ2'>\n",
        "- This model will be the baseline to compare with and it tells us what the baseline MAPE is for the given data.\n",
        "    <img src='https://drive.google.com/uc?id=1Jhwcc5xBo34PVf1NH-bZoKQNVQHPuL8L'>\n",
        "- In this example, MAPE is $25.5\\%$. We are roughly off by $25\\%$ on an average. So this is the baseline MAPE and we cannot have a model worst that this.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b2EGMXMSWFAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Naive Forecast:"
      ],
      "metadata": {
        "id": "RALokxZcee9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It uses last value as predicted value for all test points.\n",
        "    <img src='https://drive.google.com/uc?id=1P2PnPQvnON7wuhkBSBLxCr0qOA8snOH9'>\n",
        "- This model gives an MAPE of $23\\%$. This model is a high variance model. It depends on dataset.\n",
        "    <img src='https://drive.google.com/uc?id=1mx2TwWINkiLQATX6E-KLcU5dt4F1DyEd'>\n",
        "- Model output depends on where the train data stops and test data starts. Small change in dataset will change output of model. Hence they are called High Variance Model.\n",
        "- Small change in training data will have huge impact on model's output and performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R5JjoMYxee9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seasonal Naive Forecast:"
      ],
      "metadata": {
        "id": "pcPK7-bvpmag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We learnt from PACF/ACF that there is 12 month seasonality for mobile sales data.\n",
        "- If we predict $\\hat{y_t}$ as $y_{t-12}$, we get a Seasonal naive forecast.\n",
        "    <img src='https://drive.google.com/uc?id=1rlHjwo4v-UZD6gM0Pwlc3TgID1lncQky'>\n",
        "- This very simple model gives an MAPE of $5.5\\%$.\n",
        "    <img src='https://drive.google.com/uc?id=1iFRln-G58e6lpDS0GEpqPgjCM0sFCqDc'>\n",
        "- Because data has strong seasonality, it captures peaks very well and trend to a certain extent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuVV-tylpmap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drift Model:"
      ],
      "metadata": {
        "id": "c-2xHUbzqzjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine we have Time Series as shown in diagram below. Drift model takes first value and last value in training data and it computes a slope. Using this slope it predicts value in test set.\n",
        "    <img src='https://drive.google.com/uc?id=1SEa04F17aapMQALn0RzJzmbJ-eD7_XUm'>\n",
        "- In Drift model we are capturing trend but not seasonality. MAPE in this case is $22.3\\%$.\n",
        "    <img src='https://drive.google.com/uc?id=1pPkwSsxed5B88qG6buh7po8FJEpktTsn'>\n",
        "- While there is some trend, trend seems to be less important than seasonality.\n",
        "    <img src='https://drive.google.com/uc?id=14jyAbxOVluAxETMAibhMjE7oVDnAmEvu'>\n",
        "- Interestingly, we take $last-1$ value to compute slope, then we get MAPE as $11.5\\%$. So it's sensitive to last value and hence a high variance model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mHSOc4CiqzkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moving Average Forecast:"
      ],
      "metadata": {
        "id": "1YaD5i_1s3nM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Moving average with window = 3 is computed as:\n",
        "$$x_t = \\frac{x_{t-1}+ x_{t-2}+ x_{t-3}}{3}$$\n",
        "    <img src='https://drive.google.com/uc?id=1Ijq0tJlff2Rf4BS4eYkf1QLV8nBJORnn'>\n",
        "- MAPE for this model is $11.5\\%$. There is some pattern in beginning but it flattens out. But it still does not beat Seasonal Naive Forecast. This happens as to compute future value we take last 3 values to compute the average.\n",
        "    <img src='https://drive.google.com/uc?id=1EhaWWzNDkugGaINn-cdhX2pioqeupQto'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3WZR2iLEs3nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple Exponential Smoothing (SES):"
      ],
      "metadata": {
        "id": "1l7DpmGqu4Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exponential Smoothing is weighted moving average with exponentially decreasing weights.\n",
        "- Imagine we have a Time Series as shown below. If we use simple moving average with window size = 10, then all points get equal weightage.\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1W_S5Y0jN1Wqw1hudSi3wdcH8Mhow-EEZ'>\n",
        "- In SES, weighted moving average gets exponential weights such that most recent gets more weightage and older ones get less and less weightage.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jP5u4pyVu4JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "PL7Lev3V-X_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mathematically, it is represented as: $\\ \\hat{y}_{t+1} = \\alpha \\ y_t + (1-\\alpha)\\ \\hat{y_t}$\n",
        "- The equation is also said to be a recursive function. We can further substitute value of $\\hat{y_t}$.\n",
        "\\begin{equation}\n",
        "  \\begin{aligned}\n",
        "    \\hat{y}_{t+1} & = \\alpha \\ y_t + (1-\\alpha)\\ \\hat{y_t}\\\\\n",
        "      & = \\alpha \\ y_t + (1-\\alpha) \\ [ \\alpha \\ y_{t-1} + (1-\\alpha) \\ \\hat{y}_{t-1} \\ ]\\\\\n",
        "      & = \\alpha \\ y_t + \\alpha (1-\\alpha) \\ y_{t-1} + (1-\\alpha)^2 \\ \\hat{y}_{t-1} \\\n",
        "  \\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "- Data is given from $y_0,y_1,y_2....y_t$ and we predict $\\hat{y}_{t+1}$.\n",
        "    <img src='https://drive.google.com/uc?id=1t7dZpD7J7xv0299Wvm2JgiAtHeox43E9'>\n",
        "- If we keep simplifying above equation, we endup with:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\begin{aligned}\n",
        "    \\hat{y}_{t+1} & = \\alpha \\ y_t + \\alpha (1-\\alpha) \\ y_{t-1} + \\alpha(1-\\alpha)^2 \\ y_{t-2} + \\ .....\n",
        "  \\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "*Note:*\n",
        "\n",
        "$\\alpha$ lies between 0 and 1 i.e. $0<\\alpha<1$\n",
        "\n"
      ],
      "metadata": {
        "id": "aLGQo5mm0Jm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let $\\alpha = 0.8$.\n",
        "    <img src='https://drive.google.com/uc?id=1u0FQJu9KpMUYZcsR94a41hfwzXv_-gW_'>\n",
        "- Substituting the value of $\\alpha$, we get:\n",
        "\\begin{equation}\n",
        "  \\begin{aligned}\n",
        "    \\hat{y}_{t+1} & = 0.8 \\ y_t + 0.8*(0.2) \\ y_{t-1} + 0.8*(0.2)^2 \\ y_{t-2} + 0.8*(0.2)^3 \\ y_{t-2} +\\ .....\\\\\n",
        "        & = 0.8 \\ y_t + 0.16 \\ y_{t-1} + 0.032 \\ y_{t-2} + 0.0064 \\ y_{t-2} +\\ .....\n",
        "  \\end{aligned}\n",
        "\\end{equation}\n",
        "- So we are giving exponentiall smaller weights to every previous observations."
      ],
      "metadata": {
        "id": "TbumofZn5CYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Weights Plot:"
      ],
      "metadata": {
        "id": "8xhpwdpZ-hcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we draw the weights curve, it looks like an exponentially decreasing curve.\n",
        "    <img src='https://drive.google.com/uc?id=1T5DfkyayrQEjo3Wil4XKwAG4DG3PZtie'>\n",
        "- The intuition is that we are going to forget older values exponentially faster.\n",
        "- SES also has recency bias as well."
      ],
      "metadata": {
        "id": "eLW5anyP8l8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Key point to remember is that in SES we take $y_0,y_1,....y_t$ and predict $\\hat{y}_{t+1}$.\n",
        "- We then observe value of $y_{t+1}$ and predict $\\hat{y}_{t+2}$ and so on.\n",
        "    <img src='https://drive.google.com/uc?id=1_BJDj-geKlvlMERo685poowrrM3Zx7Ld'>\n"
      ],
      "metadata": {
        "id": "RnYXi5E89U-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\alpha$ value:"
      ],
      "metadata": {
        "id": "ftS68fXV_2hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suggested value of $\\alpha$ is:\n",
        "$$\\alpha_{suggested} = \\frac{1}{2*seasonality}$$\n",
        "    <img src='https://drive.google.com/uc?id=1VqEQYIcSzpkN_57_Lp76kl1I270XzjN-'>"
      ],
      "metadata": {
        "id": "G5DyaSry-wOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SES on entire Mobile Sales data:"
      ],
      "metadata": {
        "id": "WRCwkjvr_5QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we do exponential smoothing on whole data, we see the following output (Blue line is SES result).\n",
        "    <img src='https://drive.google.com/uc?id=1IpFPjhHOPPz3JyPtfUHA46eo9tRmSh1e'>\n",
        "- It looks more and more like trend line.    "
      ],
      "metadata": {
        "id": "UEp-fax9_5QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we have last 9 years data and we predict next 12 months. Here we won't have $y_{t+1}$ observation. So we will use $\\hat{y}_{t+1}$ and so on. Notice it starts to behave like a simple flat line.\n",
        "    <img src='https://drive.google.com/uc?id=1xbm4mHeUVg2ExWfUs29BqHAVEmZWmPNy'>\n",
        "- This happens because to compute $\\hat{y}_{t+1}$ we used all past data and while predicting next value we gave less weightage to previous value and lesser weightage to values before that.\n",
        "- So, if we do not have $y_{t+1}$ (observed), then it starts missing out on trend."
      ],
      "metadata": {
        "id": "_2LdyrbVAeLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SES performance on Train-Test Splits:"
      ],
      "metadata": {
        "id": "P_UhSoS66gP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SES model trains on training dataset and makes prediction for next 12 months.\n",
        "    <img src='https://drive.google.com/uc?id=1AOIF2y1t3vmxCR2yztjyEWvcISmPF6RN'>\n",
        "- Here, SES looks more like flat line. Model has $10.9\\%$ MAPE. Still not better than Seasonal Naive Forecast  but has potential.\n",
        "- So SES misses both on trend and seasonality."
      ],
      "metadata": {
        "id": "9Hg7XProgjx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Two ways to approach SES:"
      ],
      "metadata": {
        "id": "8KM0H8cbCI--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- One approach, is that we take $y_0,y_1,....y_t$ and predict $\\hat{y}_{t+1}$. We then observe value of $y_{t+1}$ and predict $\\hat{y}_{t+2}$ and so on.\n",
        "- Another approach is that we have $y_0,y_1,....y_t$ and we have to predict $\\hat{y}_{t+1},.....\\hat{y}_{t+10}$. This is much harder.\n",
        "  <img src='https://drive.google.com/uc?id=1iG80xHYX4ks3PGh3iWM3BelmRQzgxZNe'>\n"
      ],
      "metadata": {
        "id": "4IZgzfQ4CI_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Double Exponential Smoothing (DES):"
      ],
      "metadata": {
        "id": "SBpL39Bthz1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Double exponential smoothing is also referred to as Holt's Linear Model.\n",
        "- Intuitively, $DES = SES + Trend$.\n",
        "    <img src='https://drive.google.com/uc?id=1yv0UJ22EUFnHR7Nl-XBZDD-Tf2Qg-z75'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ksUj1Bnwhz2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "locF_Exthz2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SES Math:***\n",
        "\\begin{equation}\n",
        "  \\begin{aligned}\n",
        "    \\hat{y}_{t+1} & = \\alpha \\ y_t + (1-\\alpha) \\ \\hat{y}_t \\approx Exponential \\ Smoothing\\\\\n",
        "    \\hat{y}_{t+1} & = \\alpha \\ y_t + \\alpha (1-\\alpha) \\ y_{t-1} + (1-\\alpha)^2 \\ \\hat{y}_{t-1} \\\n",
        "  \\end{aligned}\n",
        "\\end{equation}\n",
        "- SES was missing trend and seasonality."
      ],
      "metadata": {
        "id": "T8Hpl4S3pbQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mathematically DES is represented as $\\hat{y}_{t+1} = l_t + b_t$.\n",
        "- Where $l_t \\to$ lag term i.e. Information from the past and $\\approx$ SES part and $b_t \\to$ trend term.\n",
        "$$l_t = \\alpha \\ y_t + (1-\\alpha) \\ [l_{t-1} + b_{t-1} ]$$\n",
        "- $[l_{t-1} + b_{t-1} ]$ is recursively expanded.\n",
        "    <img src='https://drive.google.com/uc?id=1sy-_Fb9VMxZHejUDpvDXhhmR8jr9l60v'>\n",
        "- Trend term is expressed as $b_t = \\beta (l_t - l_{t-1}) \\ + \\ (1-\\beta)*b_{t-1}$.\n",
        "- Here, $(l_t - l_{t-1}) \\to \\ current \\ slope \\ and \\ b_{t-1} \\to \\ previous \\ slope$.\n",
        "- In SES, we had single term '$\\alpha$' and we did exponential smoothing using $\\alpha$.\n",
        "- Here, we have terms '$\\alpha$' and '$\\beta$' and we do exponential smoothing using both $\\alpha$ and $\\beta$.\n",
        "- $(1-\\beta)*b_{t-1}$ gives some weightage to older trend that we had computed at $t-1$.\n",
        "\n",
        "*Note:*\n",
        "\n",
        "$\\alpha$ lies between 0 and 1 i.e. $0<\\alpha<1$\n",
        "\n",
        "$\\beta$ lies between 0 and 1 i.e. $0<\\beta<1$\n",
        "\n"
      ],
      "metadata": {
        "id": "6DppONDVhz2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To summarize:***\n",
        "\n",
        "Predicted value at $(t+1)$, $\\hat{y}_{t+1}$ is a combination of lag term and trend term.\n"
      ],
      "metadata": {
        "id": "Qdtmd1hB3i-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### $\\alpha$ and $\\beta$ values:"
      ],
      "metadata": {
        "id": "5zmQ03oE57B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suggested value of $\\alpha$ is:\n",
        "$$\\alpha_{suggested} = \\frac{1}{2*seasonality}$$\n",
        "- There are some heuristics that decide value of $\\beta$ using historical data."
      ],
      "metadata": {
        "id": "gSTGngnP57B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DES on entire Mobile Sales data:"
      ],
      "metadata": {
        "id": "_jHJAtHrhz2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we do double exponential smoothing on whole data, we see the following output (Blue line is DES result).\n",
        "    <img src='https://drive.google.com/uc?id=1ktus2oVnOMvnR35yAkv1nARwW7Ip1ERp'>\n",
        "- It capture trend very well but no seasonality.\n",
        "- If we give the entire mobile sales data and predict for future 12 months, we could see that some of the trend is capture.\n",
        "    <img src='https://drive.google.com/uc?id=1YuBWw5kjma8T5U2y_KsA-2i7FGH04e_d'>    "
      ],
      "metadata": {
        "id": "jzHDJ1AIhz2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DES performance on Train-Test splits:"
      ],
      "metadata": {
        "id": "hCkBdZqI6pQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DES model trains on training dataset and makes prediction for next 12 months.\n",
        "    <img src='https://drive.google.com/uc?id=1dPwy_5evRsz7GiSXE4u2a-QWolnKTKdz'>    \n",
        "- Here, DES tries to capture some of the trend. MAPE is $8.3\\%$.\n"
      ],
      "metadata": {
        "id": "C838IUBuhz2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Triple Exponential Smoothing (DES):"
      ],
      "metadata": {
        "id": "42uaR-XI5zM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Triple exponential smoothing is also referred to as Holt Winter's Model.\n",
        "- Intuitively, $TES = lag \\ term \\ (SES) + trend \\ term \\ (DES) + Seasonality$.\n",
        "    <img src='https://drive.google.com/uc?id=1xjYzUkqAumSrEbITBwU3F_pdFNnjX-zO'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RLnzQ3gU5zNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "5KJ_Wa4B5zNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mathematically TES is represented as $\\hat{y}_{t+1} = l_t + b_t +  S_{t-1+m}$.\n",
        "- Where $l_t \\to$ lag term and $b_t \\to$ trend term and $S_{t-1+m}$ is seasonality.\n",
        "    <img src='https://drive.google.com/uc?id=1jwAUFRFRqPkpK0WzPlN2VIj6pO0mJMCs'>\n",
        "- $l_t$ is written as $l_t = \\alpha \\ [y_t - S_{t-m}] + (1-\\alpha) \\ [l_{t-1} + b_{t-1}]$.\n",
        "- Here, $l_{t-1} + b_{t-1} = \\hat{y} - S_{t-m}$.\n",
        "- Additionally, $l_{t-1} + b_{t-1}$ and  $y_t - S_{t-m}$ tries to subtract the seasonality effect.\n",
        "    <img src='https://drive.google.com/uc?id=1LMjli6SpWPqRRP5lnFvFUR5_uxphMRS9'>\n",
        "- Trend term is expressed as $b_t = \\beta (l_t - l_{t-1}) \\ + \\ (1-\\beta)*b_{t-1}$.\n",
        "- Here, $(l_t - l_{t-1}) \\to \\ current \\ slope \\ and \\ b_{t-1} \\to \\ previous \\ slope$.\n",
        "- Seasonality term is expressed as $S_t = \\gamma \\ [y_t - l_{t-1} - b_{t-1}] + (1-\\gamma) \\ [S_{t-m}]$.\n",
        "- $y_t - l_{t-1} - b_{t-1}$ is the current seasonality after removing lag term and trend term and $S_{t-m}$ is past seasonality.\n",
        "\n",
        "*Note:*\n",
        "\n",
        "$\\alpha, \\beta, \\gamma$ lies between 0 and 1 i.e. $0<\\alpha,\\beta,\\gamma<1$\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZrvZ_mN5zNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### $\\alpha$, $\\beta$ and $\\gamma$ values:"
      ],
      "metadata": {
        "id": "FyqmSk7A6MOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suggested value of $\\alpha$ is:\n",
        "$$\\alpha_{suggested} = \\frac{1}{2*seasonality}$$\n",
        "- There are some heuristics that decide value of $\\beta$ and $\\gamma$ using historical data."
      ],
      "metadata": {
        "id": "21dEUbMR6MOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TES on entire Mobile Sales data:"
      ],
      "metadata": {
        "id": "9dpH2_Xz5zNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we do triple exponential smoothing on whole data, we see the following output (Blue line is TES result).\n",
        "    <img src='https://drive.google.com/uc?id=1QGVWXYmJzcv_m2TsDDC-205fXoJLxwAc'>\n",
        "- It capture trend and seasonality very well.\n",
        "- If we give the entire mobile sales data and predict for future 12 months, we could see that trend and seasonality is captured very well.\n",
        "    <img src='https://drive.google.com/uc?id=1c67L9IT3Vf3hCoFFpR9Rh74k9v-3DH9V'>    \n",
        "    "
      ],
      "metadata": {
        "id": "US95GVlG5zNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TES performance on Train-Test split:"
      ],
      "metadata": {
        "id": "mHOElow-AKoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TES model trains on training dataset and makes prediction for next 12 months.\n",
        "    <img src='https://drive.google.com/uc?id=1QHk9uriFLOzUsUtyGJB89gJRuA7aGoRR'>    \n",
        "- Here, TES tries to captures trend and seasonality very well.\n",
        "- Peaks and troughs are captured very well. Sales in Jan dip and this is captured by TES. MAPE is $6.8\\%$.\n"
      ],
      "metadata": {
        "id": "txxf3WKD5zNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixture of Additive and Multiplicative Model."
      ],
      "metadata": {
        "id": "e-B_0Sz4Bg-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trend and Seasonality could be additive or multiplicative."
      ],
      "metadata": {
        "id": "dA_iWoyHBkDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trend $\\to$ Additive and Seasonality $\\to$ Multiplicative:"
      ],
      "metadata": {
        "id": "ppEg4m2kB37i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $\\hat{y}_{t+1} = (l_t + b_t) * S_t$.\n",
        "- MAPE is $6.8\\%$\n",
        "<img src='https://drive.google.com/uc?id=1WosSGNhFrjyKtVb_big65tTFoe5aHSx9'>   \n",
        "\n"
      ],
      "metadata": {
        "id": "R-Xinqg6CDEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trend $\\to$ Multiplicative and Seasonality $\\to$ Additive:"
      ],
      "metadata": {
        "id": "6FJrFAztCgaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $\\hat{y}_{t+1} = (l_t + S_{t+1-m}) * b_t$.\n",
        "- MAPE is $5.5\\%$\n",
        "<img src='https://drive.google.com/uc?id=1qR0s-dI4tg7rrrd_G5nITFLICu8Tv7RW'>   "
      ],
      "metadata": {
        "id": "4jlAhL-ZCgaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What is the best model to predict monthly total population of a country? [Pre-covid]\n",
        "1.   SES\n",
        "2.   DES\n",
        "3.   TES\n",
        "4.   Moving Avergae\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "Practically, population follows a certain trend and there is no seasonality.. Hence, DES could be a good option."
      ],
      "metadata": {
        "id": "k708uAAADrt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Recap:"
      ],
      "metadata": {
        "id": "XnNQ1bM4ET6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We did data analysis on Time Series using ACF/PACF.\n",
        "- We saw techniques to detect outliers and imputation techniques.\n",
        "<img src='https://drive.google.com/uc?id=1x7oYnQERP7fXcVRuD2stCKU726oDD4EB'>   \n",
        "- We looked at Time Series foundational models like Naive models, SES, DES, TES."
      ],
      "metadata": {
        "id": "YaamMMgLEWuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complex Models:"
      ],
      "metadata": {
        "id": "adssyVHeFYBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Notations:"
      ],
      "metadata": {
        "id": "Pn_WQp3EGkx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's discuss some of the notations to used.\n",
        "<img src='https://drive.google.com/uc?id=16Dw0LfcShg0PRqAq4raGj1AwLZrmKlbD'>   \n",
        "- $y_0, y_1, ..... y_t,y_{t+1} ..... \\to$ observations.\n",
        "- $..... \\hat{y}_{t-1},\\hat{y}_{1}, \\hat{y}_{t+1} ..... \\to$ Predictions.\n",
        "- $..... \\epsilon_{t-1}, \\epsilon_{t}  ..... \\to$ Error terms, where $\\epsilon_{t} = y_t - \\hat{y}_t$\n",
        "- $\\mu \\to$ mean values of all $y_i's$."
      ],
      "metadata": {
        "id": "yaTmXX8IGnp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Auto Regressive model AR($p$):"
      ],
      "metadata": {
        "id": "d3LrHT30IuUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "MlHCUaXZLkgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ideally $y_t$ is computed as:\n",
        "$$y_t = \\mu + \\alpha_{1} \\ y_{t-1} + \\alpha_{2} \\ y_{t-2} +....+ \\alpha_{p} \\ y_{t-p} + \\epsilon_{t}$$\n",
        "<img src='https://drive.google.com/uc?id=1UnNF_ZXGv6KbRnZ-wSAaGoB8yeSDyYxJ'>  \n",
        "- Here $\\mu$ is intercept, $\\alpha_{1}, \\alpha_{2}.. \\alpha_{p}$ are weights.\n",
        "- $\\epsilon_{t}$ is error term and $\\mu + \\alpha_{1} \\ y_{t-1} + \\alpha_{2} \\ y_{t-2} +....+ \\alpha_{p} \\ y_{t-p}$ is $\\hat{y}$.\n",
        "- In this model $\\alpha_{1}, \\alpha_{2}.. \\alpha_{p}$ and $\\mu$ are parameters and $p$ is hyper-parameter."
      ],
      "metadata": {
        "id": "WcDKc8mUIuVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Comparison of SES and AR($p$):"
      ],
      "metadata": {
        "id": "jfiBYlg9Lqc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In SES, weighted average is exponential and determined using $\\alpha$. Here, $\\alpha$ is hyper-parameter.\n",
        "- In AR($p$), weighted average is learnt and $p$ is hyper-parameter.\n",
        "<img src='https://drive.google.com/uc?id=1qJm3g_7xoQ_LMnWBYWfVH7RYHD_1KEXF'>  \n"
      ],
      "metadata": {
        "id": "yD94YowILSUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train/Test data setup for AR($p$):"
      ],
      "metadata": {
        "id": "75qTJxs6M3Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, we have $y_0....y_t$. From this we construct $x_i's$ and $y_i's$.\n",
        "<img src='https://drive.google.com/uc?id=1f2lmcEZ9X3WXfYW8R9VwC_mXsdg_NZMa'>  \n",
        "- If we have 1000 datapoints and $p$ = 10, then we can prepare 990 rows of data."
      ],
      "metadata": {
        "id": "eToVLOxOM3Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moving Average MA($q$):"
      ],
      "metadata": {
        "id": "4_5X5Zd4OYYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving Average is Linear Regression model of last $q$ errors."
      ],
      "metadata": {
        "id": "jWeXeNSMPnIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "hKyMmhFoOYZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ideally $y_t$ is computed as:\n",
        "$$y_t = \\epsilon_{t} + \\mu + \\theta_{1} \\ \\epsilon_{t-1} + \\theta_{2} \\ \\epsilon_{t-2} +....+ \\theta_{q} \\ \\epsilon_{t-q}$$\n",
        "<img src='https://drive.google.com/uc?id=1Ej_CHWCSsPZntKx0lITxlWs3prYAZBkZ'>  \n",
        "- Here $\\mu$ is average of all $y_i's$ in train Time Series.\n",
        "- $\\epsilon_{t-i} = y_{t-i} - \\hat{y}_{t-i}$."
      ],
      "metadata": {
        "id": "3EIlSKV8OYZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Auto Regressive Moving Average MA($p,q$):"
      ],
      "metadata": {
        "id": "TFPalc4QQYS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, we combine AR($p$) and MA($q$). This is ARMA model."
      ],
      "metadata": {
        "id": "DyWZGB0hQYS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mathematical Representation:"
      ],
      "metadata": {
        "id": "GYYdfV4bQYS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ideally $y_t$ is computed as:\n",
        "$$y_t = \\epsilon_{t} + \\mu + (\\sum_{i=1}^p \\alpha_{i} y_{t-i}) + (\\sum_{j=1}^q \\theta_{j} \\epsilon_{t-j}) $$\n",
        "<img src='https://drive.google.com/uc?id=1eBi_xo27BhV8TrovmywVrt6ydh4pf2bI'>\n",
        "- Here, $\\alpha_{i}$ and $\\theta_{j}$ are parameters where $p \\ \\alpha_{i}$ are doing auto regression using last $p$ values and I have a simple average model on errors."
      ],
      "metadata": {
        "id": "wEDL2pnqQYS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big Picture:"
      ],
      "metadata": {
        "id": "i9uosK3XSSNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will look at following models in future lectures.\n",
        "<img src='https://drive.google.com/uc?id=1CNLIGcKGYM-J14SGbkTdtUW4LK57nOa7'>\n",
        "- ARIMA models uses integration to get back trend.\n",
        "- SARIMA models adds seasonality to ARIMA model.\n",
        "- SARIMAX uses exogenous/external variables to SARIMA model."
      ],
      "metadata": {
        "id": "44-JsspJSXIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Note*"
      ],
      "metadata": {
        "id": "BiolflsTfvuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following topics will be covered in next lecture:\n",
        "\n",
        "- Code for AR($p$), MA($q$), ARMA.\n",
        "- ARIMA model.\n",
        "- SARIMA model.\n",
        "- SARIMAX model."
      ],
      "metadata": {
        "id": "6lClAUcPf6Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9ex6ukU89WE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}