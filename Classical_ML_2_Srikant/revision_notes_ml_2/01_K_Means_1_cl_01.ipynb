{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering - 1"
      ],
      "metadata": {
        "id": "l-etnaBCqaE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- Setting up the Context: WHy do we need Clustering?\n",
        "- Intuition: What is Clustering?\n",
        "- Metrics for Clustering\n",
        "- K-means Clustering and its Variations\n",
        "- K-Means: Mathematical Formulation: Objective Function\n",
        "- Lloyd’s Algorithm\n",
        "- Some Problems with Lloyd's Algorithm\n",
        "\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "gpBpJ30oFXq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Context: Why do we need Clustering?\n",
        "\n",
        "- So far we have seen classification and regression problems.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1V8D6nffRjwCGGWzT62ecQkKC-2Pdyxm-'>\n",
        "\n",
        "\n",
        "- For a given dataset $D = {x_i, y_i}$\n",
        "  - If $y_i∈{0,1}$, then we call it a **2-class classification**\n",
        "  - If $y_i∈R$, then we call it a **regression**.\n",
        "\n",
        "- **In the case of both classification and regression**, we are trying to find a function that is used to predict ‘$y_i$’, when ‘$x_i$’ is given as the input. Both of these are **supervised learning problems**.\n",
        "- **But in case of clustering**, we are given a set of points $D = {x_i}$ and there are no $y_i$’s. The task in clustering is to group similar points. This makes clustering an **unsupervised learning problem**.\n"
      ],
      "metadata": {
        "id": "alEbTXRgIXN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's understand it further with some examples\n",
        "\n",
        "**1. We are given data of 100 million Amazon customers**\n",
        "- For these customers, we are only given $x_i$'s\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16dMAQDZLgTSBA25wWHswVceCAe7Uo-8h'>\n",
        "\n",
        "#### **Can we group similar customers?**\n",
        "- In E-commerce, companies group similar customers based on their purchasing behaviour. Here the similarity maybe based upon the type of products they purchase, or the type of the debit/credit cards they use, or their geo location, etc.\n",
        "- Once these customers are grouped into different clusters, then depending on their purchasing habits, different deals, discounts are offered.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1PWsO4MU2riD_Q354JbB2uyeWPLg1YcsZ'>"
      ],
      "metadata": {
        "id": "IKiCBMF8vHGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Given 1,00,000 words, can you cluster similar words?**\n",
        "\n",
        "**3. Given 1 million newspaper articles, can you group together similar articles?**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1spBOwCTO5QWry9pbtOKzc42f73i8loHb'>\n",
        "\n",
        "- Clustering is essentially an **unsupervised learning problem**, i.e., We do not have $y_i$’s.\n",
        "\n",
        "- The dataset for unsupervised learning is represented as $D = \\{x_i\\}_{i=1}^n$"
      ],
      "metadata": {
        "id": "g5MgXACUyKiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are other types of learning problems as well**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1eBAh1yPPc5cMZai-XEtT_BFSR9UJC-10'>\n",
        "\n",
        "\n",
        "- In **semi-supervised learning**, we have a majority of the points without\n",
        "labels and a very few points with the labels. This happens when the cost of labelling is expensive.\n",
        "- Here for a few points, we have the values for ‘$y_i$’, and for most of the points, we do not have the values for ‘$y_i$’. This is in-between supervised and unsupervised, and hence we call it semi-supervised.\n",
        "\n",
        "- There is another type of problems called **self-supervised learning**. It has a special application when it comes to images, where we use the image data itself to learn more properties or features about the images. We'll get to it later during the course."
      ],
      "metadata": {
        "id": "aWFyGTn5IxUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "lFbfcp9LkfRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition: What is Clustering?\n",
        "\n",
        "- Intuitively, It is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than those in other groups.\n",
        "- Each group here is called a **cluster**. The points in the same cluster\n",
        "are close together. The points in different clusters are far away from each\n",
        "other.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1B7owbWs8CJYh2wgosnnB6XfGtLzYuTlX'>\n",
        "\n",
        "- So, the task in clustering is **grouping the similar points**.\n",
        "\n",
        "- We perform clustering when we want to group similar items in our dataset based on **our definition of similarity**.\n",
        "\n",
        "- Since **clustering doesn’t have the class labels or the ground truth**, it is hard to measure if a clustering is good or bad in a very rigorous and critical way.\n",
        "\n",
        "- It all depends on the problem and the context we are working in, i.e., the **Business Case**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vu6eMV4qdF0k5UdB9Jp1JaRlG88j5bJC'>\n",
        "\n",
        "**Clustering, just like classification, is very dependent on the features used.**\n",
        "\n",
        "- The **clustering output can sometimes help us come up with new features**.\n",
        "\n",
        "- If we get a **new datapoint** in future, **for example, a new customer of Amazon**, we can identify which cluster or group does this new datapoint or this new customer belongs to.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ZbIuq61woMWP-uVvzCoa2lrzc9nI7lFj' width=\"500\">\n"
      ],
      "metadata": {
        "id": "B6eWAcWR1O2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "zzuBOJaIkjDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics for Clustering\n",
        "\n",
        "**What metric is then used to evaluate whether the clustering model is giving accurate results?**\n",
        "\n",
        "- The resulting clusters should make **business sense**\n",
        "\n",
        "- To evaluate the performance of a clustering algorithm, we can use the same metrics we used for supervised learning problems, like Euclidean Distance, Manhattan Distance, Cosine Similarity, etc.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YJCq1u0yXn3BVBd2v7T1OlZtXmlwqJMy'>\n",
        "\n",
        "- There is another metric called DUNN-Index which will be explained in some time.\n",
        "\n",
        "- For clustering, the dataset is denoted as $D = \\{x_i\\}$.\n",
        "\n",
        "- So far in the classification and the regression tasks, we had $y_i$’s, but here in clustering, we do not have $y_i$’s. **All the performance metrics in Classification and Regression need $y_i$’s**. So as all the performance metrics known to us so far require the ‘$y_i$’ values, those metrics do not work in clustering.\n",
        "\n",
        "**Intra and Inter-Cluster Points**\n",
        "- If a group of points lie in the same cluster, we call it **Intra-cluster**, and if a group of points are spread across different clusters, then we call it **Inter-cluster**.\n",
        "\n",
        "- All the points from the given data set are grouped in such a way that the intra-cluster distances are small and inter-cluster distances are high.\n",
        "\n",
        "- The core idea/basis of the measure of clustering is the **intra-cluster\n",
        "distance has to be low and the inter-cluster distance has to be high**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_Ow-GdD96AyhqJn_qzEBYrnk5iej2B6Q'>"
      ],
      "metadata": {
        "id": "eoSbrM7dRunN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dunn Index**\n",
        "- It is denoted by **‘D’** and is given as:\n",
        "\n",
        "$D = \\frac{min_{i,j} distance(i,j)}{max_k distance^{|}(k)}$\n",
        "\n",
        "Where\n",
        "- distance(i,j) → distance between the farthest points of the clusters ‘$C_i$’ and ‘$C_j$’ → **Inter-Cluster distance**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YZndvns6oIZqKxnOEU4uxquvWozCs87s'>\n",
        "\n",
        "- distance^{|}(k) → **Intra-Cluster distance**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-apq41uFx5Yl5YpLq-_-Atz5moZlJ7qk'>\n",
        "\n",
        "- If ‘$D$’ is high, it implies good clustering. For every pair of points from ‘$C_i$’ and ‘$C_j$’, we have to compute **$distance(i,j)$**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1K4FX--I0QDIROe0BmUDdzImMbkjOeKLK'>\n",
        "\n",
        "- **For ideal clusters, the value of the Dunn Index should be high**\n",
        "\n",
        "- For this, the distances between the points in the same cluster should be as much small as possible, and the distance between the different clusters should be as much large as possible.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_b1r30YLuOgF2DItLJnNKBHto0aEhqXf' width='500'>\n",
        "\n",
        "- If we have two Clustering algorithms and we need to decided which one is performing better, we can compare the Dunn Index of the two algorithms and pick the one with higher Dunn Index.\n",
        "\n",
        "- Dunn Index takes into consideration the **minimum Inter-Cluster distance** and the **maximum Intra-Cluster distance** - so in both cases it considers the worst-case scenario.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1NK1JK_tTiBBoCRRwBI4vX1Tuktqwam_J'>\n"
      ],
      "metadata": {
        "id": "tuqkMoRqiyIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "S95P0Z7ckory"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "- K-Means clustering is one of the popular and the simplest clustering\n",
        "algorithms. The value **‘K’ in the K-means algorithm denotes the number of\n",
        "clusters**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1rKN23CFKliCrrpqDyc2PUP3zb74Tjktv'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wNEivXxtrHXqbm9GyqCiyFPEwBWkmO2V'>\n",
        "\n"
      ],
      "metadata": {
        "id": "_Y_j51FQ5_Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let us assume we are given a 2-Dimensional dataset **‘D’** with number\n",
        "of clusters (K) = 3. So the number of centroids is also equal to 3.\n",
        "- Let ‘S1’, ‘S2’, ‘S3’ be different sets of elements, and ‘C1’, ‘C2’ and ‘C3’ be their respective centroids.\n",
        "\n",
        "  $S_1 ∪ S_2 ∪ S_3 = D$\n",
        "\n",
        "  $S_1 ∩ S_2 ∩ S_3 = Φ$\n",
        "\n",
        "  $S_1 ∩ S_2 = Φ, S_1 ∩ S_3 = Φ, S_2 ∩ S_3 = Φ$\n",
        "\n",
        "- Here all the data points belong to one or the other set and no data point exists in more than one set.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1WOKeZ64-KNsVKDWA71mJ4Gvk8WzW_QpO'>\n",
        "\n",
        "Number of Sets = K (i.e., $S_1, S_2, S_3, ….., S_k$)\n",
        "\n",
        "Number of Clusters = K (i.e., $C_1, C_2, C_3, ….., C_k$)\n",
        "\n",
        "For any set ‘$S_i$’, its centroid is given as $C_i = (1/n) * Σ_{x_j∈Si} x_j$\n",
        "\n",
        "**K-means clustering is a centroid based clustering scheme**.\n",
        "\n",
        "- Every point is assigned to the cluster closer to it.\n",
        "- The core idea of K-means clustering is to find ‘K’ centroids and each point is assigned to the cluster whose centroid is nearer to it. The biggest challenge is to find these 'K' centroids.\n",
        "- There are algorithms to find out these 'K' centroids and one of the most commonly used algorithms is **Lloyd's algorithm**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wN2P20hwzhHlk02lmq-a_HTIdKV0Cz5X'>\n"
      ],
      "metadata": {
        "id": "1h6t1NX1zEso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "mGikZh1DksdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means: Mathematical Formulation: Objective Function\n",
        "\n",
        "So far we were given a dataset ‘**D**’ of ‘**n**’ data points and our job is to find the ‘**K**’ centroids.\n",
        "\n",
        "$D = \\{x_i\\}_{i=1}^n$\n",
        "\n",
        "The ‘**K**’ sets are $‘S_1’, ‘S_2’, ‘S_3’, …., ‘S_k$’\n",
        "\n",
        "The ‘K’ centroids are $‘C_1’, ‘C_2’, ‘C_3’, …., ‘C_k’$\n",
        "\n",
        "The **objective function for K-means algorithm** is given as:\n",
        "\n",
        "$arg-min_{C_1,C_2,C_3,..,C_k} Σ_{i=1}^k Σ_{x∈S_i} ||x-C_i||^2$,\n",
        "such that $x∈S_i$ and $S_i⋂S_j=∅$\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ni3v4x0PaViRZ5BSN32g05fgRI__uXhr'>\n",
        "\n",
        "- We have to find the cluster centroids such that the points belonging to the respective clusters should be as much nearer as possible to these entroids, so that the **intra-cluster distance is minimized**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1UZk5Ff3pa-Bsm4eQ_jWXVk-PnbdJIBAG'>\n",
        "\n"
      ],
      "metadata": {
        "id": "MDAoPPA_OTuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Can we use Gradiant Descent to converge the K-Means Clustering Algorithm?**\n",
        "\n",
        "- $S_{ij}$ has to be either 1 or 0, based on whether $x_j∈S_i$ or not.\n",
        "\n",
        "- It cannot be a fraction, so GD does not work.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1twpYopy7UrQeHj43JewLfXR0H8UjxYg5'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-tTlJn_6e7mfx7tsi4ZeVW33ZL34WmzQ'>\n",
        "\n",
        "- This optimization problem is very hard to solve from the point of computation. In such cases, we go with the approximation algorithms, and find out the approximate solution for this problem, but not the exact solution, using a few hacks. One such approximation algorithm is **Lloyd’s algorithm**. It is a very simple and a good approximation algorithm.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1eYt48azC1cTA5Ls4SEXfWk1WDP4hWKrt'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U-4RlW4yQcH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "T3ThwmZSkwRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Algorithm (Lloyd’s Algorithm)\n",
        "**1. Initialization**\n",
        "\n",
        "From the given dataset ‘**D**’, we have to pick ‘**K**’ points randomly, and assume them to be the centroids. Let us denote them as $C_1, C_2, C_3, …, C_k$.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1hv9_bd-31sy9XO3lA0ri21h4Gdnf75ji'>\n",
        "\n",
        "**2. Assignment**\n",
        "\n",
        "For each point ‘$x_i$’ in the dataset ‘**D**’, we have to compute the distance of each of the above ‘K’ centroids from this point, and pick the nearest centroid. Let us denote this nearest centroid as ‘$C_j$’.\n",
        "\n",
        "Add the point ‘$x_i$’ to the set ‘$S_i$’(which is associated with the centroid ‘$C_j$’).\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Mp6n0AjrHSZHIAMAgaBv7FAoMKpX_lML'>\n",
        "\n",
        "**3. Recompute Centroid (Update Stage)**\n",
        "\n",
        "Recalculate/update ‘$C_j$’ as follows:\n",
        "\n",
        "$C_j = (1/|S_j|) * Σ_{x_i∈S_j} x_i$\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1bix-ArmjCvesifVBHRs2HKLBZM6Pl-4s'>\n",
        "\n",
        "**4. Repeat the steps 2 and 3 until convergence**. Here convergence is the stage where the centroids do not change much.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ddspk-Zef4qZNyopCY2Aei2EAr60q_fY'>\n",
        "\n",
        "For example, at the end of stage 2, if the centroids are $\\{C_1, C_2, C_3, …., C_k\\}$ and at the end of stage 3, if the centroids are $\\{C_1^|, C_2^|, C_3^|, ….., C_k^|\\}$, then during convergence the distance between the old and\n",
        "the new centroids is very small.\n",
        "\n",
        "(i.e., $C_1 - C_1^|, C_2 - C_2^|, C_3 - C_3^|, …, C_k - C_k^|$ has to be very small)\n",
        "\n",
        "Now finally the centroids we get are $C_1, C_2, C_3, …., C_k$ and the final sets/clusters of points are $S_1, S_2, S_3, …., S_k$.\n",
        "\n"
      ],
      "metadata": {
        "id": "tDT98w5n8_uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "ydAs6vm3kzpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Problems with Lloyd's Algorithm\n",
        "\n",
        "- It is **Initialisation Sensitive**\n",
        "\n",
        "  Initialization Sensitivity means the final clusters and centroids depend on how randomly we pick the points as centroids during the initialization. Differences in initialization of the centroids results in differences in clustering.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Le4Y1qo31VhDx7BbkNv3IhfGtZpIjREI'>\n",
        "\n"
      ],
      "metadata": {
        "id": "MSlBJHkkKRpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "B9Xiukd_k3jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Notes\n",
        "\n",
        "- In next lecture, we'll study this problem in detail and techniques available as a work around to these limitations of Llyod's Algorithm.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Pn6-nU0VGJW7zrzrUaH7kGovhBXWSQvJ'>"
      ],
      "metadata": {
        "id": "D6fTEbGJk5NS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRYZiDr3lKBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}