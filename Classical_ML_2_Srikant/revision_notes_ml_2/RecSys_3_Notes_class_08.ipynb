{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on August 10, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/33487/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "1.   Recap of Matrix Factorization (MF) for Recommender Systems (RecSys).\n",
        "2.   Non-Negative Matrix Factorization (NMF).\n",
        "3.   Netflix-Prize Solution.  \n",
        "4.   Code: RecSys.\n",
        "5.   MF for feature Engineering:\n",
        "    - Text.\n",
        "    - Images.\n",
        "    - Entities.\n",
        "7.   Market-Basket Analysis (ARM, Apriori Algorithm).   "
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recap: MF for RecSys\n"
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Core idea of MF in RecSys:"
      ],
      "metadata": {
        "id": "OP_I89NFaIBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine we have a ratings matrix $A_{n*m}$ where $n \\to users \\ and \\ m \\to items$. We broke it down into product of 2 metrices $B_{n*d} \\ and \\ C_{d*m}$.\n",
        "    <img src='https://drive.google.com/uc?id=1MuyxTUaHJNNpDQH0hpuhgfDgi5MZKGRg'>\n",
        "- We have $1....n \\ users$, and $B_i$ = $U_i^T$ $\\rightarrow$ d-dimensional row vector for user $U_i$ and $U_i \\in \\mathbb{R}^d$. Similarly, we have $1....m \\ items$ where $C_j$ $\\rightarrow$ d-dimensional column vector for item $I_j$ and $C_j \\in \\mathbb{R}^d$."
      ],
      "metadata": {
        "id": "_s-637WcWUHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimization Problem:"
      ],
      "metadata": {
        "id": "NKvrknotaPK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, we try to summate over all pairs of $i \\ and \\ j$ such that $A_{ij} \\neq NULL$ (i.e. we have some ratings). We minimize the function by finding $B_i$ and $C_j$ using Stochastic Gradient Descent (SGD) or Coordinate Descent Algorithm.\n",
        "    <img src='https://drive.google.com/uc?id=1vwF4hYOcYPtOpdSRIGLOKEmD1_Na-vsQ'>\n",
        "- Note, in the above equation we do not have parameter '$d$'. It is treated as hyperparameter. It id determined using Train/test split on $A_{ij}'s \\neq NULL$ or using elbow method discussed in the previous class."
      ],
      "metadata": {
        "id": "_lOIuKdyWmfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA and SVD as special case of MF:"
      ],
      "metadata": {
        "id": "Gyezm6TpaTT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have $X_{n*d}$ (Standardized data) and its Covariance Matrix Cov(X) = $S_{d*d}$ which is computed as $X_{d*n}^T.X_{n*d}$.\n",
        "    <img src='https://drive.google.com/uc?id=1bcI2y-mjPp-QyPOSVRKvq0c-S7zsACqI'>\n",
        "- We compute Eigen values and Eigen vectors of S as $S_{d*d} = W_{d*d}$.$\\Lambda_{d*d}$.$W_{d*d}^T$\n",
        "- Here, we decomposed S into 3 matrices such that:\n",
        "    - $\\Lambda$ is a diagonal matrix of Eigen values.\n",
        "    - $W$ contains column vectors $V_1, V_2...V_d$ as Eigen Vectors which are Orthogonal to each other.\n",
        "- PCA works on Square symmetric matrix.\n",
        "\n",
        "In SVD, we decompose the data $X_{d*d}$ = $U_{n*n}.\\Sigma_{n*d}.V_{d*d}^T$ where:\n",
        "- $\\Sigma_{n*d}$ is a diagonal rectangular matrix that has singular values $S_1, S_2...S_d$ and $S_i = \\sqrt{\\lambda_i}$\n",
        "- $U_{n*n}$ contains eigen vectors where $U_i = i^{th}$ eigen vector of $X_{n*d}.X_{d*n}^T$.\n",
        "- $V_{d*d}^T$ contains eigen vectors where $V_i = i^{th}$ eigen vector of $X_{d*n}^T.X_{n*d} = S_{d*d}$.\n",
        "- SVD works on square and rectangular matrix as well."
      ],
      "metadata": {
        "id": "EzN7M4j_YTfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-Means clustering as special case of MF:\n"
      ],
      "metadata": {
        "id": "4_tO__z_bH2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a dataset $X_{n*d}$ and we can write $X$ as approximately equal to product of 2 metrices $Z_{n*k}$ and $C_{k*d}$ where:\n",
        "- $Z_{n*k}$ is the cluster assignment matrix with some constraints.\n",
        "- $C_{k*d}$ is the cluster centroid matrix.\n",
        "<img src='https://drive.google.com/uc?id=12KWi4jwhjtXcOEhodeX4Y2LEtQFDHXgi'>\n",
        "\n",
        "*Note:* $Z^T.C$ and $Z.C^T$ can be used interchangeably as long as we are representing it in correct manner."
      ],
      "metadata": {
        "id": "QYgvLsbEbPAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Like we have multiple factors for big natural numbers, can we similarly decompose matrix $A_{ij}$ into 4 metrices or more? Why are we always limiting it to 3?\n",
        "\n",
        "***Answer***: Yes, we can decompose into more metrices as well. But in most applications related to Data Science / Machine Learning we do not decompose it into 4 metrices or more. This might be the case in some other fields of Applied Sciences."
      ],
      "metadata": {
        "id": "pgihIvlFZve7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-negative Matrix Factorization (NMF):\n"
      ],
      "metadata": {
        "id": "-XvqLg50lSuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intuition:"
      ],
      "metadata": {
        "id": "HKqAGKCDdpV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suppose, we have matrix $A_{n*m}$ and we are trying to decompose it into product of 2 metrices $B_{n*d}$ and $C_{d*m}$.\n",
        "- What if, we want to keep the components of $B_{ij}$ and $C_{ij}$ as non-negative i.e. $\\ge 0$. Then, this constraint enables non-negative Matrix Factorization.\n",
        "<img src='https://drive.google.com/uc?id=1e-0RbxXgq9XqefwAdcgcv47M1vwEQXjY'>\n",
        "- In K-means, the cluster assignment matrix has following constraints:\n",
        "    - $Z_{ij}$ = 1 or 0.\n",
        "    - $\\overset{k}{\\underset{j=1}{\\Sigma}} Z_{ij} = 1$\n",
        "- If we have such non-negative constraints on $B_{ij}$ or $C_{ij}$ or both, then we can think of NMF as generalization of Clustering.\n",
        "- In fact, MF $\\supseteq$ NMF $\\supseteq$ K-means(Hard/Soft)."
      ],
      "metadata": {
        "id": "uGV0l5yzdUVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why do we need it?"
      ],
      "metadata": {
        "id": "uSLUvwNd5CNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Often times, we want to vizualize data like images. Images are represented as color image (RGB) or Greyscale image where 0 $\\to$ Black and 255 $\\to$ White.\n",
        "- In Machine Learning, we use MinMax Scaling on these images. (We could do Standard Scaling i.e. Mean centering and variance scaling as well).\n",
        "- If we carry out Matrix Factorization on image data and we want to vizualize the result, then we want it to be non-negative.\n",
        "- NMF basically says that if we want to decompose matrix $A$ into product of 2 metrices $B$ and $C$ such that $B_{ij} \\ge 0 \\ \\forall_{i,j}$ and $C_{ij} \\ge 0 \\ \\forall_{i,j}$, then such constraints helps us to interpret results better.\n",
        "- Eigen-Faces is a good examples of NMF.\n"
      ],
      "metadata": {
        "id": "YSqdPaDwgqRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Is NMF always necessary?"
      ],
      "metadata": {
        "id": "in_lHzFEjKeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We want Non-negative factors for interpretability. But its not always necessary.\n",
        "- For e.g. if we want to build a simple RecSys we don't care what the $B_i$ and $C_j$ are as long as $A_{ij}$ are approximated very well.\n",
        "- But there are some case like images/clustering setup where NMF is helpful.\n",
        "<img src='https://drive.google.com/uc?id=1OcFfN_QOsvGMfgShBxHGpM7KUj9PeZqP'>\n"
      ],
      "metadata": {
        "id": "FmYiGR-mjRTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecSys Interview Scenario 1:"
      ],
      "metadata": {
        "id": "cPAjRbzTly-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's say we have rating matrix $A_{n*m}$ where $n \\to users$ and $m \\to items$ in product base company like Youtube.\n",
        "<img src='https://drive.google.com/uc?id=1Ea7QW-R14o1E58SdK-0ZSqApiH2iP7ja'>\n",
        "- We have 'Likes' matrix: $L_{n*m}$, 'Watched' matrix: $W_{n*m}$ and 'Disliked'  matrix: $D_{n*m}$.\n",
        "- Given this data, how to build a Recommender System, where we want to recommend user $U_i$ $\\to$ some videos $V_j$.\n",
        "\n",
        "**Option 1:**\n",
        "- Give higher postive weightage to 'Likes' matrix, lower positive weightage to 'Watched' matrix and negative weightage to 'Disliked' matrix and build a new matrix $A_{n*m}$.\n",
        "- This $A_{n*m}$ will be decomposed into $U_i$ and $V_j$.\n",
        "\n",
        "**Option 2:**\n",
        "- Independent factorization, wherein we decompose:\n",
        "    - $L_{n*m}$ matrix into $U_i^L$ and $V_j^L$.\n",
        "    - $W_{n*m}$ matrix into $U_i^W$ and $V_j^W$.\n",
        "    - $D_{n*m}$ matrix into $U_i^D$ and $V_j^D$.\n",
        "<img src='https://drive.google.com/uc?id=10pUhAc7MZ5txcMW9q74m2MU-oYVuV1UA'>\n",
        "\n",
        "In both the above options, we can use cosine similarity and find which vidoes have propensity to be liked and prefer suggesting such vidoes and avoid vidoes that have propensity to be disliked.\n",
        "\n",
        "- Suppose we recommend video $V_{10}$ to user $U_100$ using Matrix completion like strategy. $V_{10}$ will be very similar to other videos liked by user $U_{100}$.\n",
        "- If $L_{100,10}$ is empty, then we will get this data using $U_{100}^T.V_{10}$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ORYL8UhOl8j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we also make clusters of similar users and then find similar videos which have been liked by another users from the same cluster?\n",
        "\n",
        "***Answer:*** Yes we can.\n",
        "- We have $U_i^L \\in \\mathbb{R}^d$, a d-dimensional representation of users based on the liked vidoes. Similarly, we have $U_i^W \\in \\mathbb{R}^d$ and $U_i^D \\in \\mathbb{R}^d$.\n",
        "- We can build a clustering on top of this and vizualize using t-SNE or UMAP."
      ],
      "metadata": {
        "id": "w0xLRAD8sPqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we call this as probability of every user watching that video?\n",
        "\n",
        "***Answer:*** No. it would be slightly tricky as $L_{n*m} \\to \\ 0 \\ or \\ 1$.\n",
        "- When we do MF, we try to find values that are closer to 0 or 1. So, they need not be probability.\n"
      ],
      "metadata": {
        "id": "rkjWsdxctgO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Netflix-Prize Solution:"
      ],
      "metadata": {
        "id": "I22EnILCf9Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### About Paper:"
      ],
      "metadata": {
        "id": "r_eFDjm8yHIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- There is a very nice paper that discusses MF for RecSys written around 2008-09 by Yehuda Koren. This paper summarizes important ideas used in the prize winning solution.\n",
        "<img src='https://drive.google.com/uc?id=1cqfsBwvrEvKHAdEhdwKpjYXOEBABWDAD'>\n",
        "- $u_i$ represents $user_i$ and $m_j$ represents $movie_j$. Given $(U,M)$ predict the ratings between 0 to 5.\n",
        "- Metric used is RMSE.\n",
        "- Internal alogrithms at Netlfix gives certain RMSE and teams need to find solution that gives 10% improvement on existing RMSE.\n",
        "- Final solution has many models plus some bagging and boosting techniques.\n",
        "- Though this solution was never productionalize due to its complexity. But a lot of new research came out of this competition.\n",
        "\n",
        "https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\n",
        "\n",
        "*Note:*\n",
        "- This paper discusses important techniques used in the award winning solution.\n",
        "- The actual solution is discussed in below paper https://asset-pdf.scinapse.io/prod/54392637/54392637.pdf\n"
      ],
      "metadata": {
        "id": "WWOYBxudgxuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Notations used in Paper:"
      ],
      "metadata": {
        "id": "LkN5aCtlyUjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $D_{Tr}$ and $D_{Te}$: Training and Test dataset.\n",
        "<img src='https://drive.google.com/uc?id=1qoYszZBfMP2_2c9WlJMaEmnbaJ2f2G0f'>\n",
        "- Assume we have vector $p_u \\to$ d-dimensional representation of each user and vector $q_i \\to$ d-dimensional representation of $item_i$.\n",
        "- We want to minimize all $q_i$ and $p_u$ without any constraints by solving the following optimization problem:\n",
        "\n",
        "$$\\underset{q_i, p_u}{\\min} \\  \\underset{u,i}{\\Sigma} ( r_{ui} - q_i^T.p_u)^2$$"
      ],
      "metadata": {
        "id": "zjDSRxIeyYEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularization:"
      ],
      "metadata": {
        "id": "4qoMmxRFRGtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In any ML models, we want to regularize or else model will overfit.\n",
        "- So the above equation now becomes:\n",
        "\n",
        "$$\\underset{q_i, p_u}{\\min} \\  \\underset{u,i}{\\Sigma} ( r_{ui} - q_i^T.p_u)^2 + \\lambda(\\underset{i}{\\Sigma}||q_i||^2 + \\underset{i}{\\Sigma}||p_u||^2 )$$\n",
        "- This $2^{nd}$ component is standard regularization.\n",
        "- In paper, it is also represented as:\n",
        "    \n",
        "    <img src='https://drive.google.com/uc?id=1on4lzjj2cdAa9GVVbBjsWabuvXwscRml'>"
      ],
      "metadata": {
        "id": "t_4YnYv10tQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Whatever be the value of '$d$', we are actually making $q_i^Tp_u$ very near to $r_{ui}$ and this will be used to construct the null values of $r_{ui}$. So why we need to regularize?\n",
        "\n",
        "***Answer:*** Let say we have very high '$d$', then  q_i^T.p_u comes very close to $r_{ui}$. But we will overfit on training data and perform poorly on test data. Hence, we need to regularize."
      ],
      "metadata": {
        "id": "6oVb-dtpjV_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next level of Optimization:"
      ],
      "metadata": {
        "id": "HnRms4Qt3Mhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is a bias at the user level where certain users tend to give high/low ratings for all the movies they watch (user bias: $b_u$).\n",
        "- Similarly, popular movies like GodFather or Schindler's list will always have very high ratings (item bias: $b_i$).\n",
        "- Ratings given in Netflix ecosystem will be different than that given at Amazon Prime or IMDb (Global bias: $\\mu$).\n",
        "<img src='https://drive.google.com/uc?id=18eXJ1PtqwxlRsBZOWSg7XRB0GV6Sysk_'>\n",
        "\n",
        "So, the optimization problem was changed to:\n",
        "\n",
        "$$\\underset{\\underset{\\mu, \\ b_u, \\ b_i}{q_i, p_u}}{\\min} \\  \\underset{u,i}{\\Sigma} ( r_{ui} - \\mu - b_u - b_i - q_i^T.p_u)^2 + \\lambda(\\underset{i}{\\Sigma}||q_i||^2 + \\underset{i}{\\Sigma}||p_u||^2 + b_u^2 + b_i^2)$$\n",
        "\n",
        "***Note:*** Just like we determine $p_u$ and $q_i$ vectors, we will also find these scalars. Practically, these biases would be very close to respective average ratings. But we avoid keeping them as constants and make them as parameters of the optimization function.\n",
        "\n",
        "If you think from a regularization perspective:\n",
        "- q_i^T.p_u is a quadratic term.\n",
        "- $b_u and \\ b_i$ are linear term.\n",
        "- $\\mu$ is a constant.\n",
        "\n",
        "To summarize, $r_{ui}$ can be explained as combination of these biases and the interaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr-kdUDL3PxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Since bias is specific to movies, why should it be removed from all ratings?\n",
        "\n",
        "***Answer:*** Let's say, movie on an average is rated '2' by the users. Now if we want to find ratings for this movie by a particular user, then we need not adjust $q_i^T.p_u$ as the bias ($b_i$) will take care of it."
      ],
      "metadata": {
        "id": "RFHeAK9RHUXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why can't we compute $b_i, n_u \\ and \\ \\mu$ from given data but determine them by solving optimization problem?\n",
        "\n",
        "***Answer:*** Practically, these biases are close to average values from dataset. But we determine them as parameters due to certain scenarios. Let's say, an user gives an average rating '4' to all movies. But for one particular movie rating '1' is given. So we want $q_i^Tp_u$ to account for this particular user behaviour.\n"
      ],
      "metadata": {
        "id": "ANl_kQVbOyap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implicit Feedback:"
      ],
      "metadata": {
        "id": "JBwg_50Mj9H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Recommender systems can infer user preferences using implicit feedback, which indirectly\n",
        "reflects opinion by observing user behavior including purchase history, browsing history, search patterns or even mouse movements.\n",
        "- Implicit feedback usually denotes the presence or absence of an event, so it is typically represented by a densely filled matrix."
      ],
      "metadata": {
        "id": "OxKbFFt2Q0j-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temporal Dynamics:"
      ],
      "metadata": {
        "id": "LBY77NiOQ-CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- People liking movies or disliking them is a behaviour over time. Simple models do not capture user's behaviour over time.\n",
        "- Mathematically, we can start treating rating as a function over time: $\\hat{r}_{ui}(t) = \\mu + b_i(t) - b_u(t) + q_i^T.p_u(t)$ where:\n",
        "    - $b_i(t)$: Item rating that changes over time. There would be movies that people would be re-discovering and rating them highly.\n",
        "    - $b_u(t)$: User's taste that changes over time. For e.g. user earlier would be giving rating '3' for movies, but now give rating '4' to most of the movies.\n",
        "    - Similarly, item's vector representation ($q_i$) and user's vector representation ($p_u$) are also function of time.\n",
        "\n",
        "- So rather than modelling them as constant values across 10 years of data we have, we model them as time series model which changes over time. The vectors are not longer static functions but they change over time.\n",
        "- Now, we won't take the entire training data, but chunks of data and analyse how items behaviour is changing over time, how user behaviour is changing over time and how ($q_i$) and ($p_u$) changes over time.\n",
        "- Eco-system bias ($\\mu$) will be constant over time and the other biases $b_i, b_u \\ and \\ q_i^T.p_u$ will be time varing.\n",
        "- This yields in very high reduction in RMSE.\n",
        "\n",
        "***Note:*** Time-series model and forecasting will be discussed in future lectures.\n"
      ],
      "metadata": {
        "id": "R0SzgUzckJEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MF model's accuracy comparison for above mentioned techniques:"
      ],
      "metadata": {
        "id": "8q3YaAeQVyFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Temporal dynamics gives much better accuracy in comparison to plain MF, MF with biases and MF with implicit feedback.\n",
        "    <img src='https://drive.google.com/uc?id=1aDIQdUvN8FhGBvkI3HxAaHfcHtS1s05P'>\n",
        "\n",
        "*Source:* https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\n"
      ],
      "metadata": {
        "id": "TIQ9dYoYmXqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecSys Interview Scenario 2:"
      ],
      "metadata": {
        "id": "q-zAvv-1W9iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine your are currently working at Youtube. We have access to historical data of users. Now, how do we build temporal (Time-Series) Recommender System?\n",
        "<img src='https://drive.google.com/uc?id=1CsIhF8Wnidj21yrlplAc-SeVHw5fb-eT'>\n",
        "\n",
        "**Option:**\n",
        "- Update models i.e. $b_u, b_i, q_i, p_u$ periodically.\n",
        "- Give more weightage to recent data. This can be done by adding $W_{ui}$ to the optimization function, where $W_{ui}$ is based on recency.\n",
        "\n",
        "$$\\underset{\\underset{\\mu, \\ b_u, \\ b_i}{q_i, p_u}}{\\min} \\  \\underset{u,i}{\\Sigma} \\ W_{ui} \\ ( r_{ui} - \\mu - b_u - b_i - q_i^T.p_u)^2 + \\lambda(\\underset{i}{\\Sigma}||q_i||^2 + \\underset{i}{\\Sigma}||p_u||^2 + b_u^2 + b_i^2)$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "octA-BWdW9io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above idea works. But there is a much simpler solution.\n",
        "- Pre-compute $r_{ui}$ on nightly basis using Matrix Factorization where $r_{ui}$ is decomposed into $q_i\\to$ d-dimensional representation of item $i$ and $b_i\\approx$ average rating of item $i$ (based on popularity).\n",
        "    <img src='https://drive.google.com/uc?id=1CeSVZJffQ_YcHFC95KOAxaVtvVRVfbUS'>\n",
        "\n",
        "- So, when a video $V_{100}$ is recommended to a user then next few video recommendations are based on:\n",
        "    - Pure recency based similarity. A very fast KNN in d-dimensional space is built for this purpose.\n",
        "    - Historical taste of users.\n",
        "    - Top videos that are good to recommend.\n",
        "    <img src='https://drive.google.com/uc?id=1-oKNvC-b6yTyhyk9biREr3I9ShisIktq'>\n",
        "\n",
        "***Note:*** Nowadays, we have State-of-the-art RecSys built using Neural Collaborative Filtering (NCF) and advanced Deep Learning (DL) models."
      ],
      "metadata": {
        "id": "t5zFnhIbwuZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for Recommender Systems:"
      ],
      "metadata": {
        "id": "oTexFWR-etcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation for surprise library:\n",
        "\n",
        "https://surprise.readthedocs.io/en/stable/"
      ],
      "metadata": {
        "id": "MAb35IhyfJdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvnRviX5ewve",
        "outputId": "70f60457-7b38-464f-c754-0aad1dcdb4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633997 sha256=f579ad01257443daa4ebe862c9cc036dd5da94498c1c329a74a4f8c34cbc6a13\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: https://surprise.readthedocs.io/en/stable/getting_started.html\n",
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy"
      ],
      "metadata": {
        "id": "vmbZqAW6ewyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the movielens-100k dataset (download it if needed),\n",
        "data = Dataset.load_builtin('ml-100k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAnC8FdAew1l",
        "outputId": "37a2a8be-82ed-4fef-8e2f-80358aaf6a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] y\n",
            "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample random trainset and testset\n",
        "# test set is made of 25% of the ratings.\n",
        "trainset, testset = train_test_split(data, test_size=.25)\n",
        "\n",
        "# We'll use the famous SVD algorithm.\n",
        "algo = SVD()\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Then compute RMSE\n",
        "accuracy.rmse(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdQ3QcVYfDF9",
        "outputId": "fe8e5362-2a20-4bc7-d6d6-5f5b40862254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.9476\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9475870426780431"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
        "iid = str(302)  # raw item id (as in the ratings file). They are **strings**!\n",
        "\n",
        "# get a prediction for specific users and items.\n",
        "pred = algo.predict(uid, iid)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdEdb2Z9fDI2",
        "outputId": "fbce6edb-62f7-4a6a-b0b1-d348dbc7a1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(uid='196', iid='302', r_ui=None, est=4.309109605145767, details={'was_impossible': False})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Complexity increases with some variables. How to handle computational costs?\n",
        "\n",
        "***Answer:***\n",
        "\n",
        "- Movie-lens like datasets run quickly in Google-colab. Problem occurs when we have 10's of millions of parameters. This is handled in Deep Learning very well.\n",
        "- SGD works even for few million parameters on colab or local machines.\n",
        "- Matrix factorization is carried out on distributed systems like Spark for faster results.\n"
      ],
      "metadata": {
        "id": "CUzxli0jhXGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Factorization (MF) for Feature Engineering:"
      ],
      "metadata": {
        "id": "atZ7vjhifb2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entities:"
      ],
      "metadata": {
        "id": "HySs62-CsDaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Matrix Factorization, we cared the most about $q_i$ and $p_u$.\n",
        "- From simple raw dataset $r_{ui}$, we solve the optimization problem and get the d-dimensional representations of items ($q_i$) and users ($p_u$).\n",
        "    <img src='https://drive.google.com/uc?id=1fn8BlX-5ZFYZRQL8M2yBG1p0y1NEific'>\n",
        "- So, in a way MF does feature engineering as it provides item vector and user vectors.\n",
        "- These vectors are sensible as similar users will have similar vectors and similar items will have similar vectors.\n",
        "- These vector representations can be used in Classification/ Regression/ Vizualizations/ Clustering.\n",
        "- So MF is a feature engineering technique that uses a model.\n"
      ],
      "metadata": {
        "id": "1BHxtNSDzYa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Example:*** E-commerce Setup\n",
        "\n",
        "- Let say, a Recommender System was built using all historical data and it has generated user vectors $p_u$ and items vectors $q_i$.\n",
        "- These vector features are stored in Feature-Stores as key-value pairs. This feature store could be queryed on:\n",
        "    - user-id $\\to$ to get d-dimensional representation of an user.\n",
        "    - item-id $\\to$ to get d-dimensional representation of an item.\n",
        "- Suppose a developer is working in search team of the same E-commerce setup.\n",
        "- Now, the end-customer would search for a key term in the shopping browser. Let say \"Bluetooth\".\n",
        "- Developer can use the feature store to improve the search result by taking all the item vectors that are similar to this search term.\n",
        "\n",
        "***Note:*** This technique to built feature stores using MF was used by small startups and big firms. Nowadays, NCF or Entity embeddings are used.\n",
        "\n"
      ],
      "metadata": {
        "id": "7eJ66eld2uVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Are this features interpretable?\n",
        "\n",
        "***Answer:*** No, these features have no interpretability.\n",
        "- In manual feature engineering, we collected data like Country, Pincode... for the user. So it was interpretable.\n",
        "- But in MF base feature engineering, we get a d-dimensional representation of users. These algorithmically generated features could be vizualized using t-SNE/UMAP.\n",
        "<img src='https://drive.google.com/uc?id=14aL6HLVogV8W4cUW9sWUDIqzzqDms4SW'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YxwECMYF520b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text:\n"
      ],
      "metadata": {
        "id": "A-NohA9bIH5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can get d-dimensional representations of words. Suppose we have a large Corpus of text e.g. Wikipedia.\n",
        "- These d-dimensional representations should be sensible such that if we project these vectors in d-dimensional space, then similar words will appear together.\n",
        "<img src='https://drive.google.com/uc?id=1iyalE-qkzdWdfVBywt6wMLfYVhRQ-bZn'>\n",
        "- So how do we build them?"
      ],
      "metadata": {
        "id": "ky9kTpw_HBo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Option 1:"
      ],
      "metadata": {
        "id": "VVBQU3sfwyOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To begin with, we can arrange the information into a matrix $A_{n*m}$ as below, where $n\\to$ documents and $m\\to$ words and $A_{ij}$ = Frequency/Count of occurance.\n",
        "<img src='https://drive.google.com/uc?id=1EgRJCHUSuK2S9E7ORUZHNIoaGozGpkne'>\n",
        "- This matrix could be decomposed into 2 metrices $B$ and $C$ where:\n",
        "    - $B_i^T$ = $Doc_i^T$: A dense representation of the document.\n",
        "    - $C_j$ = $W_j$: A dense representation of the word.\n",
        "<img src='https://drive.google.com/uc?id=1-dveLiA1bq8D9rIxOeLaQdbleCXbHdvK'>\n",
        "\n",
        "- This is the simplest form of Matrix Factorization.\n",
        "- Here we get d-dimensional representations of documents as well as the words."
      ],
      "metadata": {
        "id": "kOuA_X0iwpBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Option 2:"
      ],
      "metadata": {
        "id": "SX446_HGw3Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's construct a word-word matrix $X_{m*m}$. Here, $X_{ij} = 1$ if $w_i$ and $w_j$ occur in vicinity of a context.\n",
        "<img src='https://drive.google.com/uc?id=1xvY9dO0d9KmZ4uZ5guuBVNpUov-1WFEO'>\n",
        "- Here we capture if 2 words occur in the same context. For e.g. if 2 words are within a particular word distance. (Co-occurance Matrix).\n",
        "- We can do Matrix Factorization and decompose $X_{m*m}$ into $B_{n*d}$ and $C_{d*n}$ where:\n",
        "    - $B_{i}^T \\to$ representation of words.\n",
        "    - $C_{j} \\to$ representation of words.\n"
      ],
      "metadata": {
        "id": "ZZEZJ0i0wsnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Images:\n"
      ],
      "metadata": {
        "id": "qDDWUCUgznuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2-d image can be represented as single 1-d vector (Row major form).\n",
        "<img src='https://drive.google.com/uc?id=1NXAD5v6ZNbqCLxJQpKUB38MC1FUewW9P'>\n",
        "- Given these images, we can construct a data matrix: $X_{n*d}$.\n",
        "<img src='https://drive.google.com/uc?id=1WfDMyGG5QWeat3WuCj6SFEIaErvBf9RO'>\n",
        "- Here, $n\\to$ number of images, $d = r*c$ where $r\\to$ rows and $c\\to$ columns and ith row in this matrix is $image_i$.\n",
        "- We can decompose $X_{n*d}$, into $B_{n*m}$ and $C_{m*d}$.\n",
        "<img src='https://drive.google.com/uc?id=1cAnahVwKygP-monnC1Za8cYdggKQ2X2g'>\n",
        "- If we have 1000*1000 pixel image, then $d=10^6$, but if $m=20$ then we have a 20-dimensional dense representation for each image.\n"
      ],
      "metadata": {
        "id": "37bnKYS1znuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary:"
      ],
      "metadata": {
        "id": "KqwrRRLG8dN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beauty of Matrix Factorization and its variations be it PCA, SVD, NMF, Clustering is that it helps to construct algorithmically generated features that are rich and dense."
      ],
      "metadata": {
        "id": "yk1l6zIo8fi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Note*"
      ],
      "metadata": {
        "id": "BiolflsTfvuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following topics will be covered in next lecture:\n",
        "\n",
        "7.   Market-Basket Analysis (ARM, Apriori Algorithm)."
      ],
      "metadata": {
        "id": "6lClAUcPf6Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9ex6ukU89WE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}