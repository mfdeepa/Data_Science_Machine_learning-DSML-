{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models - 2"
      ],
      "metadata": {
        "id": "Qnh4IyApv2vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- GMM: Optimization\n",
        "- Generative Methods in the context of GMM\n",
        "- Maximum Likelihood Estimation in GMM\n",
        "- Maximum Likelihood Estimation: Algorithm\n",
        "- Visualization of GMM\n",
        "- Advantages and Disadvantages of GMMs and Expectation Maximization\n"
      ],
      "metadata": {
        "id": "QcKTNIPDv71y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "wQbqUGltv74P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMM: Optimization\n",
        "\n",
        "#### **Let's start with a 1-Dimensional Gaussian Distribution**\n",
        "\n",
        "- That is, a Gaussian Distribution having only 1 feature.\n",
        "\n",
        "- Let's say we have n observations of a random variable $x$ in our data:\n",
        "\n",
        "    $x_1, x_2, x_3, ..., x_n$\n",
        "\n",
        "- If we know that this random variable follows Gaussian Distribution $N(\\mu, \\sigma)$, we can estimate **Mean** $\\mu$ and **Standard Deviation** $\\sigma$ or **Variance** $\\sigma^2$ of the Distribution as follows:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YpHuscmOGZz7SAqUcggbbJC75ycQB5tk'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IPYUnGRkv76-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Now, moving on to d-Dimensional Data**\n",
        "\n",
        "- We'll have to use **Multi-Dimensional Gaussian Distributions**.\n",
        "\n",
        "- Let's say we again have $n$ data points in our dataset:\n",
        "    \n",
        " $x_1, x_2, x_3, ..., x_n$\n",
        "\n",
        " Where, each $x_i$ is a d-Dimensional data point, i.e., each data point has $d$ features.\n",
        "\n",
        " And, the random variable $X$ can be represented as a d-Dimensional Gaussian Distribution $N_d(\\mu^{(d)}, \\sum^{(d \\times d)})$\n",
        "\n",
        "- Remember? **Vector of Means** $\\mu^{(d)}$ and **Covariance Matrix** $\\sum^{(d \\times d)}$ are the **parameters of d-Dimensional Gaussian Distribution**.\n",
        "\n",
        "- We can compute the vector of Means $\\mu^{(d)}$ and the $d \\times d$ Covariance Matrix $\\sum^{(d \\times d)}$ for Gaussian Distribution of a d-dimensional data as follows:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YDl8OcDuRlbl0qo_67n9M2j8YN5trFtQ'>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nI-VZuDgv79W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now, What is our objective here?**\n",
        "\n",
        "#### **We want to find the K-Gaussians so that we can develop a Gaussian Mixture Model from the given data.**\n",
        "\n",
        "- Given a d-Dimensional data,\n",
        "\n",
        "  For every K, we need to compute the d-dimensional vector of means $\\mu^{(d)}$ and the $d \\times d$ Covariance Matrix $\\sum^{(d \\times d)}$\n",
        "  \n",
        "  That is, find the K-Gaussian Distributions.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Du-ZB3JEiL7Fw2tI0QmBepzxPolMYjkn'>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rH3kwhJv7_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "wB_WY_iRv8Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Now the questions is: How can we find these K-Gaussian Distributions?**\n",
        "\n",
        "## Generative Methods in the context of GMM\n",
        "\n",
        "- Generative methods start by making some assumptions on how the data has been generated.\n",
        "\n",
        "#### **What do you think will be the assumption in Gaussian Mixture Models on how the data has been generated?**\n",
        "\n",
        "- It assumes that the **data was generated from a Multi-modal Gaussian Distribution**.\n",
        "\n",
        "- For K-Gaussian Distributions, there is a probability of each data point belonging to each of the K-Gaussians, which can be mathematically represented as:\n",
        "\n",
        " $P(Y = j) ∀j : 1 → K$\n",
        "\n",
        " where $Y$ is the random variable that can values from 1 to $K$,\n",
        "\n",
        " and $j$ refers to the $j^{th}$ Gaussian (cluster).\n",
        "\n",
        "- It tells us what is the probability of a data point belonging to a Gaussian (or Cluster).\n",
        "\n",
        "- This probability function can be applied to all the data points in our dataset. Essentially, it gives us the probability of a data point $x_i$ belonging to each of the $K$-Gaussians (hence, $∀j : 1 → K$) in the Gaussian Mixture Model.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1OsanNAzR8EM3pP2Xdp_g6P-jRfbd7WKo'>\n",
        "\n",
        "- So, GMMs are probabilistic models that assume that the instances were generated from a mixture of several Gaussian distributions (a.k.a Normal Distributions) whose parameters are unknown.\n",
        "\n",
        "- All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid.\n",
        "\n",
        "- Each cluster is modelled according to a different Gaussian distribution, with different Mean $\\mu_j$ and Co-Variance $\\epsilon_j$.\n",
        "\n",
        "- Each cluster can have a different ellipsoidal shape, size, density, orientation and weights relative to each other.\n",
        "\n",
        "When you observe an instance, you know it was generated from one of the Gaussian distributions, but you are not told which one, and you do not know what the parameters of these distributions are.\n",
        "\n",
        "Note: The computational complexity of training a GaussianMixture model depends on the number of instances `m`, the number of dimensions `n`, the number of clusters `k`, and the constraints on the covariance matrices.\n",
        "\n",
        "Each cluster is modelled according to a different Gaussian distribution. Mathematically we can define GMM as mixture of K gaussian distribution that means it’s a weighted average of K gaussian distribution. So we can write data distribution as :\n",
        "\n",
        "![gmm_formula.png](attachment:gmm_formula.png)\n",
        "\n",
        "Where N(x|mu_k,sigma_k) represents cluster in data with mean mu_k and co variance epsilon_k and weight pi_k.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yDou78xxn5LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi-nomial Random Variable**\n",
        "\n",
        "#### **What is the distribution of the random variable Y in case of K-Gaussian Distributions?**\n",
        "\n",
        "- Remember what do we call a random variable which can take only two values 0 and 1 with a certain probability of taking each of the values?\n",
        "\n",
        "  - **Bernoulli Variable**\n",
        "\n",
        "- Similarly, a random variable $Y$ that can take values from 1 to $K$ with a probability associated with each value is called a **Multi-nomial Variable**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1V3N_4q4JrEgsic2bZU2Uq7h9UO2WYldV'>\n",
        "\n",
        "#### **Now, Let's say we know these probabilities, i.e., we know the probability of a data point coming from the $j^{th}$ Gaussian**.\n",
        "\n",
        "- That is, $P(Y = j) ∀j : 1 → K$ is known to us.\n",
        "\n",
        "#### **Let's say we also know the parameters of Gaussian Distribution**\n",
        "\n",
        "- That is, the d-Dimensional vector of Means $\\mu^{(d)}$ and the $d \\times d$ Covariance Matrix $\\sum^{(d \\times d)}$ for each underlying Gaussian are known to us.\n",
        "\n",
        "- Then we can generate a sample data point from each of the Gaussian Distributions, for example, we can generate a sample point for $Y = 1, 2, 3 ...$\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wzO5Zc0mtwei0wjJxWsYm_SOGjtpfzjV'>\n",
        "\n",
        "\n",
        "Now, if we repeat the above steps many times, we can generate a whole set of data points, i.e., a dataset that is very similar to our original observed d-Dimensional dataset.\n",
        "\n",
        "- This works because of the **assumption of Generative Methods in the context of GMM**, that the **data is generated from (or comes from) a d-Dimensional Multi-modal Gaussian Distribution** $N_d(\\mu^{(d)}, \\sum^{(d \\times d)})$.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1BLIpEZmoXA08VAR_IAOxMtPdaAR1NgnE'>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9py196XJn5OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Difference from Discriminative Methods**\n",
        "\n",
        "- All methods and algorithms we had studied before this were **Discriminative Methods**.\n",
        "\n",
        "- It means that given a data point in the dataset, all previous techniques tried to discriminate whether the point belongs to Cluster 1, or Cluster 2, ... so on.\n",
        "\n",
        "- They did not try to generate the data. They only discriminated the existing data into different groups.\n",
        "\n",
        "- However, now with this GMMs, we are studying **Generative Methods of Clustering**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1JGxf6uPSvE4QAHXLkah347M7g8QjafV6'>\n"
      ],
      "metadata": {
        "id": "BnudpKhcn5Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "7Q3J6TWFn5Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood Estimation in GMM\n",
        "\n",
        "#### **A Gaussian Mixture Model is very similar to K-Means**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ZoJWbJ8tcmZECoWanozwARwL4NJ7HuJx'>\n",
        "\n",
        "- In **GMM**, we do a **soft cluster assignment** → Because every data point has a probability of belonging to each of the Gaussians (Clusters).\n",
        "\n",
        "- Whereas, **K-Means** is a **hard cluster assignment** → Because every data point is assigned to only one cluster."
      ],
      "metadata": {
        "id": "k7flS1NzRYMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Under the hood, the Generative Method of determining K-Gaussians in a GMM uses an algorithm called **Maximum Likelihood Estimation**.\n",
        "\n",
        "- As we have seen so far, a Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose **parameters** are unknown.\n",
        "\n",
        "- The problem here is to find those parameters, i.e., for a d-Dimensional K-Gaussian Mixture Model (GMM), we need to determine all the $K$ probabilities, all the $K$ d-Dimensional vectors of Means and all the $K$ $d \\times d$ Covariance Matrices.\n",
        "\n",
        "- All these form the set of parameters $Θ$ that we need to find.\n",
        "\n",
        "#### **Mathematical Formulation of Maximum Likelihood Estimation Problem**\n",
        "\n",
        "So, we need to find:\n",
        "\n",
        " $\\theta = [P(Y = j) ∀j : 1 → K; \\mu^{(d)}, \\sum^{(d \\times d)} ∀j : 1 → K]$\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10zEQ_TiIU9xzagh8eyoP9LGn18GWnt5N'>\n",
        "\n",
        "\n",
        "- When we observe an instance, we know it was generated from one of the Gaussian distributions, but we are not told which one, and we do not know what the parameters of these distributions are.\n",
        "\n",
        "### **Problem Statement with Maximum Likelihood Estimation**\n",
        "\n",
        "Given the dataset $D: \\{x_i\\}_1^n$,\n",
        "\n",
        "Find the parameters $\\theta$, such that the probability of generating the dataset $D$ is maximum.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10zEQ_TiIU9xzagh8eyoP9LGn18GWnt5N'>\n",
        "\n",
        "- This is the basic idea behind Maximum Likelihood Estimation. We need to find the set of parameters such that the probability of generating the given dataset is maximized.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hwsZtLPSn5xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is called **Maximum Likelihood Estimation** because we find the set of parameters $\\theta$, such that the probability $P(D)$ of observing the dataset $D$ is maximized.\n",
        "\n",
        "- Now the dataset $D$ consists of data points $x_1, x_2, x_3, ..., x_n$.\n",
        "\n",
        "- There is another basic assumption in MLE approach that all the data points are independent of each other.\n",
        "\n",
        "- So, $\\text{max}_\\theta P(D) = \\text{max}_\\theta P(x_1, x_2, x_3, ..., x_n)$\n",
        "    \n",
        "    $= \\text{max}_\\theta P(x_1).P(x_2).P(x_3)...P(x_n)$\n",
        "\n",
        "    $= \\text{max}_\\theta ∏_i^n P(x_i)$\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1sgY8Z4HbaHV-w-esd_7aTmTfGV0Cv7SD'>\n",
        "\n",
        "For a **K-Gaussian Mixture Model**, the problem can be formulated as follows:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1OPcl2RqtvebUgaNfAbU-myp4-g7cfJ5H'>\n",
        "\n",
        "- This is something similar to **Conditional Probability**, where we have **Cluster Priories** (similar to Class Priories) and **Likelihood of generating a data point $x_i$ using $j^{th}$ Gaussian**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=11fpuSLONlp8XITRHgMR3N6k6PGww8-cH'>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d2T8-f4mPR5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "tuEAsrg0jKUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood Estimation: Algorithm\n",
        "\n",
        "### **This optimization problem is solved with the help of Expectation Maximization**\n",
        "\n",
        "- **Expectation Maximization is essentially a 2-step process: Expectation or E step and Maximization or M step**.\n",
        "\n",
        "\n",
        "- First, we choose starting guesses for the location and shape of the Gaussians, which is fixed.\n",
        "\n",
        "- Then, we repeat until convergence:\n",
        "\n",
        "    - **E-step:** For each point, find weights encoding the probability of membership in each cluster.\n",
        "    \n",
        "    - **M-step:** For each cluster, update its location, normalization, and shape based on all data points, making use of the weights.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12HZqAmGODyZZ9wHmrGUPTpWznqJf4cTm'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OO0Iqn-G3Ey6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Let's expand on the two steps of Expectation Maximization bit more**\n",
        "\n",
        "- Let's say we are given $n$ data points: $x_i$'s where $i$ ranges from 1 to $n$.\n",
        "\n",
        "- Our ultimate goal is to determine which Gaussian (Cluster) does each $x_i$ belongs to.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1oI-l6oUQyIW-jKDDMkOjdiqXRsn76e8a'>\n",
        "\n",
        "1. **Expectation (E) Step** - For each $x_i$, we compute the probability of it belonging to $j^{th}$ cluster.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1H9OVGASNCuVDVcoMiFombHyoeVRuYI4J'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1R2YMk171T5x8As24z3nrO4zbmg67NpLH'>\n",
        "\n",
        "\n",
        "\n",
        "2. **Maximization (M) Step** - Re-estimate the Gaussian parameters for all Gaussians.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1LwLrPC2A1j87bVyKozr01Ir_W-2NZqJQ'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrG_5zwhl5lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Expectation Maximization (EM) is a Coordinate Ascent**\n",
        "\n",
        "- It is **Ascent** because it is a Maximization problem.\n",
        "\n",
        "- In **Coordinate Ascent**, in order to reach the optimal point, we increase 1 parameter while all other parameters are fixed. This is then followed by increasing another parameter while keeping all other parameters fixed.\n",
        "\n",
        "- We do this ascent till we reach the optimal value of the combination of parameters.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1GT3KKznEezx5e31PGXmzt91g5dx1dEQO'>\n"
      ],
      "metadata": {
        "id": "Z2WYmJF9N-Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n"
      ],
      "metadata": {
        "id": "R8g-uAjkN-Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of GMM\n",
        "\n",
        "- As we mentioned earlier, Gaussian Mixture Models are probabilistic models that assume that the instances were generated from a mixture of several Gaussian Distributions whose parameters are unknown.\n",
        "\n",
        "- All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid.\n",
        "\n",
        "- Each cluster is modelled according to a different Gaussian distribution, with different Mean $\\mu_j$ and Co-Variance $\\epsilon_j$.\n",
        "\n",
        "- Each cluster can have a different ellipsoidal shape, size, density, orientation and weights relative to each other.\n",
        "\n",
        "- When we observe an instance, we know it was generated from one of the Gaussian distributions, but we are not told which one, and we do not know what the parameters of these distributions are.\n",
        "\n",
        "**Note:** The computational complexity of training a GaussianMixture model depends on the number of instances `n`, the number of dimensions `d`, the number of clusters `K`, and the constraints on the Covariance Matrices.\n",
        "\n",
        "- Each cluster is modelled according to a different Gaussian distribution. Mathematically we can define GMM as mixture of K-Gaussian Distributions, that means it’s a weighted average of K Gaussian Distribution.\n",
        "\n",
        "\n",
        "- Let's say we have 3 Clusters (Gaussians) to begin with - Red, Green and Blue contours that you see below:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1nY-WkObSQ2t3YMT5hLzBA8q5aI6ZzQc5'>\n",
        "\n",
        "- The small circles represent the data points.\n",
        "\n",
        "- Each circle has portions of Red, Green and Blue colors, where each colored portion represents the initial probability of the data point belonging to each of the clusters.\n",
        "\n",
        "- For example, if a data point is more Red, it has higher probability of belonging to Red cluster, and if it is more Blue, it will have higher probability of belonging to Blue cluster, ... and so on. And if a data point has equal portions of all three colors, it has equal probability of belonging to each of the 3 clusters.\n",
        "\n",
        "- In the beginning, the vector of Means $\\mu_d$ and Covariance Matrix $\\sum_{d \\times d}$ are constructed at random.\n",
        "\n",
        "\n",
        "- If there are $n$ random variables $Y(i)$ (from $Y(1)$ to $Y(n)$) and $n$ random variables $x_i$. There are also $K$ means $\\mu_j$ and $K$ covariance matrices $Σ_j$. Lastly, there is just one weight vector $\\theta$ (containing all the weights $\\theta_1$ to $\\theta_k$).\n",
        "\n",
        "- Each variable $Y(i)$ is drawn from the categorical distribution with weights $\\theta$. Each variable $x_i$ is drawn from the normal distribution, with the mean and covariance matrix defined by its cluster $Y(i)$.\n",
        "\n",
        "\n",
        "As shown in below picture, after the first iteration of Expectation and Maximization Step, the Green Gaussian Distribution starts moving towards first set of points, Red Gaussian towards second set of points and Blue Gaussian towards third set of points.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DaLsOyjG2mplbey_ySTa0RTZ4r6X7oSx'>\n",
        "\n",
        "#### **The Maximization/Optimization Function $\\text{max}_\\theta ∏_i^n P(x_i)$ is updated at each iteration for each of the K-Gaussians in order to reach the optimal (maximum in this) value of this Optimization Function**.\n",
        "\n",
        "- For each iteration, during the **Expectation Step**, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters).\n",
        "\n",
        "- Then, during the **Maximization Step**, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster. These probabilities are also called the **responsibilities** of the clusters for the iteration.\n",
        "\n",
        "- During the **Maximization Step**, each cluster’s update will mostly be impacted by the iteration it is most responsible for.\n",
        "\n",
        "\n",
        "- After a few more iterations of E and M steps, the clusters start becoming much more clear. The probability of each data point belonging to a specific cluster starts increasing, and its probability of belonging to other clusters start decreasing.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1OMNPRxXvxs1JSjQLsPaMQgF5T6If_-qd'>\n",
        "\n",
        "- After more iterations, see how the clusters become well-formed.\n",
        "\n",
        "- The probability of each data point belonging to a specific cluster becomes very high, and the probability of it belonging to other clusters becomes extremely low (but non-zero). Hence, it's a **soft assignment**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Fyd1uZFnOq8yQZD5FqWRws4corcL0RuH'>\n",
        "\n",
        "- As we can see, each colored cluster is represented by its respective contour.\n",
        "\n",
        "- We can think of EM as a generalization of K-Means that not only finds the cluster centers $(μ_1$ to $μ_k)$, but also their size, shape, and orientation $(Σ_1$ to $Σ_k)$, as well as their relative weights $(\\theta_1$ to $\\theta_k)$.\n",
        "\n",
        "- That is why we said that, unlike K-Means, Expectation Maximization uses **soft cluster assignments**, not hard assignments.\n",
        "\n",
        "\n",
        "So, this is how Expectation Maximization works to identify the shape of Clusters in a Gaussian Mixture Model, as well as grouping the data points into those Clusters."
      ],
      "metadata": {
        "id": "3TcS5EwNuEde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "6dsNMGWyuFAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages and Disadvantages of GMMs and Expectation Maximization\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "- The basic two steps of the EM algorithm i.e, E-step and M-step are often pretty easy for many of the machine learning problems in terms of implementation.\n",
        "\n",
        "- The solution to the M-step often exists in the closed-form.\n",
        "\n",
        "- It is always guaranteed that the value of likelihood will increase after each iteration.\n",
        "\n",
        "- **Speed:** It is the fastest algorithm for learning mixture models.\n",
        "\n",
        "- **Agnostic:** As this algorithm maximizes only the likelihood, it will NOT bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "- It has slow convergence.\n",
        "\n",
        "- It is sensitive to starting point, converges to the local optimum only.\n",
        "\n",
        "- It cannot discover K (likelihood keeps growing with number of clusters).\n",
        "\n",
        "- It takes both forward and backward probabilities into account. This thing is in contrast to that of numerical optimization which considers only forward probabilities.\n",
        "\n",
        "- **Singularities:** When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.\n",
        "\n",
        "- **Number of components:** This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1B6TuTHzM5NKwggEIFpCkadDnX-XilVhB'>\n",
        "\n",
        "That is why Gaussian Mixture Models are used less in practice.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rAk_rDUMN-TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "7j1yVOdXN-Vx"
      }
    }
  ]
}