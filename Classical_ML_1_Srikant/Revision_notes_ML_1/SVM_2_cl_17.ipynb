{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b><font style=\"Times New Roman\">Lecture | DSML Advanced : SVM-2</h1></b></font>\n",
        "\n",
        "Lecture Link : https://www.scaler.com/meetings/i/dsml-advanced-svm-2-2/archive\n",
        "\n",
        "<h2><b>Content</h2></b>\n",
        "\n",
        "1. <h3><b>RBF Kernel</b></h3>\n",
        "\n",
        "> - <b>Quick Recap of SVM-Dual form</b> ( 00:03:00 )\n",
        "> - <b>RBF kernel</b> ( 00:20:32 )\n",
        "> > - Effect of various hyper-parameters\n",
        "\n",
        "\n",
        "> - <b>Support Vector Regression ( SVR )</b> ( 01:22:35 )\n",
        "\n",
        "\n",
        "> - <b>Interpretability of SVM</b> ( 01:32:31 )\n",
        "> - <b>Visual representation of SVM with different hyper-parameters</b> ( 01:35:34 )\n"
      ],
      "metadata": {
        "id": "HvcJcBJv7Qma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Quick Recap of SVM-Dual form</b></h2>\n",
        "<p>The dual form can be represented as shown below such that for x<sub>i</sub> there exist a <font size=4>&alpha;<sub>i</sub> </font> and x<sub>i</sub> only exist in the form of x<sub>i</sub><sup>T</sup> x<sub>j</sub>: <br> <br>\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1-ocW0jIAqWmsWWIXfnACRzOCxMAo6IWf'/>\n",
        "\n",
        "and to calculate y<sub>q</sub>'  for any query point x<sub>q</sub>', it can be done caluclating this summation.\n",
        "\n",
        "We also saw an example of Quadratic kernel, how it converted a 2D data to 6D to find a best seperating hyperplane.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1v1tJy_Lz2CZYSMjoOOpiCZg14ea9seAG'/>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h4tNBiIm7PTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  In PCA we decrease dimension but here in SVM we are increasing, isn't is contradictory?\n",
        "---\n",
        "No, because purpose of both the methods are different. PCA is mostly used for visualization because it is easy to visualize data in low dimension.But In SVM, our aim is classification i.e. to find best seperating hyperplane which is much easier in higher dimensions.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1PaesPo6FfYR-Zgy25Bm0efz2E-uzDeAw'/>\n"
      ],
      "metadata": {
        "id": "fvkoz-7mhAoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: So in real life scenario after we do PCA we go back the original dimension to find a model for the data ?\n",
        "---\n",
        "No, also PCA is agnostic to class labels which means it does not guaraantee classification will always be best. PCA aim is to reduce dimension and some models like Random forest, Decission tree might work better as they expect low dimensional data.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1br0rR1YMdoAOyAURSqj4GV_pa5McIcJh'/>\n"
      ],
      "metadata": {
        "id": "8QNEYXCAh2GL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange multiplier is used when ever you have optimization problem subjected to some constraints.**\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1AH1uC6c2ag4hV7aTSBNwi6B_MtSQs5ME'/>\n"
      ],
      "metadata": {
        "id": "OJKFXNgl69oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: Do we need to split train and test data in PCA ?\n",
        "---\n",
        "We generally apply PCA to train data of dimension D such that we get data in dimension D' ( D > D' ). Now for  test data  we multiply each x<sub>i</sub> with top D' eign vectors to get x <sub>i</sub>'\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1_jQ3secYgd7Rjonzxb9_-l7v_WtEuk9d'/>\n"
      ],
      "metadata": {
        "id": "08HFvNmr7qUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>RBF kernel</b></h2>\n",
        "\n",
        "> - Most widely used kernel\n",
        "> - Can be used as a default kernel if you don't have any domain specific kernel.\n",
        "> - It is widely used due to it's similarity with Gaussian distribution.\n",
        "\n",
        "The RBF kernel looks similar to PDF of Gaussian distribution.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1CDvb4dyQV0AOvSjomAVtCOpN9N5T6KNb'/>\n",
        "\n",
        "The RBF kernel function for two points X₁ and X₂ computes the similarity or how close they are to each other. This kernel can be mathematically represented as follows:\n",
        "\n",
        "\n",
        "\n",
        "> <font color=\"green\" size=5>K( X₁, X₂ ) = exp( - $\\frac{|| X₁ - X₂ ||^2}{2\\sigma^2}$)</font>\n",
        "\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1IOb1CL0xDyOFMZ9g2lXPXnGCoWwNCut3'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1jENY5ukffAksrQTrBwUiWJ8Un_E_KEAo'/> </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "where,\n",
        "1. ‘σ’ is the variance and our hyperparameter\n",
        "2. ||X₁ - X₂|| is the Euclidean (L₂-norm) Distance between two points X₁ and X₂\n",
        "\n",
        "The kernel equation can be re-written as follows if we consider d<sub>12</sub> as distance between X₁ and X₂:\n",
        "> <font color=\"green\" size=5>K( X₁, X₂ ) = exp( - $\\frac{d_{12}^2}{2\\sigma^2}$)</font><br>\n",
        "\n",
        "Observations while plotting with values of ‘σ’ and d<sub>12</sub>:\n",
        "> - Lower the ‘σ’, thinner the curve will be and vica versa\n",
        "> - Distance d<sub>12</sub> ( ecludian distance ) can be thought of as a similarity metric, larger the d, less similar the points are.\n",
        "> - Another point to notice is that relation is exponential which means even for slight increase in d distance, similarity or K<sub>RBF</sub> will fall exponentially.\n",
        "> - The maximum value that the K<sub>RBF</sub> ( RBF kernel ) can be is 1 and occurs when d₁₂ is 0 which is when the points are the same, i.e. X₁ = X₂.\n",
        "\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1jwnozxZ9UxCJwh1aTRV3Vv2pai2v-5om'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1Y7Mw429uUR2fjtrqj2ad9jfrIlueuK3b'/> </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "> - ‘σ’ is the variance and our hyperparameter. so we need to carefully choose our ‘σ’ to decide which points will be considered similar. <br>\n",
        "> - Very low ‘σ’ means , thinner curve which implies if points are extremely close they will be considered similar.\n",
        "> -  Very high ‘σ’ means , fat curve which implies even if points are far away they will be considered similar .\n",
        "\n",
        "Below is the plot of curve with different values of &sigma;<br>\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1gMatWyxcoqEFEy24M7EmLae7vDNsNfSh'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1_ZbI3TmHS6OnTRmuQ_JCp7EBKOtRWeWg'/> </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "         <td colspan=2 text-align=\"center\" > <img width=50% src='https://drive.google.com/uc?id=1rfpvOJHtcgwhBCdW_QCZE82MJomsHMV-'/> </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "> It can be further observed that RBF with small ‘σ’ behaves similar to KNN with small K as both behave as proximity based models ( also both overfits ).\n",
        "K in KNN can be seen as analogous to ‘σ’ in SVM-RBF kernel.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=18PE_vM9NVitYCbcQi1JccWGEh4PuNeFL'/>\n",
        "\n",
        "> Even if you are given any complex data, RBF-SVM is still be able to seperate the data by constructing a hyperplane in D' dimension but for that to happen, there is condition that D' -> &infin; proof of which is beyond the scope .\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1Ihiyl3cH1nRX8Iq9Grz6RkwITryIhdhm'/>"
      ],
      "metadata": {
        "id": "Lh2xY2Pi8tmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: while studying KNN we discussed euclidan distance fails in high dimesions so we go manhatten, minkoshi. but here we calculate eucldian in high dimesion here, why .\n",
        "---\n",
        "Yes, but here we are not computing || X₁ - X₂ || in D' dimension, we are still computing || X₁ - X₂ || in D dimension but then we are apply RBF kerenl function to it. So interestingly we are not transforming the data but the distance between the data.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1Mr9wv7LSCfXDNEPw10LytJOMmJSm5P5P'/>"
      ],
      "metadata": {
        "id": "2uPAsehOPRXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another intuitive way of understanding above concept.Let's say we have two features f1 and f2:\n",
        "> -  if we use only these two then our seperator will be a straight line in the space of f1 and f2.\n",
        "> - But if we use qurdratic transformation of them ( 5 dimensions ) like f1,  f2,  f1<sup>2</sup>,  f2<sup>2</sup>, and f1f2 then seperator will be somewhat a parabolic curve or any quadratic curve similar to it but again in the space of f1 and f2 only not in these 5 dimensions .\n",
        "> - similarly, we can say RBF kernels it will used &infin; many feature so that we will get a very complex curve as a seperator in space of f1 and f2, which implies that all this computation is done implicitly not explcitly.\n",
        "> - <b>To summarize, RBF kernel SVM actually does is to create non-linear combinations of your features to uplift your samples onto a higher-dimensional feature space where you can use a linear decision boundary to separate your classes.</b>\n",
        "\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=18a9kUtX52X-PNoKDhfWDL0_8-Z-2iBku'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=14JXARacohMZ2eg2hDtU_7HhdA_B1eFKR'/> </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "         <td colspan=2 text-align=\"center\" > <img width=50% src='https://drive.google.com/uc?id=1UWP0Wmqk-mA3UJ9elyGpqdxtUlA_44cE'/> </td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "I-LSFs_vPRgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: With regards to curse of dimensionality in context of RBF Kernel : it seems like the distance which can range between (-inf, +inf) is getting squased between 0 and the peak of the gaussian curve. Does this help alleviate the problem a bit?\n",
        "---\n",
        "Not much, because our peak value changes with σ and so our peak could be anything may be infinity.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1y2-WLm-LCzxvQzd9XkATYReeVEg_agvv'/>\n"
      ],
      "metadata": {
        "id": "36vGAosmw-qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### ***Question***: So to summarise we can say RBF-SVM is very much similar to KNN and σ in svm is similar to k in KNN, then why RBF-SVM is used.\n",
        "---\n",
        "Answer lies in the runtime complexity of both.\n",
        "> For KNN : <b>O( n * d )</b> where n is no of data points and d is dimension of data<bR>\n",
        "> For RBF-SVM : <b> O( #SV * d ) </b> where  #sv is no of support vectors and #SV < n will always be true.\n",
        "\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1IhxXoVNBZHPKSJcBzgQ2qdjTzGfpLrIu'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=16QajZ3a-XNPtcRKcJUwIXnD5n9UWj95r'/> </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "         <td colspan=2 text-align=\"center\" > <img width=50% src='https://drive.google.com/uc?id=1JJg8IP7MOJMv9Kt8pbNuGPlhK-xNNnUq'/> </td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "CFt8YK_byJId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### ***Question***: xi^T*xj does not give similarity but after applying kernel it gives the similarity so similarity is k(xi,xj) is it correct?\n",
        "---\n",
        "No xi^T*xj also gives similarity but as it is a dot product it gives angular similarity between xi and xj while k(xi,xj) give another type of similarity bases on the function we are using.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1VuaYMP1xAGCEoyXTcu2DWtz94tFf_GSP'/>\n"
      ],
      "metadata": {
        "id": "XG1-ImJW06fi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b> Support Vector Regression ( SVR )</b></h2>\n",
        "\n",
        "The main intuition behind SVR is to find a best fitting line such that <font  color=\"green\" > y ( actual ) - y'( predicted ) < &epsilon;</font> where  &epsilon; is the error value. We will not go in much depth of it as it is not used much.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1T781Qh03bWv8ttvDTPK4cn5RlLllgeYU'/>\n"
      ],
      "metadata": {
        "id": "tq7bO6iTU_Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Interpretability of SVM's</b></h2>\n",
        "\n",
        "There is no native intepretation of SVM but we can think of RBF-SVM as similiar to KNN then for any query point we can think of it as finding nearest neighbors.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=18Uefkx5ai-BrN-lb2KJlvxeS16bcUmhz'/>\n"
      ],
      "metadata": {
        "id": "L5S3kOPPXPCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Visual representation of SVM with different hyper-parameters</b></h2>\n",
        "\n",
        "\n",
        "\n",
        "> In scikit-learn, we use <font size=5><b>&gamma;</b></font> ( inverse of &sigma; ) as our hyperparameter in SVM-RBF. we can also look for other parameters like coef0 , cache_size in more details [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1VdBKs4Gq18emZKY63wvGWiJDnj_dBUj-'/>\n",
        "\n",
        "\n",
        "Another interesting visualization you can see is [here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py).\n",
        "we can observe that SVM-RBF is very similar to KNN in region of data points, but region where there is no data point they differ. In the region of no data points, for any test point SVM-RBF will still predict but with a low probability.<br><br>\n",
        "<img width=80% src='https://drive.google.com/uc?id=1KU6es-WY7aoqUFAjeRDD68TmXErSjx64'/>\n",
        " <img width=80% src='https://drive.google.com/uc?id=1PlesIJpysszwBatoG63LFUD98T6k36Ri'/>\n",
        "  <img width=80% src='https://drive.google.com/uc?id=1ovNT5eFqpa_vY5BJux6RxgBm1h5SAjgL'/>\n"
      ],
      "metadata": {
        "id": "eYnizKP8Zup1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: For any domain specific kernel is there any way we can incorporate in the same code? or we can use only the kernels implemented in this sklearn ?\n",
        "---\n",
        "Yes, you can create your own kernel and call it in sklearn code. Refere [this](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#:~:text=squared%20l2%20penalty.-,kernel,-%7B%E2%80%98linear%E2%80%99%2C%20%E2%80%98poly%E2%80%99%2C%20%E2%80%98rbf) for more details.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1qxrM9_dp0uOs-CqV46VrwVP2zhKeuLKa'/>"
      ],
      "metadata": {
        "id": "SNYvHzzefxB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Another important effect of hyperparameter like <b>C and gamma </b>you can observe is: </p>\n",
        "\n",
        "\n",
        "> - If we move horizontally in this image you can observe the effect of gamma ( &gamma; ) or &sigma; you can say. We can observe how model overfits with increasing gamma ( &gamma; ) or decreasing &sigma;.<br>\n",
        "> - Similary if we move vertical, we can see the effect of increasing C i.e. model overfits, more complex non-linear boundaries.\n",
        "<br><br>\n",
        "<img width=80% src='https://drive.google.com/uc?id=1cFlJkTuGcCbJXXwKqmx6Qu9JDf7JTZC6'/>\n",
        " <img width=80% src='https://drive.google.com/uc?id=1YQo_yNR1EscGlStvsYyXRSf7Mzn7gg5y'/>\n",
        "  <img width=80% src='https://drive.google.com/uc?id=17XPQYsTNnJbLyLcO7lETWC5HT8Hur4Y8'/>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_px0_BM2gTkV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sy48vTnOfS1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}