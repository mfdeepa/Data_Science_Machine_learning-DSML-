{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on June 10, 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/28569/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "*   Regularisation:\n",
        "    *   What are Regularisation techniques?\n",
        "    *   How do they address Bias-Variance Trade-off?\n",
        "    *   Lasso and Ridge Regression.\n",
        "    *   Advantages and Dis-advantages.\n"
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularisation Techniques:\n",
        "\n",
        "- Regularisation techniques are used in Linear Regression along with the Loss function (be it Square Loss, Log loss, Hinje Loss and Huber Loss) to ***reduce overfitting*** and try to balance the Bias-Variance Trade-off."
      ],
      "metadata": {
        "id": "2ExToxIIk_-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L1 - Regularisation:\n",
        "- L1 regularisation is also known as *Lasso* or *Tikhonov* regularisation or L1-norm.\n",
        "\n",
        "- Cost function while using L1 regularisation looks like:\n",
        "\n",
        "    $$\\min_{w_j} \\mathcal{L} (Loss function) + \\lambda \\sum_{j=0}^d |{w_j}| $$\n",
        "\n",
        "  *where:*\n",
        "\n",
        "    * Loss function could be either of Squared Loss, Log Loss, Hinge Loss, Huber Loss.\n",
        "    * d: Number of features. ($j:0→d$ so as to include $w_0$ as well)\n",
        "\n",
        "\n",
        "- Weights of useless features becomes zero.\n",
        "\n",
        "- Plot of an absolute function ($|{w_j}|$) looks like:\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1ZATxtsfhd1dPz5Up6mWaom_dX0oe-XNH'>\n"
      ],
      "metadata": {
        "id": "-XvqLg50lSuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L2 - Regularisation:\n",
        "- L2 regularisation is also known as *Ridge* regularisation or L2-norm.\n",
        "\n",
        "- Cost function while using L2 regularisation looks like:\n",
        "$$ \\min_{w_j} \\mathcal{L} (Loss function) + \\lambda \\sum_{j=0}^d {w_j}^2 $$\n",
        "\n",
        "    *where:*\n",
        "    *   Loss function could be either of Squared Loss, Log Loss, Hinge Loss, Huber Loss).\n",
        "    *   d: Number of features. ($j:0→d$ so as to include $w_0$ as well)\n",
        "              \n",
        "- Weights of useless features becomes very small (Close to Zero).\n",
        "\n",
        "- Plot of quadratic function (${w_j}^2$) looks like:\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1KhChxtLsfO9wRdzFaxBBoY3uCT-dWaQI'>"
      ],
      "metadata": {
        "id": "C_9cOxWGGLsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question:*** Do you notice any problem with L1 regularisation?\n",
        "\n",
        "Mathematical Explanation:\n",
        "- In Gradient Descent, we update the weights by calculating derivative of Loss function wrt to weights.\n",
        "- 2nd component in the Cost function is not differentiable at zero!!!\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=16LLvzW3GaCB4cyVORswHF-VOAWmV9nK1'>\n",
        "\n",
        "- So, how do we fix it?\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1qPqL4JmbR-l8JtMyssL68ebgqxAZ92dF' width=\"825\" height=\"400\">\n",
        "\n",
        "- But one may ask, why this is ok?\n",
        "$\\frac{d|w_j|}{dt}\\Bigr\\rvert_{w_j = 0} = 0$\n",
        "\n",
        "- The answer to above equation lies in how weights are updated in Gradient Descent:\n",
        "\n",
        "- ***Quick Recap:*** How weights are updated in Gradient Descent Algorithm.\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=14RCxy8tjD0Q4Kb1Iwc-YK4IE2BvsWXfo' width=\"825\" height=\"400\">\n",
        "\n",
        "- In the update step we do the following:\n",
        "    - ${w_j}^{new} = {w_j}^{old} - \\eta \\frac{\\partial \\mathcal{L} }{\\partial w_j} $\n",
        "    - If $\\frac{\\partial \\mathcal{L} }{\\partial w_j}\\Bigr\\rvert_{w_j = 0} = 0$, then ${w_j}^{new}$ also becomes zero in subsequent update.\n",
        "\n",
        "- To summarise, $\\frac{d|w_j|}{dt}\\Bigr\\rvert_{w_j = 0} = 0$ is ok $\\because {w_j} = 0$ means that ${f_j}$(feature) is useless.\n",
        "\n",
        "***Interview Alert!!!***\n",
        "Asked in MAANG. Interviewer would ask why $\\frac{d|w_j|}{dt}\\Bigr\\rvert_{w_j = 0} = 0$. Why not some other value other than 0?\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egW7E4sOHfTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interesting property of L1 regularisation:\n",
        "- L1 regularisation results in sparse solution. i.e. all less useful feature's weight becomes 0.\n",
        "- On other hand in L2 regularisation, all less useful feature's weight becomes close to 0, but often not exactly 0.\n",
        "\n",
        "Can you guess why this happens? Let's try to find the answer intuitively.Again the answer lies in how the weights are updated in L1 and L2 regularisation.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1WXLLMVTzXBovaRJSMp8Pxrsuz9Wisv3o' width=\"825\" height=\"400\">\n",
        "\n",
        "Geometric Intuition:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1cMSTeM6aDQwRYC1QvukyPOnfXTrISmN5' width=\"825\" height=\"400\">\n",
        "\n",
        "- In L1 regularisation, ${w_j}^{new}$ becomes 0. But in L2 regularisation as we approach 0, gradient also reduces. So ${w_j}^{new}$ approaches 0, but may not becomes 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "m0tV3nmRggvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disadvantages of L1 regularisation:\n",
        "- If d > n (d is dimensions and n is #datapoints.):\n",
        "    - Typically a situation in GNOME data.\n",
        "    - L1 regularisation will lead to atmost \"n\" non-zero features even if more than \"n\" features are useful.\n",
        "- When we have lot of correlated features:\n",
        "    - If L1 regularisation is used, then one of the feature will be non-zero and other correlated features become 0.\n",
        "    - This is sometimes okay. But while using weights for feature importance, it will lead to wrong interpretation and messup feature importance."
      ],
      "metadata": {
        "id": "ka2YKPZijQy4"
      }
    }
  ]
}