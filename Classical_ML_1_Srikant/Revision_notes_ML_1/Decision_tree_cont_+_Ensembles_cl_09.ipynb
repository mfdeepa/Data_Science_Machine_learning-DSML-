{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**lecture link :** https://www.scaler.com/meetings/i/dsml-advanced-ensemble-bagging-2/archive"
      ],
      "metadata": {
        "id": "hYCz-t15fTzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content"
      ],
      "metadata": {
        "id": "4cjlgrAefbwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recap (2:00 - 24:06)\n",
        "2. Gini-Impurity (24:10 - 56:52)\n",
        "3. Overfit Vs Underfit (56:52 - 1:06:56)\n",
        "4. Sklearn Library ( 1:07:21 - 1:15:00)\n",
        "4. Geometrical Interpretation (1:16:20 - 1:20:27)\n",
        "5. Run time Complexity (1:36:20 - 1:45:00)\n",
        "6. Regression using a decision tree (1:46:31 - 1:59:45)\n",
        "7. Multi class classification (2:01:00)\n",
        "8. Interpretability (-)\n",
        "9. Feature Importance (2:03:00 - 2:11:00)\n",
        "10. Summary (2:11:00 - 2:12:20)\n",
        "11. Ensembles (2:12:20 - 2:28:30)"
      ],
      "metadata": {
        "id": "p_C5a606ff3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "d6cc9gDofifX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In the previous lecture we saw how a decision tree is built basing on the concepts like **Entropy** and **Information Gain.**\n",
        "* We also saw how **Recursive Partitioning** works in building a decision tree.\n"
      ],
      "metadata": {
        "id": "U3MFkye-fn8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick racap of the concepts and application of the concepts using the example which we saw in previous class."
      ],
      "metadata": {
        "id": "B5ptmFSkf5le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given a dataset where we have categorical features like Outlook, Temperature, Humidity and Windy basing on which we have to determine whether we can play tennis or not.\n"
      ],
      "metadata": {
        "id": "3qIbeThDhR8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, here what we do is split the data basing on the **Information Gain**\n",
        "* Where information gain is give by :\n",
        " * IG = Entropy at parent node - Weighted entropy at the child nodes\n",
        "* We pick feature which has maximum information gain\n",
        "* Here in the example we can see Information Gain for feature \"Outlook\" is maximum. So, we choose the same for splitting."
      ],
      "metadata": {
        "id": "SL6ELEHQjfb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1E2ZsduDaD-onw1ZAJKVgNeqLAqlP68cp'>\n"
      ],
      "metadata": {
        "id": "5nYjQ-jEoPbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens next.?**"
      ],
      "metadata": {
        "id": "3gejQHxNpi2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* We keep on splitting further the each child node in such way that the information gain is maximum for the next child nodes.\n",
        "* We keep on doing the same till we get a pure node.\n",
        "\n",
        "* This is called as **Recursive Partitioning**"
      ],
      "metadata": {
        "id": "ncu37BSno9KJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a pure node.?**\n",
        "* A node which has data belonging to only one class."
      ],
      "metadata": {
        "id": "yeNDtovYqW7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=14ivyDPcnRcXFzqWPQtgOgXoYHhVsZI00'>\n"
      ],
      "metadata": {
        "id": "DCh_RvfRBhP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **key lesson** here is\n",
        "* At every node of the decision tree we have to try every featutre and possible split to pick the best decision to split on\n",
        "* The best criteria is based on Information Gain, decision which has maximum information"
      ],
      "metadata": {
        "id": "nT-mbNXnp5kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1qHlnRBD281GCH0gTp9Lkv_G-VdaZGRt3'>"
      ],
      "metadata": {
        "id": "txjcnnRlB4y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if there are lots of features?**\n",
        "\n",
        "* If there are lots of features it takes a lot of time to check every possible split and compute entropy\n",
        "\n",
        "**So, what can be done about that.?**\n",
        "* We can use distributed computing or multi-processing\n",
        " * **Distributed Computing** is a process in which the data is split and worked on different cores.\n",
        "* We avoid using Decision trees as when the dimensionality is high a simple linear seperator can be used, by using logistic regression\n",
        "* Either use Random forest or GBDT\n"
      ],
      "metadata": {
        "id": "ObC3BG5G5fUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we use PCA for dimensioanlity reductionality.?**\n",
        "* PCA is principal component analysis which is used for dimensionality reduction\n",
        " * it is used to reduce data from $d$ dimensions to $d'$ dimensions where $d' < d$\n",
        "* Yes we can use it, but it might not be helpful always as PCA do not look at our y values\n",
        "* So, we can try and see if it works\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gFXV7jZs-Alg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1E4OjOt-0MJ-8Abebr6IOElYhnV3L2W_z'>"
      ],
      "metadata": {
        "id": "BcSmXRuKCcdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gini - Impurity"
      ],
      "metadata": {
        "id": "c9B5pIIGC6Ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini impurity is also a measure like entropy which is used to measure purity\n",
        "let GI be gini impurity\n",
        "* Gini Impurity is given by\n",
        " * GI(y) = $1-∑_{i=1}^k(p(y_i)^2)$\n",
        " * For a binary classification:\n",
        "   * GI(y) = $1-[(p(y_i=1))^2+(p(y_i=0))^2]$"
      ],
      "metadata": {
        "id": "5hlISRDXEP0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zBVRJWvFWt3PPMCqqDIlrc9MWVorSNQZ'>"
      ],
      "metadata": {
        "id": "jwSOElN6GOFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Gini-Impurity compares with entropy?"
      ],
      "metadata": {
        "id": "CNtD_CGlGUtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**case 1:**\n",
        "\n",
        " let p($y_+$) be 0.5 and  p($y_-$) be 0.5\n",
        "* Here the entropy is 1\n",
        "* GI = 1 - (0.25 + 0.25) = 0.5"
      ],
      "metadata": {
        "id": "1KGDXKZDGkTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**case 2:**\n",
        "\n",
        " let p($y_+$) be 1 and  p($y_-$) be 0\n",
        "* Here the entropy is 0\n",
        "* GI = 1 - (1 + 0) = 0"
      ],
      "metadata": {
        "id": "ctVsczzoHhGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the above two examples we can see that Gini-Impurity is high when entropy is high and it is low when the entropy is low\n",
        "* When the nodes are pure i.e for\n",
        " * if p($y_+$=1) and p($y_-$=0)\n",
        " * if p($y_+$=0) and p($y_-$=1)\n",
        "\n",
        " the Entropy and Gini-Impurity are zero\n",
        "* when the pronanility of p($y_+=\\frac{1}{2}$) and p($y_-=\\frac{1}{2}$) the entropy and Gini- Impurity are maximum\n",
        "\n",
        "* So, we can conclude that Gini-Impurity has same behaviour as entropy.\n",
        "  "
      ],
      "metadata": {
        "id": "oa8ubKf-HxyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1ei3q-Wt-Ts9_9BEV_FOKJZ1rqH68X3ck'>"
      ],
      "metadata": {
        "id": "ccCZ0LltIGrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Gini-Impurty is preffered over Entropy?**\n",
        "\n",
        "* Let's see the formula for both\n",
        " * Entropy = H(y) = $-∑_{i=1}^k p(y_i)*log(p(y_i))$\n",
        " * Gini-Impurity = GI(y) = $1-∑_{i=1}^k(p(y_i)^2)$\n",
        "* Here, as log is more computionally expensive than simple squaring, we choose GI and avoid entropy.\n"
      ],
      "metadata": {
        "id": "ov_rxw_MKUJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1w4R3c9MIzxstTALQkEE1pfCnrIrZzR1S'>"
      ],
      "metadata": {
        "id": "DlVlR4m8MYh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can get Information gain by using Gini-Impurity also,\n",
        " * Information Gain = Weighted GI of the child nodes - GI of parent node."
      ],
      "metadata": {
        "id": "MNdMpd8fMw2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do you split for numerical features?\n",
        "\n"
      ],
      "metadata": {
        "id": "F_wrwHyIOotR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have categorical features we can simply split basing on features, but how do we do the splitting for numerical features?\n"
      ],
      "metadata": {
        "id": "zC1hYpFMPyWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let us consider a numerical feature $f_1$ of $n$ values and categorical feature $f_2$\n",
        "* We compare the each value of $f_1$ with a threshold and split them basing on the threshold.\n",
        "\n",
        "\n",
        "**But, how do we choose the threshold?**\n",
        "* First we arrange $f_1$ in increasing order and set each value of $f_1$ as threshold and calculate the IG of that split. Which gives $n$ IG values say\n",
        "IG$_1^1$,IG$_2^1$,IG$_3^1$.....IG$_n^1$\n",
        "\n",
        "* Now, we compare these $n$ GI values of numerical feature and IG values of categorical feature (IG$_1^2$) and choose the split has maximum Information Gain (IG).\n"
      ],
      "metadata": {
        "id": "MlF4VB3NQJVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1CD3wKwzwFOdFwwEXtsfp-GHD3Bo9BWYW'>"
      ],
      "metadata": {
        "id": "pr4aPFDJWeX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1VTifugVGsRwCTRbncdKhHobuthrNCk7w'>"
      ],
      "metadata": {
        "id": "EaoucTltWlNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If the IG's of two different features are minimal you can pick any one and do the splitting."
      ],
      "metadata": {
        "id": "6kAgu9RBV19i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Computing IG for every feature is not very computationally efficient. so, algorithms follow set of rules to bin it carefuuly.\n",
        "* The purpose of binning is to make it more computationally efficient.\n",
        "* There are some techniques to do binning but the simplest binning method is to use Quantiles (Q1,Q2,Q3,Q4)"
      ],
      "metadata": {
        "id": "tXMxhYI_U0co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfit Vs Underfit"
      ],
      "metadata": {
        "id": "fgABfTXDWUzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When do you think we will overfit a decision tree model?**\n",
        "\n",
        "* We can say that the model is overfit when we go on splitting, which increases the Depth of the tree.\n",
        " * **Depth** is the distance from the root node to the farthest leaf.\n",
        "* In simple terms we can say that , as the depth increses we overfit more and more\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1VHFXJCFKHwmiHTnBDAXS1MUMyVmLo0dw'>\n",
        "\n",
        "**But why?**\n",
        "\n",
        "* As the depth increases the data set becomes smaller.\n",
        "* At the leaf node i.e (pure node) the number of data points will be very less in number, which might be the outliers or noise.\n",
        "* This results in Overfit.\n"
      ],
      "metadata": {
        "id": "bQN6eRoSW5OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1QdjAJqW1aUd4DGUXEHMlLloyvu1-rytJ'>"
      ],
      "metadata": {
        "id": "LJsfo0gIZf28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, what if the depth is too low?**"
      ],
      "metadata": {
        "id": "ncN1il-6ZuMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When the depth is too low it results in, **shallow tree**, a tree which has a very low depth.\n",
        "Let us assume a tree with depth 2, and has 500 +ve points and 200 -ve points at it's leaf node.\n",
        "* so for every query point $x_q$ we label it as +ve.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=17CK5fc4hjWJoVQOLzs1RKL8iopA480jD'>\n",
        "\n",
        "**What if the depth is 0? (Extreme case)**\n",
        "\n",
        "We just decide on the root node's data points which is not efficient answer.\n",
        "This results in **Underfitting**"
      ],
      "metadata": {
        "id": "71PaeiyNZ42H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, here depth is the control\n",
        "* There methods to control depth, by giving some conditions like\n",
        " * Split only if atleast $n$ data points are there\n",
        " * Split only if IG is greater than some threshold value $x$\n",
        "* so, here **Depth** is our **Hyper parameter**"
      ],
      "metadata": {
        "id": "kxOZUOeLbjO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, let's go through the library and see the terms"
      ],
      "metadata": {
        "id": "HdGe7uJ8cwfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Criterion = gini**, states that make splits basing on gini impurity\n",
        "* **Splitter = best**, states that make the split basing on the best computed value instea of random values, these random values are used in Extremely Randomised Trees.\n",
        "<img src='https://drive.google.com/uc?id=1m1xst72zil-C-lTsiorVRP8IcidHK5Hx'>\n",
        "\n",
        "* We can set maximium depth upto which a tree can split by using **'max_depth'**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HXEbToB4eUYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **min_samples_split** is used to set the minimum number of data points required to split further. which helps us to control depth which therefore prevents **overfit**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1eLlkvweWTXbjoIEkjvITReAtV8F_5jXx'>"
      ],
      "metadata": {
        "id": "1QnSJF8viqwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **min_samples_leaf** helps us to set the minimum number of samples which a leaf node can have\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1RkFfAuVD5vEzhUoCciRVboLu2MT4Mo0_'>"
      ],
      "metadata": {
        "id": "qcA4o8Cwi50s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **max_leaf_nodes** helps us to set the maximum number of leaf nodes that a tree can have. This also used to control depth of the tree.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vnP-HaMRnNAc3a3AfRKlHulln8up2h1E'>"
      ],
      "metadata": {
        "id": "aScvhgoLjAhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **max_features** is used to set the features to be considered while deciding the best split.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_7SOrt9M_aHLqbFtzSG_tpL8gmcHXgyg'>"
      ],
      "metadata": {
        "id": "epKABjgTjHtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **n_jobs** in random forest uses this paraemeter to give number of  processes that can be used to split these, in multi core system.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Wmeo10UCzSsvtt5scPGSSStxT1I3x-33'>"
      ],
      "metadata": {
        "id": "CLdB2Yo-jI7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's see another term **\"Decision Stump\"**"
      ],
      "metadata": {
        "id": "4G8cXuyHkCbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A Decision Stump is a Decision Tree with depth 1.\n"
      ],
      "metadata": {
        "id": "EiaGZzuHu1Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1LW9DYPPyKUx5v50TxBAeirQOcEnUZT_F'>"
      ],
      "metadata": {
        "id": "eSh0pNLDvXf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Geometrical Interpretation\n",
        "\n"
      ],
      "metadata": {
        "id": "gA9XSoUxkOsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume we have 2 features $f_1,f_2$\n",
        "* Geometrically every split is a axis parallel hyper-plane which devides your data space\n",
        "\n",
        "* In case of a **shallow tree**, the depth is less which means the splits are less which there by infer that the **number of hyper-planes deviding the data space are less** i.e less chances of overfitting"
      ],
      "metadata": {
        "id": "OBvLZRRBv4y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Yxhita92ZYwIHSUbIoNWZGygemAI76m5'>"
      ],
      "metadata": {
        "id": "8Viyn6RIxPIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happes when the depth is high.?**"
      ],
      "metadata": {
        "id": "khyQb4JwxXlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the depth is high, we are breaking the  data space into many small divisions.\n",
        "* These small spaces very few data points , those might also be outliers or noise.\n",
        "* Hence the higher chances of overfitting."
      ],
      "metadata": {
        "id": "2Q9y2gAGxewW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1JMVOQ5vfvujC4ViAM2k9HcRGamMXNz80'>"
      ],
      "metadata": {
        "id": "CoNk0cX4yAoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the impact of the outliers?**\n",
        "* Outliers impact a decision tree when the **depth is high**\n"
      ],
      "metadata": {
        "id": "sPq9Mzx7yPQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is Standardisation  required for Decision tree?**\n",
        "\n",
        "* Though standardisation is required in optimisation based problems like Linear Regression, Logistic Regression, PCA, as standardisation **doesn't effect the entropy or the Information Gain** of the data it doesn't add any value in Decision tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcyPVYbaWMNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1l44qKs25iQvz5qJfaQ2rRNB19RBc9SiA'>"
      ],
      "metadata": {
        "id": "3aqO0LIAXPdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* How can we encode  a categorical feature with mnay categories like a zip code to a decision tree?  \n",
        "\n",
        "* We can do **\"no encoding\"**\n",
        " * if we did not do encoding, data set becomes too small\n",
        "* We can do **target encoding**\n",
        " * covert to numerical and then give it to decision tree\n",
        "* We cannot do **one hot encoding**\n",
        "  * Because that increases the dimensionality of the data\n",
        "\n",
        "**key lesson:**\n",
        "* Appropriate feature encoding depends on the model that you are using like if you are using logistic regression you can use one hat encoding but not in decision tree.\n"
      ],
      "metadata": {
        "id": "qbVwTXbJXZBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1gtSEg36jCNkhdaAYOPuctlWEqJEH7l2j'>"
      ],
      "metadata": {
        "id": "tAQzP0YoDDAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run time complexity"
      ],
      "metadata": {
        "id": "sdCwgPjQ4ur3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let n be the number of data points , m be the number of nodes and d be the depth of the tree\n",
        "\n",
        "* The **Time complexity** of the tree of depth d is O(depth)\n",
        "* The **Space complexity** of the tree is O(m)\n",
        "\n",
        "Here,  depth is the function of number of nodes i.e log(m)   \n",
        " * If $d_{best}$ is computated using cross validation the decision tree will be very efficient at runtime.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4uADt855zMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1lgpPR9T2IxGxWsmUH9D8hO-5NTQtJFqE'>"
      ],
      "metadata": {
        "id": "MEd1KnQfB0CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to do Regression using a decision tree?"
      ],
      "metadata": {
        "id": "qthT9V2UBVCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's see what we were doing in Classifiaction first\n",
        "* In classification we had entropy and Gini Impurity but here fro regression we have to find an alternative for the same.\n",
        "* In leaf node when we had few values we given the query value the label of **majority data points** in the leaf node\n",
        "\n",
        "**What do we do in the Regression?**\n",
        "* In regression we take the mean or median of all values in the leaf node and give that value to the query node\n",
        "\n"
      ],
      "metadata": {
        "id": "RlfrJMoiBg8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1-yRG4noCrjUYUll3iKna7lsZ5flGofYO'>"
      ],
      "metadata": {
        "id": "bXkBdbYpH_ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But now, what is the alternative for Entropy**?\n",
        "* here, as we saw in linear regression already,we can use **Mean Squared Error or Median Absolute error**\n",
        "\n",
        "Let us assume a data D at root node with $y_i$ points which is split into D1 and D2\n",
        "* Now we calculate the MSE of these $y_i$ points in parent node and also the weighted MSE for the child nodes\n",
        " * Now, the difference between the MSE of parent node and weighted MSE of child nodes can be used as the criteria.\n",
        " * (weighted MSE of child nodes) - (MSE of parent node)\n",
        "\n",
        "* MSE is lowest when all $y_i$'s are same and high when they are diverse"
      ],
      "metadata": {
        "id": "njYXMnDqFtom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=10cOcLPE1gu2uwGkR91UdhjVa5WQSe-l_'>\n",
        "\n"
      ],
      "metadata": {
        "id": "fdr81NSnIKiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see how the MSE is calculated.\n",
        "\n",
        "let us assume we have a data of $y$ data points\n",
        "* We consider the mean or medain of these points as predicted value i.e $ŷ_i$\n",
        "* Now we calculate the MSE or MAE for these points, this will be the MSE of parent node.\n",
        "* After the data is split\n",
        " * We now consider the mean of all values ($y_i'$) in child node as predicted value $ŷ_i'$\n",
        " * we now calculate the MSE or MAE for these in each child node and then calculate the weighted  MSE of the child nodes.\n",
        "* Then we find the difference and use this as the critearia to be compared among the features to decide the split."
      ],
      "metadata": {
        "id": "-4QxqIriIiYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vjLpifPMjCs6P7-iIrUsdHyviiFW0f7p'>"
      ],
      "metadata": {
        "id": "cPDDoE7IL4fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions"
      ],
      "metadata": {
        "id": "AxN_rwRjQ0bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is MAE better or MSE?**\n",
        "* MSAE would be a better choice as it is **resistant to outliers**"
      ],
      "metadata": {
        "id": "eUcQpacdNZu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1a5FsNMltDHiteGJeER5aUrcwEq3N2kFU'>"
      ],
      "metadata": {
        "id": "QTOeAiXfa3sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **What do we do if we have imbalanced data?**\n",
        "\n",
        " We have to rebalance the data beacuse the Entropy and Gini Impurity are fucntions of probability of classes\n",
        "* Assume  a condition where there is an imbalance in the root node\n",
        "  * having 99% positive data\n",
        "  * This skews the entropy or Gini Impurity\n",
        "* We can rebalance in many ways like\n",
        " * using class weights\n",
        " * up - samplinig\n",
        " * SMOTE can be used.  "
      ],
      "metadata": {
        "id": "ODkNXTitN74U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DapAf45-GyB4NZ_jjB2Mu-sDgFgk7q3R'>"
      ],
      "metadata": {
        "id": "Zp0Z2b7BbGzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi class classification"
      ],
      "metadata": {
        "id": "Hel_aeClPzzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How will DT work for multi class classification?"
      ],
      "metadata": {
        "id": "VFh6NmauF8_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whether it is binary or multiclass, DT works the same.\n",
        "\n",
        "We just need to calculate the Entropy or Gini Impurity.\n",
        "\n",
        "However, there is a catch\n",
        "* at the leaf node if we have more classes we take the majority class.\n"
      ],
      "metadata": {
        "id": "H1s9aViHQFlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=17owOUzkFT4ba2LkkU23M_x5Xlkeb9eIA'>\n"
      ],
      "metadata": {
        "id": "Wot1LN-RWnrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretability\n",
        "\n",
        "* Decision trees are very easy to interpret because they can be written as combination if-else statements\n"
      ],
      "metadata": {
        "id": "Gm9XDwaqQrJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance"
      ],
      "metadata": {
        "id": "I7Xt7WagcJHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In case of logistic and linear regression after standardisation we can just consider  absolute value of weights which give us feature importance\n",
        "\n",
        "* In decision tree we compute a **Normalised Infromation Gain**\n",
        "\n",
        "Let us consider a feature $f_i$ which is used twice in splitting a decision tree with 10,000 points at the root node\n",
        "\n",
        "* At first split let the number of data points be 5000 and Infromation Gain is IG$_1$\n",
        "* Let number of data points at second split be 500 and Infromation Gain is IG$_2$\n",
        "\n",
        "* We calculate feature importaance of $f_i$ by\n",
        " * $f_i$ = IG$_1$ * $\\frac{5000}{10,000}$ + IG$_2$ * $\\frac{500}{10,000}$\n",
        "* These values of normalised infromation gain gives the feature importance.\n"
      ],
      "metadata": {
        "id": "Xni3XadmXMX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12gJv36lGDumsuJ0bdafTB1Iq8bPpbs68'>\n"
      ],
      "metadata": {
        "id": "RTSKeOpqbaE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Building an optimal Decicion Tree is a NP-Complete problem which is exponentially hard time complexity.**\n",
        "* So, we do **Greedy approximation**, which picks the feature which gives the best Information Gain"
      ],
      "metadata": {
        "id": "cXeqnEt5b9-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1MPGvP0RVuw_yiXomwYH_D1DRms2VyzjH'>"
      ],
      "metadata": {
        "id": "bhHtVq5kB-bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Decision Tree"
      ],
      "metadata": {
        "id": "xWMzXsKF7Y21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarise everything what we have learnt about decision tree:\n",
        "\n",
        "1.   Decision Trees work well **when the d (depth) is not too large**\n",
        "2. They have a very low **Run-time complexity**\n",
        "3. **No standardisation** is needed.\n",
        "4. Depth is the key **hyper-parameter**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uTxatbr07ecp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1qAHiBvWa19rqX-lI49DmmdZ4ATwoW9AB'>"
      ],
      "metadata": {
        "id": "gPaE3__iBpWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembles"
      ],
      "metadata": {
        "id": "69Ik1Zaj8fQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The word ensemble in english means multiple things\n",
        "\n",
        "* Till now we have trained only one model\n",
        "\n",
        "* But what if we can **train multiple base learners or models** which are as different as possible and **combine them smartly**\n",
        " * Example: Instead of training one Decisioon tree we can train 100 decision trees and combine them\n",
        "* This is the key principle of ensembles\n",
        "\n",
        "* We can also train various machine learning models like Decision tree, logistic Regression, KNN and can combine them smartly.\n"
      ],
      "metadata": {
        "id": "vSUz2Vmj81s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1PvJuXnvX6OY1_uAq8eOXFDjYvuxWh3vc'>"
      ],
      "metadata": {
        "id": "a6reFKkJBvzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are four main types of ensemble\n",
        "1. Bagging\n",
        " * Example : Random Forest\n",
        "2. Boosting\n",
        " * Example : GBDT\n",
        "3. Stacking\n",
        "4. Cascading"
      ],
      "metadata": {
        "id": "4tH7R4Lu-1um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1njR8DUne8941A1S9jwxGcG8JkzWxlVGM'>"
      ],
      "metadata": {
        "id": "hHUIcfveB2vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging"
      ],
      "metadata": {
        "id": "3D93J6eW_-mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bagging is simply the **Bootstrapped Aggregation**\n"
      ],
      "metadata": {
        "id": "Jgq4u5EFABzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intution"
      ],
      "metadata": {
        "id": "RyXudRiEk1z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume a train data set $D$ with $n$ data points i.e $D_n$\n",
        "* Now, we **sample $m$ data points with replacement** to get $D'_m$\n",
        "* We do the sampling again for $m$ points to get $D'_2$, **Repeat the same for $k$ times** and we get $D'_k$\n",
        "* Now, we **train $k$ different models**($M_1,M_2,....M_k$) basing on the $k$ datasets obtained , there models are called **Base Learners**.\n",
        "* After training we **cross validate each model** **with remaining $n-m$ data points**\n",
        "* Now, we do **Aggregation**\n",
        " * We use majority vote for Classification\n",
        " * We use Mean/Median for Regression\n",
        ""
      ],
      "metadata": {
        "id": "CpgKZyAZlzhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Working :**\n",
        "  * When a query point ($x_q$) is given, we pass that point through all the $k$ models and aggrigate the output of the models."
      ],
      "metadata": {
        "id": "b4KxYF4Zo_Da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1G7iHV05GhViHb1t82gQJXze1NCfINMYh'>\n"
      ],
      "metadata": {
        "id": "snKHF1SxpzKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1oP6x44zrH2NvJ19EmCcBPuMEQBk1I9io'>\n"
      ],
      "metadata": {
        "id": "CpqBZrQqpz1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the challenge we are facing here?**\n",
        "\n",
        "* Here the major thing is the $k$ models should be different from one another as there is no use if all the models are same\n",
        "\n",
        "**But how to ensure that all the models are different?**\n",
        "* We can use different set of features or data points for different models which is **Feature Selection**\n",
        "* We can **tune the depth** of the tree\n",
        "* The models can be different if **number of points** $m$ in each sample  **is smaller**, This is **already hapenning in Row Sampling**  "
      ],
      "metadata": {
        "id": "cZPIklyEqk7Y"
      }
    }
  ]
}