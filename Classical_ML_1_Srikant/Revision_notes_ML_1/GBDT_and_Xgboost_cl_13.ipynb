{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lecture link:** https://www.scaler.com/meetings/i/dsml-advanced-gbdt-and-xgboost/archive"
      ],
      "metadata": {
        "id": "GiURrKom3W69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content"
      ],
      "metadata": {
        "id": "IVaXrqG13sKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recap (10:30 - 19:30)\n",
        "2. Questions(19:30- 46:20)\n",
        "3. GBDT (55:30 - 1:48:35)\n",
        "4. Xgboost (1:48:35 - 2:16:00)\n",
        "5. LightBGM (2:16:00 - 2:26:15)\n",
        "6. Adaboost (2:26:15 - 2:37:00)"
      ],
      "metadata": {
        "id": "NQouXjbz3wR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "ZjP_qHbNcWVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is pseudo residual?**\n",
        "\n",
        "we have studied this in previous lecture let's quickly recap that.\n",
        "\n",
        "* Let us assume we have $n$ number of data points\n",
        "and trained $m-1$ classifiers then the pseudo residual of $m^{th}$ classifier is given by differentiation of loss function with respect to final model of $(m-1)^{th} $  classifier\n",
        " * $res_{i,m} = [\\frac{d(L(y_i,F(x_i))}{d(F(x_i))}]_{(F_{m-1}(x_i))}$"
      ],
      "metadata": {
        "id": "Ev4B5n9zcZb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1N89WqSgoLwaTw4TOXEf1ski0jqm2XNwO' >"
      ],
      "metadata": {
        "id": "4Kts6MWcfW4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's assume our loss is squared loss\n",
        "* Therefore our loss becomes\n",
        " * $L(y_i,F(x_i)) = (y_i - F(x_i))^2$\n",
        "* now it's derivative is\n",
        " * $\\frac{d(L)}{d(F_m(x_i))} = - 2 (y_i - F_m(x_i)) $"
      ],
      "metadata": {
        "id": "zbgXNDP8iWZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we compute the pseudo residual?**"
      ],
      "metadata": {
        "id": "dCbLkY4fjZot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By comparing the above two equations we can say that\n",
        "* $res_{i,m} = 2 (y_i - F_m(x_i))_{F_{m-1}x_i} $\n"
      ],
      "metadata": {
        "id": "uNQ4_4mxjfkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1LT91ZmqU4pysZf337xqdwnsLJBYB36hX' >"
      ],
      "metadata": {
        "id": "_1yUJlpShufq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do we already have?**\n",
        "* We already have our data $(x_i,y_i)_{i=1}^n$\n",
        "* We also have the function $F_{m-1}(x_i)$\n",
        "\n",
        "So now all we have to do is to pass the $x_i$ in the function and use the values of $y_i$ to find the residual  $res_{i,m}$"
      ],
      "metadata": {
        "id": "Slpp29DXkTo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train our $m^{th}$ model on $(x_i, res_{i,m})$\n",
        "where $res_{i,m}$ is the residual of $(m-1)^{th}$ model and it is used to train the $m^{th}$ model"
      ],
      "metadata": {
        "id": "9l3CPW2bl3PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here the key idea is that pseudo residul is equal to the negative of the differentiation of the loss function with respect to final output of the previous model\n",
        "* So, for any loss function given which is differentiable, we can easily compute our residual\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9e_CvZtcSZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1JuRzFFhW3v7tafBl9pbLGsmAsKGcrhKk' >\n",
        "\n"
      ],
      "metadata": {
        "id": "-QNxSDuTh3z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions"
      ],
      "metadata": {
        "id": "camec7kVjra-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When are $Œ±$ calculated?**\n",
        "\n",
        "* Firstly, let's use $Œ≥$ instead of $Œ±$ from now on\n",
        "* So our $F_m(x_i) = Œ≥_0 h_0(x) + Œ≥_1 h_1 (x) +......+ Œ≥_m h_m(x)$\n",
        "* In the $m^{th}$ iteration when we are training the $h_m(x)$ we also compute the $Œ≥$ value by using a simple optimization."
      ],
      "metadata": {
        "id": "efTzpFgxeJQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens to the residuals in the test time?**"
      ],
      "metadata": {
        "id": "-GrPeVqwgyZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the puedo residuals only to train the base learners $h(x_i)$ whixh are shallow trees in GBDT to get the $y$\n",
        "* So in run time when a query point $x_q$ is given we just plug in the value of $x$ in eaxh our base learners and get the output.\n",
        " * $‚àë_{i=0}^M r_i h_i(x_q) = yÃÇ$"
      ],
      "metadata": {
        "id": "URuPvg98iUWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1PaBn5nxOlKr859RdZPoVtiSsKvZo1UBc' >\n"
      ],
      "metadata": {
        "id": "9I-ub9IZmn6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the benifit of training our base learners on the errors of the previous models?**"
      ],
      "metadata": {
        "id": "QyEBXKZTjbdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets us assume $x_i$ is a 4 dimensional vector $x$ = [0,1,2,0] and $y_i$ be 26.2 and all $Œ≥=1$\n",
        "\n",
        "* after training ,our first or mean model let the value be 12,i.e the model $h_0(x_i)$\n",
        "* we know $F_m(x_1) = Œ≥_0 h_0(x) + Œ≥_1 h_1 (x) +......+ Œ≥_m h_m(x)$, so the sum of all other models except the mean model should be 14.2.\n",
        "* So, here after every iteration as we are tarining on the residuals of the previous model,  the next models train on lesser and lesser values"
      ],
      "metadata": {
        "id": "xqDmGhZCjuh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1s1FLz84eW4L97hFUka3IOAC0q3Jxe0bE' >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Gd32nnumvN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if residual becomes zero?**\n",
        "\n",
        "Here **$m$ is our hyper-parameter**, so, we have to control $m$ and stop the model learning after the residual is minimum, or the model will **overfit**."
      ],
      "metadata": {
        "id": "LKlyoFZom37U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does this work for classification?**"
      ],
      "metadata": {
        "id": "ZC1XDonQoLtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume we have the data ($x_i,y_i$),but our values of $y_i$ be 0 or 1.\n",
        "* Now, we have the predict the probability of $y_i$ being 0 or 1 i.e $p_i$\n",
        "* for model $F_m(x_i)$ we want to predict $p_i$\n",
        "* The loss-function for the classification is log-loss\n",
        " * We can prove that $\\frac{d(L(y_i,F(x_i))}{d(F(x_i))} = (p_i - y_i)$\n",
        "\n",
        "* For model where $x_i$ = [0,1,3,1] and $y_i=1$\n",
        "  * let $h_0(x_1)$ be 0.4\n",
        "  * The sum of other models should be 0.6 ad so on it continues just like we saw in regression.\n",
        "\n",
        "When the $y_i$=0\n",
        "* The models after mean model tries to predict negative values so that final output becomes 0.\n"
      ],
      "metadata": {
        "id": "jfaFsq2Mxn4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1I2c-bxsgVreoPxR04gj46s_G0hcLUODj' >\n"
      ],
      "metadata": {
        "id": "6kHK3t2B1lue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1u-Oo_5Ofzn7hrmRCBScUddd-_EwXYWGg' >\n"
      ],
      "metadata": {
        "id": "QzsehH7c1tE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1LSTGiUi69qIY3_XjEe99mJJdOJcIU5_d' >\n"
      ],
      "metadata": {
        "id": "5pkH1Pli111H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1pjZuf3zDLeY60NHPHOrb-IKYddoNRGSv' >\n",
        "\n"
      ],
      "metadata": {
        "id": "z1C5miib1-yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GBDT"
      ],
      "metadata": {
        "id": "Kn93oDs-yg5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GBDT Algorithm"
      ],
      "metadata": {
        "id": "aHW7ul5x2JSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we have 3 inputs\n",
        "  * The data set, $(x_i,y_i)_{i=1}^n$\n",
        "  * A differentiable loss function $L(y_i, F(x))$\n",
        "  * The number of iterations $M$\n",
        "\n",
        "Let's see the steps in the algorithm\n",
        "\n",
        "\n",
        "1. Initialize the model $F_0(x) = arg\\ min_Œ≥ ‚àë_{i=1}^n L(y_i,Œ≥)$\n",
        "  * That is initialising the model with $Œ≥$ which minimises the sum of loss function across all  values\n",
        "     * This will be mean for sqaured loss\n",
        "     * and $p(y=1)$ across the dataset for log-loss\n",
        "\n",
        " * Let's consider for sqaured loss\n",
        "    * $F_0(x)= arg\\ min_Œ≥ ‚àë_{i=1}^n (y_i-Œ≥)^2$\n",
        "    * Here the only variable is $Œ≥$, so minimize that using Gradient Descent\n",
        "    * Therefore, differentiation of the loss function with $Œ≥$ gives $\\frac{d g(Œ≥)}{dŒ≥} = ‚àë_{i=1}^n -2 (y_i-Œ≥)$ which should we equated to 0, to get minimum\n",
        "       *  $‚àë_{i=1}^n y_i = n Œ≥$\n",
        "       * $ \\frac{1}{n}‚àë_{i=1}^n y_i= Œ≥$\n",
        "\n"
      ],
      "metadata": {
        "id": "DCaCyBfY5-u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now, we have to iterate $M$ times and build $M$ base learners beyond 1st base learner\n",
        "* So, the very first step is to calculate psuedo residuals\n",
        "  * $res_{i,m} = [\\frac{d(L(y_i,F(x_i))}{d(F(x_i))}]_{(F_{m-1}(x_i))}$ for all $i$ 1 to n\n",
        "\n",
        "* Now, train a base learner $h_m(x)$ on $(x_i, r_{i,m})_{i=1}^n$\n",
        "\n",
        "* Now compute $Œ≥_m$ by using the one-dimensional optimization problem, which uses the loss function to find the Œ≥, so that loss function is minimum.\n",
        " * $Œ≥_m= arg\\ min_Œ≥ ‚àë_{i=1}^n L(y_i , F_{m-1}(x_i)) + Œ≥h_m(x_i)$\n",
        "* Now update the model as\n",
        " * $F_m(x) = F_{m-1}(x) + Œ≥_mh_m(x)$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ih7OodQKSO-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The final output is the model $F_M(x).$"
      ],
      "metadata": {
        "id": "18ClRMLeXEFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1jlwNnKXWO2pCem4M4XWdYdKpTvqGIDKD' >"
      ],
      "metadata": {
        "id": "89mlJjrLWQI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=12e63puZrN4pke50UEqaLXL6yQx1TRokM' >\n"
      ],
      "metadata": {
        "id": "sQiw7IR1WXla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1aSNgvc4Zmvwsr1rADhNf8zM15dfPR_d-' >\n"
      ],
      "metadata": {
        "id": "GhEANl25XeSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting-Underfitting Trade-off"
      ],
      "metadata": {
        "id": "TWx0suhpY-JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that Regularization tries to avoid overfitting of the model.\n",
        "\n",
        "**How can we Regularize the GBDT model?**\n",
        "\n",
        "**What are the hyper-parameters we can use?**\n",
        " * $M$ - The number of base learners\n",
        "  * As $M$ increases, the model will overfit\n",
        "\n",
        "* The depth of base learners\n",
        " * As depth increases,model overfit\n",
        "\n",
        "The another method of regularization is\n",
        " * Shrinkage/ Regularizer/ Learning-rate"
      ],
      "metadata": {
        "id": "bYeoxBouZNjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1OehWNYBxUvBszDv8-VGFKaS4aefZbVZH' >"
      ],
      "metadata": {
        "id": "SVfhMWYXl1UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In shrinkage all we do is to add a regularization term $ùúà$\n",
        "in the model of $F_m(x)$\n",
        " * $F_m(x) = F_{m-1}(x) +ùúà\\ Œ≥_mh_m(x)$\n",
        " * This ùúà always lies between 0 and 1, this is a way to scale Œ≥, which is a hyper-parameter\n",
        "\n",
        "**What happens as ùúà increases?**\n",
        "\n",
        "* Let's see what happens when **$ùúà=0$**, we will have only $h_0(x)\n",
        "$ which is a very underfit model\n",
        "\n",
        "but **as ùúà increases, chances of overfitting also increases**\n",
        "\n",
        "* We have **same ùúà for all models in GBDT**"
      ],
      "metadata": {
        "id": "LWyjr5YYjs9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1CSNOZVJYp3NyRSjMTxeMj06Quve82fD9' >\n",
        "\n"
      ],
      "metadata": {
        "id": "qP7MyxlUmOD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1zTQ_3nJ8nWZgUzCVTjJuAUoY_uQ-qY_N' >\n",
        "\n"
      ],
      "metadata": {
        "id": "8HUckhJlmXAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What should be done to $M$ if ùúà is decreased to keep the performance same?**\n",
        "\n",
        "* Number of **base learners should be increased**, because as we are decresing ùúà we are restricting base learners contribution due to which the error increases\n",
        "\n",
        "\n",
        "We use ùúà even when the $Œ≥$ is there to avoid overfit, as there are high chances of overfitting"
      ],
      "metadata": {
        "id": "rUj8eJfKmgDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we add a regulariser to Œ≥?**\n",
        "\n",
        "* Yes we can add regularisation to vector $Œ≥$ whixh is\n",
        "$Œ≥$=[$Œ≥_1,Œ≥_2,Œ≥_3....Œ≥_M$]\n",
        "* We can even add other terms controlling the number of leaf nodes which also becomes hyper parameter\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kl17Us5ioDQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1EtZi-SVT7_ARPEuI2cxJ1QNOAQl0GVYJ' >"
      ],
      "metadata": {
        "id": "3OxoWxektIwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Gradient Boosting"
      ],
      "metadata": {
        "id": "Ha-3rVJ-tP3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The stochhastic gradient boosting follow the same framework as GBDT, but it also does row sampling and column sampling\n",
        "* We do this sampling because it's **randomisation as regularisation**\n",
        "* The libraries like Xgboost uses this.\n"
      ],
      "metadata": {
        "id": "A57y8oZgu0PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1b0t8tmqt6ksDPEK2Uqe-Q01IXUFPmSfC' >"
      ],
      "metadata": {
        "id": "cGQR2nNAvx4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train time"
      ],
      "metadata": {
        "id": "taXLTMrtv-jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We saw that Random forest is trivially parallelizable.\n",
        " because we could train them on different cores or system\n",
        "\n",
        "* But GBDT is the sequential model, Hence it is slow to train\n",
        "\n",
        "**How can we speed up GBDT?**\n",
        "\n",
        "* Row sampling and column sampling can speed up the model a little\n",
        "* While building each base leraners we can apply **parallelisation**, i.e one sub-tree can be built on a core and the other sub-tree can be built on another core\n",
        "* Even for bulding a single tree, **we have to calculate entropy of many features, this can be parallelised**"
      ],
      "metadata": {
        "id": "jaA33tW3yqWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12V2N2vzcO9mBSj40dWX_xqv_5QezuJYK' >"
      ],
      "metadata": {
        "id": "U_VqvCCl0mW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xgboost"
      ],
      "metadata": {
        "id": "4Pe4czVs1kVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Xgboost have a lot of code and algorithms optimized\n",
        " * Example : if you have numerical feature $f_i$, instead of tryinig all the values for thresholding, it build a histogram of data and use simple rules like quartiles and percentiles to make thresholding.\n",
        "* It also does multi core optimization."
      ],
      "metadata": {
        "id": "uDH-wH761qPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xgboost library terms"
      ],
      "metadata": {
        "id": "pGdVVzudUx0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Eta** or the learning rate is the shrinking/regularization term which we studier earlier\n",
        "* **min_split_loss** this gives the minimum Information Gain which you want for further split.\n",
        "\n",
        "**What happens if min_split_loss increases?**\n",
        "* If the min_split_value of the model is indreased, the splitting stops if the min_split_loss is not met. Due to this the depth of the decreases resulting in shallow tress, which results in the underfitting of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "bHwaJ4RCU2HS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1tvyj1KYedvVhaeIJIAh_ZillGmf5p5rE' >"
      ],
      "metadata": {
        "id": "ckHd2kWIXFH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **max_depth**, this parameter is used to set the depth of the base learners  \n"
      ],
      "metadata": {
        "id": "qjYNy3lhXApy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1k_JD_dg0las0-N03UWQ_jSzkIP1M-b9W' >"
      ],
      "metadata": {
        "id": "YjHHBAPwXizX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **min_child_weight** you can increase the weight of the child due to which the splitting stops if the erquired threshold is not met"
      ],
      "metadata": {
        "id": "OShdCyR2XCbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **subsample**. it's nothing but the row sampling\n",
        " * There are many ways of sampling, same with column sampling"
      ],
      "metadata": {
        "id": "1X8kzQ0UX8-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Lambda**, it is the used apply the L2 regularization on the weights, which is $Œ≥$ in our case"
      ],
      "metadata": {
        "id": "l0n-UauKYoym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Vvz_rxFrx0EAgwClXn8OFwkhZ_5eD9C0' >"
      ],
      "metadata": {
        "id": "ZUoUNjITZFFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Tree_methods**, there are many ways to build a tree, we can specify the method basing on the conditions"
      ],
      "metadata": {
        "id": "l49eF-03ZXCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=19vAWSdAFmHYcIh5pWgxPLneqyD8Eze1d' >"
      ],
      "metadata": {
        "id": "bL_BnJPVZtgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **n_estimator** in scikit learn API, which sets the number of base learners in the model"
      ],
      "metadata": {
        "id": "bqcDQpHsb6vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* These are some of the most used parameters"
      ],
      "metadata": {
        "id": "u2S8tbitZzTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1n0JuWHd8NerqKnYnWFQqIwKeNuJG7PUr' >"
      ],
      "metadata": {
        "id": "3qGNJ2_pacN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**how to implement random forest using Xgboost?**\n",
        "\n",
        "There few terms which are to be learnt like\n",
        " * **booster** which is to be set to gbtree as we are using a tree based model\n",
        " * **subsample**, this is used for row sampling and also a little column sampling\n",
        " * **num_parallel_tree** gives the number of parallel trees can be built, like while building a model $h(x)$, multiple ways of the same model can be built in seperate cores.\n",
        "  * if u set these value to 100,  100 trees are parallely built in random forest , and should also set **num_boostinig_roounds=1**,\n",
        "  so that only one base learner is built in 100 ways **\n",
        ""
      ],
      "metadata": {
        "id": "dxRylRHRcW97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1FS9Y5K8mhDawpJUMSaLV5KEbD0muHpPW' >"
      ],
      "metadata": {
        "id": "IcF-gSaEdtHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10mps_xf9i7cpGiRM-CjK5qhso57O2a7u' >"
      ],
      "metadata": {
        "id": "2sinnEmkfKyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "l8YrMxzKbUt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was built at microsoft, primarily for a faster GBDT\n",
        "\n",
        "It is typically faster than Xgboost because of the more code optimization\n",
        "  * One such strategy is **GOSS - Gradient based one side sampling**\n",
        "    * When we are building the $m^{th}$ model the points we have is $ x_i,res_{i,m} $, so here instead of considering all points we drop the points in which the $res_{i,m}$ is small along with a  sampling model where probability of getting large residual value is higher\n",
        "    * Here the key is to reduce the number of data points due to which the model becomes faster\n",
        "  "
      ],
      "metadata": {
        "id": "kgmf4KdlbYOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " <img src='https://drive.google.com/uc?id=1NGKKYLSyz8MzFKQtzxnh9PTUbGQoM9JP' >"
      ],
      "metadata": {
        "id": "qj6dmciC2nX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The other strategy is **Exclusive Feature Bundliing**\n",
        "   * Let us assume we have a categorical feature with 3 categories and if we do **one hot encoding**, for each row only one of them will always be set.\n",
        "   * so, what exclusive feature bundling does is it looks at all the dimensions and look at feature pairs if they are exclusive and group these features and then do target encoding\n",
        "   *  So, here the key objective of EFB is to reduce the number of features"
      ],
      "metadata": {
        "id": "Qp4giSc32mYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?id=1G2Ilv5_nhQ0rrX0KUp60v9UoqlDApWl4' >\n"
      ],
      "metadata": {
        "id": "cEXawqZG3KFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost"
      ],
      "metadata": {
        "id": "OEk0gCgD3S0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBost means Adaptive Boosting\n",
        "* We prefer GBDT over AdaBoost beacuse of the flexibility of choosing loss-function\n",
        "\n",
        "**What does AdaBoost do?**\n",
        "\n",
        "Let us assume a toy dataset with negatives and positives\n",
        "* At first we have a weak classifier which is a horizontal or a vertical plane\n"
      ],
      "metadata": {
        "id": "5E_BPuyA47o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Og9gvnSUpx3uqaS12NUybZatrwLWhIqZ' >\n"
      ],
      "metadata": {
        "id": "i8z4mHadE0aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* $œµ$ is the error of the model\n",
        "\n",
        "* $Œ±$ is how good our model is, whixh is considered as the weight of the model"
      ],
      "metadata": {
        "id": "Z2gPo0GhG1qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1e17PWvIvvhH0NeL_hAB45iFI8pBZAnPt' >\n",
        "\n"
      ],
      "metadata": {
        "id": "Ne6WGRK1FuMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After building the first model, we give more weightage to the erronous data while building the second model\n",
        "* Now as more weightage is given to the erronous data now the second model will be based on the erronous data\n"
      ],
      "metadata": {
        "id": "nkIJjNgUE0tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1EauFehFWBMZB7scQaBKgv9bcTYwv9N_f' >\n",
        "\n"
      ],
      "metadata": {
        "id": "teuVjrawE4-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Again after the second model there will be few erronous points in the classification, so these points will be given more weightage while building the next model\n",
        "* so, here what we are doing is giving weights to points proportional to the errors they are making\n",
        "* So, to get final model, we multiply each model with a weight and combine them"
      ],
      "metadata": {
        "id": "xDiKJIy0E5Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1PX8ikROK910A_ms432Wfm6CB0vt7yg9H' >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEz_g03eE7Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1cWTRYGLGzv0"
      }
    }
  ]
}