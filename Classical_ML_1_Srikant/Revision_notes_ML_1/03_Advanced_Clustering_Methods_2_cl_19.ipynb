{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "Hierarchical (Agglomerative) Clustering\n",
        "- Approach and Algorithm\n",
        "- Methods to Compute Distance b/w Clusters\n",
        "- Pros and Cons of Different Distance Methods in Hierarchical Clustering\n",
        "- Time and space complexity of Agglomerative Clustering\n",
        "- Limitations of Hierarchical Clustering\n",
        "\n",
        "\n",
        "DBSCAN\n",
        "- Density Based Clustering\n",
        "- MinPts and Eps : Density\n",
        "- Core, Border and Noise points\n",
        "- Density edge and Density connected points\n",
        "- DBSCAN Algorithm\n",
        "- Picking Hyperparameters : MinPts and Eps\n",
        "- Advantages and Limitations of DBSCAN"
      ],
      "metadata": {
        "id": "Df6fpLHDmQpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "GM7CrZRLmQtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agglomerative Clustering\n",
        "\n",
        "#### **Remember Agglomerative Approach from previous lecture?**\n",
        "\n",
        "1. At first, each point on it’s own is a cluster.\n",
        "2. Then, for each point, we find it’s closest neighbour (in terms of euclidean distance) and group it.\n",
        "3. Finally, we get 1 large cluster from n clusters before.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1FIvomu34pSJIkw2FPcGO5JEiKKoPvgyQ' width='400'>"
      ],
      "metadata": {
        "id": "9IYqaCEJmQ3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agglomerative Clustering Algorithm:**\n",
        "\n",
        "1. At first, each data point is treated as a cluster.\n",
        "\n",
        "2. Then, we need to compute the proximity matrix. Assume we have n points $p_1, p_2, p_3..p_n$.\n",
        "We will define a $n \\times n$ matrix **P** where each entry $P_{i,j}$ describes the proximity between point $p_i$ and point $p_j$.\n",
        "For example, $P_{1,2}$ denotes the euclidean distance between point $p_1$ and point $p_2$. As described below in the image, we will have a matrix **P** of size $n \\times n$. The entries in the diagonal are always zero. This is because distance between a point $p_i$ and itself is zero.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1P1c-XrkFekzgF5iMc77xyjFXEAw0v7Sh'>\n",
        "\n",
        "3. **Repeat:**\n",
        "\n",
        "  **3a.** Merge the two closest clusters → At first, find the smallest value in the **P** matrix. Let’s say that the smallest value is present in the $P_{i,j}$ entry. This means, the point $p_i$ and point $p_j$ are close among all other pairs. So, we group the two points into a single cluster. Now, simply\n",
        "mark the entry $P_{i,j}$ as done i.e., it’s a cluster.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1RNNS7tBkzf9ssag6L2SwXz0gscggs7gd'>\n",
        "\n",
        "  **3b.** Update the proximity matrix → Now, we will update the proximity matrix since points $p_2$ and $p_3$ are grouped into a single cluster.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1TJYzZEfax0sJzyr1lsZcxtIMIRyk5i9p'>\n",
        "\n",
        "3. Until we have only one cluster. The algorithm stops at this stage.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1r7fZGWSt_Afd5qg17rpeGQ7r1XQJJKmQ'>\n",
        "\n"
      ],
      "metadata": {
        "id": "np8mTmhhmQ6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dendogram**\n",
        "\n",
        "#### **Remember from previous lecture?**\n",
        "\n",
        "- **We can visually represent Agglomerative Clustering using a Dengogram**\n",
        "\n",
        "- Its a visual representation of the records and the hierarchy of clusters to which they belong (the hierarchial relationship).\n",
        "\n",
        "- It is an inverted tree that describes the order in which factors are merged (bottom-up view) or clusters are broken up (top-down view).\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1E1JrBbqiHP_SDsCcGY0tNA2yF0RhlcFi'>\n",
        "\n",
        "- With a Dendogram, we can see which clusters were formed using which lower level clusters, which can help us decide how many clusters (**K**) we want to keep."
      ],
      "metadata": {
        "id": "xV0gt0livJut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "CYjsSNlyMavo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods to Compute Distance b/w Clusters\n",
        "\n",
        "#### **Now the questions that arises is: How to compute the distance between clusters $C_i$ and $C_j$?**\n",
        "\n",
        "- We need to find a method to compute the distance between the cluster and a point p, as well as distance between clusters.\n",
        "\n",
        "- Eucledian Distance is only useful to find distance b/w two points.\n",
        "\n",
        "\n",
        "The methods to compute distances b/w a cluster and a point or b/w two clusters are :\n",
        "1. MIN\n",
        "2. MAX\n",
        "3. Group Average\n",
        "4. Distance between Centroids\n",
        "5. Ward's Distance\n",
        "6. Other methods driven by an objective.\n",
        "\n",
        "#### **We'll briefly go through them one by one:**\n",
        "\n",
        "1. **MIN** - The minimum distance b/w two closest points belonging to the two different clusters.\n",
        "\n",
        "  Min Similarity ($p_i,p_j$) where $p_i$ belongs to $C_1$ and $p_j$ belongs to $C_2$. Here, similarity is nothing but the euclidean distance or any other valid metrics. For example cosine similarity, manhattan distance, etc.\n",
        "\n",
        "  In simple terms, create a set $C_1$ where the set has all points belonging to $C_1$. Also, create a set $C_2$ where the set contains all the points belonging to $C_2$. Now, do this:\n",
        "  ```\n",
        "  Min_sim = infinity or a largest possible integer value in python.\n",
        "  For pi in C1:\n",
        "    For pj in C2:\n",
        "      Min_sim = min(Min_sim,Similarity(pi,pj)\n",
        "      #Similarity(pi,pj) can be retrieved from the similarity matrix\n",
        "  Return Min_sim\n",
        "  ```\n",
        "  \n",
        "2. **MAX** - The maximum distance b/w two farthest points belonging to the two different clusters.\n",
        "\n",
        "  **Procedure for computing MAX distance:**\n",
        "   ```\n",
        "   Max_sim = - infinity\n",
        "   For pi in C1:\n",
        "    For pj in C2:\n",
        "      Min_sim = max(Max_sim, Similarity(pi,pj)\n",
        "      \n",
        "  Return Max_sim\n",
        "  ```\n",
        "\n",
        "3. **Group Average** - The average of the distances b/w all pairs of points belonging to the two different clusters.\n",
        "\n",
        "  **Procedure for computing AVG distance:**\n",
        "  ```\n",
        "  Count_of_points = 0\n",
        "  Sum_of_similarity = 0\n",
        "  For pi in C1:\n",
        "    For pj in C2:\n",
        "      Count+=1\n",
        "      Sum_of_similarity += Similarity(pi,pj)\n",
        "\n",
        "  Return Sum_of_similarity/Count_of_points\n",
        "```\n",
        "In simple terms, we are adding all the similarity($p_i,p_j$) for all i,j and then dividing by the total number of combinations if i,j we can make.\n",
        "\n",
        "4. **Distance between Centroids**\n",
        "\n",
        "  This method is very simple.\n",
        "  - First, create a set $C_1$ where the $C_1$ contains all points p which belongs to the cluster $C_1$.\n",
        "  - Then, create a set $C_2$ where the $C_2$ contains all points which belong\n",
        "to the cluster $C_2$.\n",
        "  - Now, simply take the average of all points in the set $C_1$ and call it as\n",
        "$p_1$.\n",
        "  - Now, simply take the average of all points in the set $C_2$ and call it as\n",
        "$p_2$.\n",
        "  - Now, compute the similarity between point $p_1$ and $p_2$ and return.\n",
        "\n",
        "  **Algorithm:**\n",
        "  ```\n",
        "  mean_point_C1 = initialize this vector with zeros of size n\n",
        "  mean_point_C2 = initialize this vector with zeros of size n\n",
        "  count_C1 = 0\n",
        "  count_C2 = 0\n",
        "  For pi in C1:\n",
        "    count_C1+=1\n",
        "    For i = 1 to n:\n",
        "    mean_point_C1[i]+=pi[i]\n",
        "  mean_point_C1/=count_C1\n",
        "  \n",
        "  For pi in C2:\n",
        "    count_C2+=1\n",
        "      For i = 1 to n:\n",
        "        mean_point_C2[i]+=pi[i]\n",
        "  mean_point_C2/=count_C2\n",
        "```\n",
        "\n",
        "\n",
        "5. **Ward's Distance** - The average of the **squared distances** b/w all pairs of points belonging to the two different clusters.\n",
        "\n",
        "  - It’s similar to group average, if the distance between points is\n",
        "distance squared.\n",
        "  - Less susceptible to outliers.\n",
        "  - We are simply squaring the distance.\n",
        "  - **Biased towards globular clusters**.\n",
        "  - Can be used to initialize K-Means.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1bjp-7P0LKxnaSTfsOPip0308KZvhEz4n'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6bwospV3mQ8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Using MIN Distance:**\n",
        "\n",
        "- The important thing in Agglomerative Clustering is the **similarity**. We need some function which can give us a similarity between points. One such function is the euclidean distance. The euclidean distance will be less for points which are close and be more for points which are far away. So, this gives some sort of similarity. Since , we are relying on similarity, we can kernalize it as we did in SVM.\n",
        "\n",
        "- Let’s consider an example for MIN.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=14zbt6wycfkgrpO5AcuaXbO5i8mZN9ueg'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1d5jDgn8WQIKj27LUoWbZ0b7G0bzm_Ngx'>\n",
        "\n",
        "- We can see the similarity matrix defined above in the image. We can also see the **dendrogram** which we have studied earlier which records the sequence of operations we made while performing the Agglomerative Clustering.\n",
        "\n",
        "- The diagonal entries are always zero. Since the distance between the point $p_i$ and itself is zero.\n",
        "\n",
        "- First, we will find the minimum entry in the similarity matrix. We can\n",
        "see the entry (6,3) which has the minimum value of 0.11.\n",
        "- So, we group points 3 and 6 into a single cluster.\n",
        "- Again, we will perform the same steps as we discussed earlier.\n",
        "- After performing the procedure, we will obtain the clusters as given in\n",
        "the above image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zRDbM7XrmQ-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "jn2lu1uANPnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pros and Cons of Different Distance Methods in Hierarchical Clustering\n",
        "\n",
        "#### **Strength of MIN clustering:**\n",
        "\n",
        "- It **can handle non-elliptical shapes**. As long as there’s a separation between two clusters, it can handle very well.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DOZ8SJNmgTaGYtgGyitTWM1LEyEXTVqB' width='500'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1L0th2e3nAREesHqJzKan9x3VOU-I1nAj' width='500'>\n",
        "\n",
        "#### **Limitations of MIN:**\n",
        "\n",
        "- It’s sensitive to noise and outliers.\n",
        "\n",
        "- Since noise points are also closer to the clusters themselves, the algorithm will think that this point belongs to that cluster and will group it.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Z4w0tigKxLxSn_PkU-g---0nAsMPbqbp' width='500'>\n",
        "\n",
        "#### **Strength of MAX:**\n",
        "\n",
        "- It’s less susceptible to noise and outliers. Even Though the noise points are closer to some of the clusters, since we are considering MAX, it is ignored.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vTtphrLa-z04WvrdMgmAr1NVgM8HEEmf' width='500'>\n",
        "\n",
        "#### **Limitations of MAX:**\n",
        "\n",
        "- Tends to break large clusters.\n",
        "- Biased towards globular clusters.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1qOynhqZiVY_vg2W-TKCMSFj3KcUmyjvI' width='500'>\n",
        "\n",
        "#### **Strengths and Limitations of Group Average:**\n",
        "\n",
        "- It’s a combination of both MAX and MIN. So, both strengths and limitations are shared.\n",
        "\n",
        "- It's less susceptible to noise and outliers\n",
        "\n",
        "- However, it’s biased towards globular clusters, i.e, if there is any symmetrical shaped cluster or any shape , then the algorithm might group it. But in reality, it may not be the case.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1utyueDgj8Gwn6BLrAPHhP4AMCDhtGiWX' width='500'>\n",
        "\n",
        "#### **Strengths and Limitations of Ward’s Method:**\n",
        "\n",
        "- It’s similar to group average, if the distance between points is distance squared.\n",
        "- Less susceptible to outliers.\n",
        "- As you can see, we are simply squaring the distance.\n",
        "- It can be used to initialize K-means.\n",
        "- But, it is biased towards globular clusters.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1JDtCZPQHjy-whvnzv45CtkWP76CA012a' width='500'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euiCRxEKGuZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visual Comparison of Different Distance Methods in Hierarchical Clustering**\n",
        "\n",
        "_ We can make a comparison among different methods we have studied for measuring cluster distance in Hierarchical Clustering.\n",
        "\n",
        "- Clustering results or Dendograms can be different for different distance measures.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1BWrxkQAX0yesy6Juh_CWP9IXvKYVpzyD'>\n",
        "\n"
      ],
      "metadata": {
        "id": "nK4PZkdk0nJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "kK13haUVM2mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time and Space complexity of Agglomerative Clustering\n",
        "\n",
        "**Space complexity:**\n",
        "\n",
        "- $O(n^2)$.\n",
        "\n",
        "- This comes from the fact that we need to store the similarity matrix. If there are n points, then the shape of the similarity matrix is $n \\times n$ or there are $n \\times n$ entries. So, the space complexity is $O(n^2)$.\n",
        "\n",
        "**Time complexity:**\n",
        "\n",
        "- $O(n^3)$\n",
        "- There are at most $n$ iterations.\n",
        "- At each iteration, we group 2 clusters.\n",
        "- We also need to update the similarity matrix. → $O(n^2)$.\n",
        "- So, it’s roughly $O(n^3)$.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1fCETHrfm4R-IJRIp2aU6H3dPr2o-_WRw'>\n",
        "\n",
        "- We can optimize certain operations by using some advanced data structures. - We can reach $O(n^2*log(n))$.\n",
        "- The time complexity is quite high. **N is usually large in ML problems**. We\n",
        "may have a million records. So, $O(n^3)$ is large.\n",
        "\n",
        "As a final conclusion, an **Agglomerative Algorithm is not useful if $n$ is large**."
      ],
      "metadata": {
        "id": "XhY3VXcB0nL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "8YdPOLnS0nOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of Hierarchical Clustering\n",
        "\n",
        "- There is no mathematical objective that we are directly solving. For K-Means, we had a very clear mathematical objective. We are basically using methods like MIN, MAX, AVG rather than defining an optimization function and then optimizing it.\n",
        "\n",
        "- Methods like MIN, MAX, AVG, etc. have their own limitations.\n",
        "\n",
        " <img src='https://drive.google.com/uc?id=1SjD1IM_kWRC8SW761XWs2qfjg7kZcElO' width='500'>\n",
        "\n",
        "\n",
        "- The time complexity is $O(n^3)$ which is not feasible if n is very large.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1_wWtkooKG3KXfNojplsH9fWYeW2uByLp'>\n"
      ],
      "metadata": {
        "id": "DQpLKUSv0nQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "3r5Q6ZkT0nTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN\n",
        "\n",
        "- So far, we have studied centroid-based clustering (K-means) , Hierarchical Clustering. Now, we are going to study **Density Based Clustering**.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DCfQ_8JjwZL8LMUM8n9A6dldWrHgam-b' width='500'>\n",
        "\n",
        "- **DBSCAN : Density-Based Spatial Clustering of Applications with Noise**\n",
        "\n",
        "- As usual, we are given a bunch of points.\n",
        "- Let’s call them $D : {x_i}, i = \\text{1 to n}$, where n represents the total number of data points.\n",
        "- Our aim is to partition these $x_i$ points into dense regions and sparse regions. Typically the **dense regions become the clusters and sparse regions become the noise**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gVXzesRV0nYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What is dense and sparse in simple terms?**\n",
        "\n",
        "- **Dense :** It basically means, the points are clustered together.\n",
        "- **Sparse :** It basically means, there is some empty space in the region of where those $x_i$’s live.\n",
        "\n",
        "#### **How to mathematically quantify the above terms?**\n",
        "\n",
        "Let us look at the important terms we are going to learn now.\n",
        "1. Min point\n",
        "2. Epsilon\n",
        "3. Core point\n",
        "4. Border point\n",
        "5. Noise point"
      ],
      "metadata": {
        "id": "CJD9eZu40nbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "OlQFlN4H0ysk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinPts and Eps : Density\n",
        "\n",
        "- MinPts and Eps are the **hyperparameters** for the DBSCAN algorithm.\n",
        "\n",
        "1. **Density at a point P :**\n",
        "\n",
        "- Number of points within a hypersphere of radius Epsilon around P.\n",
        "- Hypersphere - A sphere in n-dimensional space .\n",
        "- For example if $n = 2$ , then we call it as a circle.\n",
        "\n",
        "  Let’s consider an example.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1GPSvX31MSp9Xghkl3SGLPOqbECpSXeJ-'>\n",
        "\n",
        "- Using the definition given above, we can calculate the density at a point P.\n",
        "\n",
        "- First, draw a hypersphere of radius 1 (Epsilon) with the point P as the centre.\n",
        "- Next, count the number of points which lie inside the hypersphere.\n",
        "- Include the point P also in the count.\n",
        "- The count simply represents the density at a point P.\n",
        "- Here, density is 4.\n",
        "\n",
        "\n",
        "2. **Dense region :**\n",
        "- A hypersphere of radius Epsilon that contains at least MinPts points.\n",
        "\n",
        "  Let’s take an example.\n",
        "  \n",
        "  Let’s assume MinPts is 10 and Epsilon is 1.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=18t4IZDJcASfzTre6Uejw2K5rKno8nXA6'>\n",
        "\n",
        "- We will first draw a hypersphere of radius 1 around the point P. Now, we will count how many points lie inside that hypersphere. We can see there are 20 points (including the point P). Since $20 > MinPts$, we conclude that as a\n",
        "dense region.\n",
        "\n",
        "- Let’s consider another hypersphere. We will first draw a hypersphere of radius 1 around the point P. Now, we will count how many points lie inside that hypersphere. We can see there are 6 points (including the point P). Since $6 < MinPts$, we conclude that as a sparse region.\n",
        "\n"
      ],
      "metadata": {
        "id": "7TXgf4iK0nds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "reTEcera0wFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core, Border and Noise points\n",
        "\n",
        "$D = {x_i}, i = \\text{1 to n}$.\n",
        "\n",
        "Given a point $x_i$ from the dataset **D**, we can categorize them as core point or border point or noise point.\n",
        "\n",
        "1. **Core point :**\n",
        "\n",
        "- A point P is said to be core if it has greater than or equal to MinPts points in an Epsilon radius around it.\n",
        "- In simple terms, we will first draw a hypersphere of radius Epsilon with point P as a centre. Then, we will count the number of points which lie inside that hypersphere including the point P itself. If that count is greater than or equal to MinPts, then we declare the point P as core point.\n",
        "- It always belongs to a dense region.\n",
        "\n",
        "  **Note :** Dense region : A hypersphere of radius Epsilon that contains at least MinPts points.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1q6oqDx8Dx1zLScFMH0-8MyF42YQXlVwU'>\n",
        "\n",
        "2. **Border point :**\n",
        "\n",
        "- A point P is said to be a border point, if p is not a core point. This means, P has less than MinPt points in Epsilon radius.\n",
        "- Also p lies in the neighborhood of another core point Q i.e, distance between P and Q is lesser than or equal to Epsilon.\n",
        "\n",
        " **Note :** If a point A lies in the neighborhood of another point B, then distance between them is lesser than or equal to Epsilon.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1geQBjpBoBF7H5gI4TzYcrojPW52_anIc'>\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=15GDC0Dy135T5nmjDAQ5cgHEaAF6nqlrN'>\n",
        "\n",
        "3. **Noise point :** Any point P which is neither a core point nor a border\n",
        "point is termed as a noise point.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1YgPBsChftY7gjwbuDN9-wAcUIPJimp4c'>"
      ],
      "metadata": {
        "id": "nY-ewQBr0ngt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Let’s consider a real dataset.**\n",
        "\n",
        "#### **Can you classify the Core, Border and Noise points?**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=14qo3kG3CwqH3oV340QciRLjNzgSHNKOi'>\n",
        "\n"
      ],
      "metadata": {
        "id": "xqvse41g0njU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "piiGlqzn0tMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Density edge and Density connected points\n",
        "\n",
        "1. **Density edge :**\n",
        "\n",
        "- If P and Q are two core points and if the distance between P and Q is lesser than or equal to Epsilon, then we create a graph with two vertices P and Q and connect them with an edge.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1XydbBP0k8m9Bc7NjzgHaxQCSf-2_9fHq'>\n",
        "\n",
        "2. **Density connected points :**\n",
        "\n",
        "- P and Q must be a core point. This is slightly a different definition.\n",
        "- P and Q are said to be density connected points if there exists a path between P and Q and in that path there are also other points which have density edges.\n",
        "- In simple terms, $p_1 - p$ has density edge , $p_2 - p_1$ has a density edge, $p_3 - p_2$ has a density edge $q - p_3$ has a density edge and $p - q$ has a density edge.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-3XF9rP-tjv0fdfh-Ijxlaj2sOAvXy3w'>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHf83RyhzUrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "F6nH6jG-zUt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN Algorithm\n",
        "\n",
        "1. For each point $x_i$ belonging to a dataset **D**, label them as Core point, Border point, Noise point.\n",
        "\n",
        "2. For checking the above conditions and for labelling, we can use range query.\n",
        "\n",
        "3. Range Query basically returns a set of points which are Epsilon distance away from a point P. For example, a range query (P, Epsilon) will return a set of points S which is Epsilon distance away from P. It’s implemented using kd-trees. We learnt about kd-trees in the K-NN module.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1TDuEoWvPb7SBpiEHnr9JEqBxi_y_vdTa'>\n",
        "\n",
        "4. Remove all Noise points i.e, sparse regions.\n",
        "\n",
        "5. For each core point P that is not assigned to a cluster (Initially, none\n",
        "of the core points is assigned to a cluster):\n",
        "  - Create a new cluster with P\n",
        "  - Add all the points that are densely connected with P.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1CHMTX8JsUgVjsRlYMLDlWRyH4OJcFFha'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vZrPEnjoIzaRXBTZVRJlsrTAo8vKF58c' width='400'>\n",
        "\n",
        "6. Assign each border point to the nearest core point’s cluster.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1WQJg5zdWKE89_P3ON-b1_wIXmaUX7tuG'>\n",
        "\n",
        "**The core computation we need to perform is a range query**. With kd-tree, the range query operation can be done in $O(log(n))$.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_y_PmUzgoOz7FwCYgEdkElepCOw6_gaC'>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ij77coopzUw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "n9VjwjwzzUz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Picking Hyperparameters : MinPts and Eps\n",
        "\n",
        "#### **Now the big question is: How do we decide the value of MinPts and Eps?**\n",
        "\n",
        "1. **MinPts :**\n",
        "\n",
        "- A rule of thumb is to have $MinPts >= d+1$ where $d$ is the dimensionality of our dataset. Typically, MinPts can be set to $2*d$.\n",
        "- If your dataset is noisy, then have a large value for MinPts. It’s because, if we have a large value for MinPts, then the chance of the noise point being classified as a core point is very less.\n",
        "\n",
        "  **Note :** MinPts is a main criterion deciding whether a point is a core point or not.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1MdTpZy6aDdJA4EDhIbhAii_UBHfpgGz9'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1rCJY_n4Fc9Wjbn4YEkIHyZ9jmY52U8hh'>\n",
        "\n",
        "- It is often chosen by a domain expert.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=118haI-e6ct3lgAGyCo9FZ4k7bemsw66E' width='500'>\n",
        "\n",
        "\n",
        "2. **Epsilon :**\n",
        "\n",
        "- Let’s say we are given a value for MinPts as $m$.\n",
        "\n",
        "- Given a $x_i$, first compute the distance between $x_i$ and the nearest neighbor of $x_i$.\n",
        "\n",
        "- For each point $x_i$, compute the distance and get the list of distances.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1HxO3FDYhk8fVwr9kMQCd2PCtzZFFjpdD'>\n",
        "\n",
        "- Now, sort the distances in increasing order.\n",
        "\n",
        "- Simply plot the distances and the point index.\n",
        "\n",
        "- We can now use the elbow method which is discussed earlier in the course and then find the inflexion point. It is a point where the curve changes its direction. Then corresponding to it, we choose the epsilon.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ngAIlQ7Kj5NRFhKSb57E7kx_ajvx7dvf'>\n",
        "\n",
        "**Let's see this with an example:**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1E5NhYuMePIjxhRCIL-ystbaoPQc8FCM5'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qhw6NE3GzU4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "MiAbnMPCzU7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages and Limitations of DBSCAN\n",
        "\n",
        "#### **When does DBSCAN work well?**\n",
        "\n",
        "- It’s resistant to noise\n",
        "- Can handle clusters of different shapes and sizes.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1dix72PseJKbLWkcx-i7_5ABsXvDYJ8SM'>\n",
        "\n",
        "- It doesn’t require one to specify the number of clusters a priori.\n",
        "- It requires only two parameters: MinPts and Epsilon.\n",
        "- It is designed for use with databases as it’s created by the database\n",
        "community.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10VZH7HjRRioi_xLquyc1173__WF2g-Nv'>\n",
        "\n",
        "#### **Limitations :**\n",
        "\n",
        "- Even a small change in the hyperparameters, we can get a completely different type of clusters. So, it’s quite sensitive to the choice of hyperparameters.\n",
        "- Varying densities.\n",
        "- High-dimensional data. It’s due to the curse of dimensionality.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1cq13UI9Cw9oDVDcG-NWXQrG-coFvMaGc'>\n",
        "\n",
        "**Example of different clustering results generated by DBSCAN with different values of MinPts :**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1w3YXaf5kwG3DyZcFmfe9Mr6K643tdnjy'>\n",
        "\n",
        "As we can see from the above image, since there are varying densities in the dataset, **with a slight change in the hyperparameters, we get different results**.\n",
        "\n",
        "- DBSCAN is not entirely deterministic.\n",
        "\n",
        "- If the data and the scale are not well understood; choosing a meaningful distance threshold Epsilon would be difficult.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-otcnGSYzU-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "CNb73oA9zVAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Notes\n",
        "\n",
        "- Agglomerative Clustering and DBSCAN Algorithms are very simple to understand.\n",
        "\n",
        "- We'll do a Case Study pertaining to Hierarchical Clustering in the next lecture.\n",
        "\n",
        "- There are a lot of terms and concepts involved. But with some revision, you'll be able to grasp and retain these concepts."
      ],
      "metadata": {
        "id": "D29sdKBfJ2Ti"
      }
    }
  ]
}