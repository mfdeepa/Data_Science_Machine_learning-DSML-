{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b><font style=\"Times New Roman\">Lecture | DSML Advanced : SVM</h1></b></font>\n",
        "\n",
        "Lecture Link : https://www.scaler.com/meetings/i/dsml-advanced-svm-2/archive\n",
        "\n",
        "<h2><b>Content</h2></b>\n",
        "\n",
        "1. <h3><b>SVM</b></h3><nobr>\n",
        "\n",
        "> - <b>Brief history</b> ( 1:39:15 )\n",
        "> -  <b>General idea behind SVM  </b>\n",
        "> -  <b>Geometric intuition </b> ( 1:42:20 )\n",
        "> > - Linear SVM's\n",
        "> > - Hard Margin SVM's ( 2:06:00 )\n",
        "> > - Soft Margin SVM's ( 2:18:00 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr4InsQBBJ-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Brief History of SVM</h2>**\n",
        "<ul>\n",
        "<li>Inititally invented around 1960-70's by USSR but was brought to public notice during 1990's</li>\n",
        "<li>One of the most challenging algorithm in terms of mathematics</li>\n",
        "<li>Less frequently used nowadays because of more advanced models like boosting and bagging.</li>\n",
        "</ul><br>\n",
        "\n",
        "<img width=800px src='https://drive.google.com/uc?id=1jPlmhzPK7ldIUW2-CIE6BxJ3enjT0FuO'/>\n"
      ],
      "metadata": {
        "id": "kuowaP-9mPJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>General idea behind SVM</h2>**\n",
        "<p>In any classification problem, the main goal is to find a line/plane that can best seperate the two classes.<br> For example : </p>\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1JxBKNkQa6K2hLi8b09ye75EZ063PogLb)\n",
        "\n",
        "<p>In the above graph you can see that depending on which line we choose, a test point ( X ) might be assigned a different label\n",
        "\n",
        "Observation:\n",
        "<ol>\n",
        "<li>There can be 1000 thousand such lines which can seperate these classes.</li>\n",
        "<li>Drawing a line between classes is not enough, choosing a particular line may affect the performance during testing.</li>\n",
        "</ol>\n",
        "\n",
        "The main idea behind SVM algoithm is to find the line or plane that can best seperate the given classes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JJtMOU5LtNHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Geometric intution behind SVM</h2>**\n",
        "\n",
        "For better understanding, let's consider a data set containg two classes as shown by positive(+) and negative(-) and the classes are linearly seperable.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1xU6TygVDQErHVIzU7hxJ0O_Sx54YWYHd)\n",
        "\n",
        "Our main goal is two find a line ( <b>called as hyperplane</b> ) with the largest margin possible, where margin can be defined as the distance between the hyperplane  and the observations closest to the hyperplane ( <b>called as support vectors</b> ).\n",
        "\n",
        "Now to understand maths behind it, let us consider the following variables:\n",
        "\n",
        "\n",
        "> &pi; : seperating hyperplane with maximum margin possible <br>\n",
        "> &pi;+ :  line passing through the positive data point closest to &pi; <br>\n",
        "> &pi;&nbsp;- : line passing through the negative data point closest to &pi;\n",
        "\n",
        "<b>Here &pi;+ and &pi;- are called as support vectors.</b>\n",
        "\n",
        "All these can be interpreted in the form of some equations where margin can be simply calculated as distance between two parallel lines.\n",
        "<table>\n",
        "    <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1lFpXDwSeBZ1haEmLePCpf-q3S-yE2JIx'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1RspiVk65K543NhaxLIdLSM6Wdr2b9LLx'/> </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "So the whole problem narrows down to maximizing this value such that all the data points are as far as possible from their respective support vectors :\n",
        ">  <font size=5 color=\"green\"> max<sub>(w,b)</sub> &nbsp; $\\frac{2}{||w||}$</font>&nbsp;&nbsp; or  equivalent to &nbsp;&nbsp; <font size=5 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "NfLYwoEtVAg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: What if lines are of form Wx + b = K where k is some constant.\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- It does not matters as final function in this form will be <font color=\"green\"> max<sub>(w,b)</sub> &nbsp; $\\frac{2K}{||w||}$</font> and our maximization only depends on W .<br> <br>\n",
        "<img width=70% src='https://drive.google.com/uc?id=1QLs3YEbAosO4wvZNTdDpsmOxmeVdlbKX'/>\n"
      ],
      "metadata": {
        "id": "s7n0sQSqBQwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li>Above we studied are called as Linear - SVM's which is the simplest version of SVM with simple constraints</li>\n",
        "<li>SVM's are basically margin maximizing classifiers. </li>\n",
        "<li> This was the example of Hard Margin SVM's as our data is linearly seprable and SVM is very rigid in classification.</li><br>\n",
        "</ul>\n",
        "<img width=70% src='https://drive.google.com/uc?id=1D0eSCz9S3dsfhn2XEhd2K0zoMUQmzQt0'/>\n"
      ],
      "metadata": {
        "id": "krbxUrvIDli4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Linear SVM with a Hard Margin</b></h3>\n",
        "<p>If we strictly impose that all instances must be off the street (margin) and on the correct side, this is called hard margin classification.\n",
        "There are two main issues with hard margin classification:-</p>\n",
        "<ul>\n",
        "<li>First, it only works if the data is linearly separable.</li>\n",
        "<li>Second, it is sensitive to outliers.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "4-uvELBgK4LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: When would Linear SVM with Hard Margin fails ?\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- When data is almost linearly seperable, it contains some data points on opposite side of their correct space.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1XIIrNFbjwEavFnDdrzGvraiCrE9Hh1jw'/>\n"
      ],
      "metadata": {
        "id": "Wxw5AaHMFNzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>SVM with a Soft Margin</h3></b>\n",
        "<p>To avoid the issues of Hard Margin SVM, use a more flexible model. It is possible that data is almost linearly seperable which means few data points are either on the wrong side of margin or inside the margin.<br>\n",
        "The objective is to find a good balance between keeping the margin as large as possible and limiting the margin violations (i.e., points that end up in the middle of the margin or even on the wrong side).\n",
        "This is called soft margin classification.</p>\n",
        "\n",
        "<img width=40% src='https://drive.google.com/uc?id=1uAgWdg20k8LVIBHsnUR9hFweBs2FeNP7'/>\n",
        "\n",
        "\n",
        "For this to understand, let's define &zeta; as error metric to penalize incorrectly placed data points.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1) ζ = 0 for correctly placed data point\n",
        "2) ζ > 0 and ζ < 1 which represents incorrectly placed data point but will be still be classified correctly.\n",
        "3) ζ > 1 which represents incorrectly placed and  misclassified data point\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SzttsYKsLWzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with the introduction of &zeta; the final function will change to <br>\n",
        ">  <font size=5 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$ + $\\frac{C}{N}$$\\sum_{i=1}^N \\zeta_i$</font>\n",
        "\n",
        "Where C is hyperparamter, and we need to minimize mean of all incorrectly place points error to get best classification.\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1ZDVI2XzRPjBlB9PG6bbE9q1RkV9kKpii'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1VzX7daT4w5JUViQiJS2V7H6X5Y00p20c'/> </td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "y2uhaGl3EWTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above equation holds true for both positive as well as negative points which is explained in calculations below:\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1BpQLbfnLK-MHtEVkGkh1H_o5g8V7r63f'/>\n"
      ],
      "metadata": {
        "id": "YqD3THQbEWhx"
      }
    }
  ]
}