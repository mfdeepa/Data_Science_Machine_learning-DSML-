{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture link : https://www.scaler.com/meetings/i/dsml-advanced-ensemble-boosting-2/archive\n"
      ],
      "metadata": {
        "id": "J-m9loaZ-Sea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "1. Recap (2:00-14:06)\n",
        "2. Random Forest - introduction(14:06 - 26:00 )\n",
        "3. Hyper parameter tuning (35:00 - 1:05:06)\n",
        "4. SKlearn library (1:55:30 - 2:16:00)\n",
        "5. Extra trees (2:16:00 - 2:29:22)\n",
        "6. Summary (2:29:22 - 2:31:01)\n"
      ],
      "metadata": {
        "id": "qSzi8hdp-Yzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "EeLFp0dX-k9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In previous lecture we saw what bagging is, lets take a quick recap of what it is and how it works\n",
        "\n",
        "* Bagging is nothing but **Bootstrapped sampling + Aggregation**\n",
        "\n",
        "**How it works ?**\n",
        "\n",
        "Let us assume a train data set $D$ with $n$ data points i.e $D_n$\n",
        "* Now, we **sample $m$ data points with replacement** to get $D'_m$\n",
        "* We do the sampling again for $m$ points to get $D'_2$, **Repeat the same for $k$ times** and we get $D'_k$\n",
        "* Now, we **train $k$ different models**($M_1,M_2,....M_k$) basing on the $k$ datasets obtained , there models are called **Base Learners**.\n",
        "* After training we **cross validate each model** **with remaining $n-m$ data points**\n",
        "* Now, we do **Aggregation**\n",
        " * We use majority vote for Classification\n",
        " * We use Mean/Median for Regression"
      ],
      "metadata": {
        "id": "MDqlYCAE-oDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1lrPYRlQsJT3TJvVJG-tVJvjr4CVMyDJS'>\n"
      ],
      "metadata": {
        "id": "Ft7bt-Ka_Xgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's now see what a Random forest is**"
      ],
      "metadata": {
        "id": "o092oMT-_8vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "wa_oCZKkAHdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As you can understand from the name a Random forest is nothiing but a group of trees.\n",
        "* It's basically a bagging based system with Decision tree\n",
        " * Base learners of Random Forest are Decision tree\n",
        "* So, as a whole, Random forest is Applying Randomisation on the data and ensemble of Decision trees and later bagging these."
      ],
      "metadata": {
        "id": "NoA26OFsAK26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see Random forest is built"
      ],
      "metadata": {
        "id": "1Ck_DNs8MH0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let us assume we have a train data set ($D_n$) with $n$ rows and and $d$ columns\n",
        "* And now we apply **row samplinig and column sampling** on the data to get new data set ($D^1_m$) with $m$ rows and $d'$ columns\n",
        "  * where $m<n$ and $d'<d$\n",
        "\n",
        "**But why do we apply colummn sampling?**\n",
        "\n",
        "*  Because we want all our **Base learners** to be different, choosing different set of features may help them to be different, for achieving this we do column sampling as well.\n",
        "* Adding column sampling makes the models to be little more different\n",
        "\n",
        "We do the same again and again to get $k$ different data sets\n",
        "* So we get $D^1_m$,$D^2_m$....$D^k_m$ now the models $M_1$,$M_2$,...$M_k$ will be different  \n",
        "* Now we aggregate the models\n"
      ],
      "metadata": {
        "id": "Ymxoj9AnMW_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1M0jzMtHBUGU41ldGm95NbO6FdxCQ24Ds'>\n"
      ],
      "metadata": {
        "id": "Vww_YBw6DSnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do you see a problem here?**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHHDSdFPPj8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have $k$ different train data sets ($D^1_m$,$D^2_m$....$D^k_m$) to train $k$ different models ( $M_1$,$M_2$,...$M_k$) respectively\n",
        "\n",
        "**Where is the cross validation data?**"
      ],
      "metadata": {
        "id": "yHbCEoQdQYOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The remaining $n-m$ rows can be used as cross validation data\n",
        "* for $D^i_{n-m} = D_n - D^i_m$ ,is Set Difference\n",
        " * This is reffered to as Out Of Bag(OOB)\n",
        "\n",
        "* So, we train the models on ($D^1_m$,$D^2_m$....$D^k_m$)these data sets and cross validate them on ($D^1_{n-m}$,$D^2_{n-m}$,....$D^k_{n-m}$) this data which is OOB sample\n",
        "* If some data points are not present in any of the train data sets, they will be in every cross validation data."
      ],
      "metadata": {
        "id": "PhT1mo-kQx2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1ctmM4gTDOOA-DvYz69TVPUWrk_8bGs73'>\n"
      ],
      "metadata": {
        "id": "PmEPDlOMD8Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Will Random Forest always out perform Decision tree?**\n",
        "\n",
        "* Yes, in most cases of the cases they should ut perform\n",
        "\n",
        "**When will the performance of both Random Forest and Decision Tree be similar?**\n",
        "\n",
        "* This may happen when the data is very simple, that a simple Decisison tree by using simple rules can get a very good performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B5xzgjIiVBq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=18r8gsNEwlVWxe1LrcxsBmHdt1T_7aFHr'>\n"
      ],
      "metadata": {
        "id": "H1i7ODbJEsYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In random forest The Base learners don't need seperate cross validation data set because the remaining data after sampling(OOB) are used for cross validation\n",
        " * i.e from $D_n$, $D^i_m$ is sampled and $D^i_{m-n}$ is used for cross validation\n",
        "\n",
        "* But the random forest as a whole has the cross validation data and test data to tune the Hyper Parameters of random Forest\n",
        "\n"
      ],
      "metadata": {
        "id": "u8n2ksucXUjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1TAWUGCqO3pHkMeAEihu8yrctu8mIl1Ld'>\n"
      ],
      "metadata": {
        "id": "9caPO-SNERyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**What happens if $k$ increases?**\n",
        "\n",
        "* The Decision tree base learners in random forest are Deep Decision Trees (non-shallow).\n",
        "* As they are deep, the models overfit slightly on a sub sample of data, as the base learners are made of $m$ rows and $d'$ columns only.\n",
        "* Now, we perfrom aggregation\\averaging on these slightly overfit models, on a sub-sample of data, which have high variance adn low bias\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1TZFIobjeaDkNolHkyM4B7yPhuY1CJ2Je'>\n",
        "\n",
        "* But it is statistically proved that aggregating reduces the variance without much increase in the bias.\n",
        "* In statistical Machine learning the error of a model can be represented as\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1EepwXmzSOEOnwxQlYgUu1tW1ni399agY'>\n",
        "\n",
        "\n",
        " * **Error = Bias$^2$ + Varinace + Irreducable error.**\n",
        " * Here we saw due to aggregation, the variance decreases without trading-off to bias, due to which the overall error of the model decreases.\n",
        "* Therefore the error of the random forest reduces.\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ZhxBCswDiPvPoOO8gX-F2PEuDczWjkqF'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3RkPyNuQYUqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, as we saw each step of random forest\n",
        "\n",
        "* Let's see the core working of a random forest\n",
        "  1. It uses Bagging,  which is row sampling, and column sampling\n",
        "  2. It takes Base learners  which are slightly overfit i.e having high variance and low bias\n",
        "  3. It reduces this variance by aggregation without much increase in its bias.  "
      ],
      "metadata": {
        "id": "uMdBhwwELEZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1HayWwymSPwsHn1MWBRYcvcJ37_tg8mYy'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Yom_bcXNOELi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can we use Non Decision Tree models in Random forest?**\n",
        "\n",
        "* Yes we can you use slightly overfit models i.e having high variance models.\n",
        " * For example: We can use KNN with small $k$\n",
        " * We can use Logistic Regression with higher order features + low value of λ"
      ],
      "metadata": {
        "id": "SOy6CO7dOQ5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-Parameter tuning of Random Forest"
      ],
      "metadata": {
        "id": "_PmfUSzfRnjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see  various hyper parameters of Random Forest\n",
        "* Note that we consider the whole dataset $D$ not only the $D_{train}$ which is split into train data set for base learners"
      ],
      "metadata": {
        "id": "lcLlC7TlR9rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the hyper parameters for random forest?\n",
        ""
      ],
      "metadata": {
        "id": "H3LQLxOVPRJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Number of Trees($k$)"
      ],
      "metadata": {
        "id": "3h5Ik9k7XvCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens if number of base learners ($k$) increases?**"
      ],
      "metadata": {
        "id": "N3tt1qwOSpnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As $k$ increases, we Aggregate more number of base learners\n",
        "* We already saw that, Aggregation decreases the varaince without any significant increase in the bias.\n",
        "* Hence, we can say that we avoid overfitting."
      ],
      "metadata": {
        "id": "KRkWAjf6S21z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1ZJB00f8_E465mwVr6h_FEZ1KwDMTkM9d'>\n"
      ],
      "metadata": {
        "id": "_UqGnNpnUWXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if my base learners underfit instead being overfit?**\n",
        "\n",
        "* If the base learners underfit, the varaince is  already low\n",
        "* Hence the aggregation which should reduce the variance doesn't work as expected."
      ],
      "metadata": {
        "id": "GuaeN8eYWjy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Row sample size ($m$)\n",
        "\n",
        "\n",
        "\n",
        "* In some libraries $\\frac{m}{n}$ is also considered where this value becomes maximum i.e 1 ,when $m=n$"
      ],
      "metadata": {
        "id": "SepeRaGuX626"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens when $m=n$ ? ($d=d'$)**\n",
        "* All  Base learners overfit on the whole data and they be similar.\n",
        "* So, as the $\\frac{m}{n}$ increases the over-fitting of the model  increases"
      ],
      "metadata": {
        "id": "B-pxhXdzZR99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Number of columns Sampled ($d'$)\n",
        "* Consider $m=n$\n"
      ],
      "metadata": {
        "id": "NkTXdSTXaUT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As $\\frac{d'}{d}$ increases, then all the features are being  used by each of the tree.\n",
        "* This implies the overfitting increases i.e the variance increases  \n"
      ],
      "metadata": {
        "id": "GM6JKchMbWIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Depth of Base learners\n",
        "* This is optional, some libraries let us use this when we dont want to use the Out Of Bag (OOB) data  fro cross validation\n",
        "* As depth increases, the overfitting increases\n",
        " * When the ratios $\\frac{m}{n}$ and $\\frac{d'}{d}$ are very less, that is when the base leraners are seeing very few columns and rows , and when the number of trees increases ($k$) the overfitting chances are low.  "
      ],
      "metadata": {
        "id": "Cdq_NI3CcdgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1fxH8dxv8mvAJM8SucB54Q4kBpudeBnzS'>\n"
      ],
      "metadata": {
        "id": "PuQMNluDeYkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1kvcAmHsp6HMVhi3r0s_OqU-waLTKqwOk'>\n"
      ],
      "metadata": {
        "id": "ekttkpwXeiJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions"
      ],
      "metadata": {
        "id": "R9Tgp3M-XzOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if the models have high bias in an ensemble?**\n",
        "\n",
        "* We know Random forest is an ensemble of high varaince models, that is slightly overgfitting models\n",
        "* We see the Gradient Boosted Decision Tree which applies boosting, which deals with high bias or underfitting models"
      ],
      "metadata": {
        "id": "zOhqhojCer9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1nAOfG-cO5tV71OaEZWI4t9CgUBTLPA71'>\n"
      ],
      "metadata": {
        "id": "EFm1D-fN200h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if ensemble a deep decision tree and a simple linear classifier?**\n",
        "\n",
        "* This doesn't work as expected beacuse the random forest expects highy variance models, as it does aggregation at the end which reduces the variancce, but do not effect the bias.Due to which if a model of high bias is there, the high bias remains the same which we should avoid."
      ],
      "metadata": {
        "id": "7pJdBUrJ0whw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1lAQIpDbsaoAvUrcVlRduRdq8xpE4GfCb'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X9YYaDuR3Sdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What happens when $m$ is very close to $n$ ?**\n",
        "* When $m$ is close to $n$ the base learners overfit on the whole train data set. Due to which Aggregation doesn't work in reducing the variance\n",
        "* So, when $\\frac{m}{n}$ increases, Random forest overfits.\n",
        "* But whwn $\\frac{m}{n}$ decreases the base leraners overfit, due to which we require more number of trees for to reduce the variance."
      ],
      "metadata": {
        "id": "5spvniYD3cfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1XABn5maJst8ugowX4fX4FJwYUbw1jYnV'>\n"
      ],
      "metadata": {
        "id": "lxzf8jZN7d7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rjwbJPNX7lrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So here, Should we take weighted average of the base models in aggregation ?**"
      ],
      "metadata": {
        "id": "Wu2VJd2w8RXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this is not required beacuse we are taking Base learners with equal weight\n",
        "\n"
      ],
      "metadata": {
        "id": "uUxbvUqL8ZyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Q5mVqPrdxHw6i88VFZrRAop9pvPz1o86'>\n",
        "\n"
      ],
      "metadata": {
        "id": "kILCvBL59Lit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We saw that aggregation is the key in Random forest ,\n",
        "\n",
        "* But we might have two different types of Base learners in the random forest\n",
        " * With high variance and low bias or\n",
        " * With medium varaince and medium bias\n",
        "* Here as wwe do aggregation both the models become\n",
        " * with low variance and low bias and\n",
        " * with low variance and medium bias respectively\n",
        "\n",
        "So it's better to have high variance and low bias base learners.\n"
      ],
      "metadata": {
        "id": "ozMID6AKBz1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=10eSeKSrmQsNc3jLmqhxLDFxJ7bsETR6X'>"
      ],
      "metadata": {
        "id": "YAp3FGUzDAmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a random forest"
      ],
      "metadata": {
        "id": "OylVTQ9hDI9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* With present day improvements we can easily run a Random forest with a billion data with 10 million sample size and with thousand base learners\n",
        "* These base learners can be trivially parallelised.\n",
        "* As each models is **trained independently** we can even take these models to distributed computing\n",
        "* In a distributed computing system each processor is given with different model's data set , the system can work on multiple models parallely, if the processor is a 8 core processor the datasets of 8 different trees can be worked on a system.\n",
        "* So, we are parallelisig the Random forest on multiple cores\n",
        "due to which the process becomes fast.\n",
        "* The time complexity here is O($k$ * max_depth of tree)"
      ],
      "metadata": {
        "id": "jNnr5Y_2X9U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1fGTLvjgW-ynmIWXM9DdOc8lJMHF8w6nN'>"
      ],
      "metadata": {
        "id": "MBDvVIRns0FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1dWqfOVxoz_bGUUojZcmCfNjdX8ZXzQ7O'>\n",
        "\n"
      ],
      "metadata": {
        "id": "fe3xbVUstNbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1FWex1MrUl0QMt6ROJM9sKSvdnP3fPWfD'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Pzt4iB8xtWw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OOB Score"
      ],
      "metadata": {
        "id": "-tdfnVgjtfkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we already studied that after sampling the data for each model we have some data remaning which is the **Out Of  Bag** data\n",
        "* This OOB data ($D'_{n-m}$) is not used as cross validation data, it is used to find the performace of the model on data, this is called the **OOB score** of the data\n",
        "* When the trained data performance is  more than the OOB data perfromance, we can conclude that the model is over-fitting, which we want upto some extend\n",
        "* In the same way we can say that the model is overfitting if the trained data error is less than OOB data error"
      ],
      "metadata": {
        "id": "K0QdOIFsu4dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1AbtzsIlvUBalGafi5KT9QkuoKtg-75aI'>\n"
      ],
      "metadata": {
        "id": "AjhzsbzAxC0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn library (terms and usage):"
      ],
      "metadata": {
        "id": "NxrVw-TXxNY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OOB score\n",
        "\n",
        "* By default the OOB score is set to false, we can keep it true to get the OOB scores of each model\n",
        "* Which can be compared by the perfromance of the trained data know the extent of overfitting in the base learner model"
      ],
      "metadata": {
        "id": "YUirda0q6MZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1137U9OLnPMlpHmikjd3g44dkke1sseoz'>\n",
        "\n"
      ],
      "metadata": {
        "id": "KbaKxNKIEYyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### max depth\n",
        "\n",
        "* This is used to decide the depth of the each Decision tree in the model\n",
        "* As we already discussed, as depth increases the model overfits more due to which we may need more base learners for model to work"
      ],
      "metadata": {
        "id": "M_JQqKQpzKFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1nkMitJtyQBTO5m2fKoZdXYXFKfz4bXvF'>\n"
      ],
      "metadata": {
        "id": "XYsjUv5vEKb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### min_samples_split\n",
        "* This is used to set the minimum number of data points required in a node to split further\n",
        "\n",
        "**What happens if we  increase the min_sample_split?**\n",
        "* If we increase the min_sample_split value the splitting stops if there are less number of datapoints than the set value, which controls the overfitting and may also result in underfitting and shallow trees."
      ],
      "metadata": {
        "id": "njnlsonM6q_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Ds2k9yrwkjUC7IPwr0f8druPAIhJZGew'>\n",
        "\n"
      ],
      "metadata": {
        "id": "bQMyGBS-EkM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### min_samples_leaf\n",
        "* This gives the minimum number of data points required in the leaf node.\n",
        "* If the value of min_samples_leaf is increased the base learners become shallower\n"
      ],
      "metadata": {
        "id": "wJM8KvhH8Usc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1xwig8jfexndg9yz-i8rzOAA8ww0vKS23'>\n"
      ],
      "metadata": {
        "id": "sEd_PswNEu9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n_jobs\n",
        "\n",
        "* This gives the number of jobs that can be run in paralle.\n",
        "* This does the multi core parallelisation\n",
        "* If the value of n_jobs is set to 4, four cores of your processor will be used\n",
        "* We can use all the cores of the processor by setting this value to -1.\n"
      ],
      "metadata": {
        "id": "kY6UY-zH9amm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1K012L47rFSUnCSF9v7ZX_sjgVTK-ojJ8'>\n"
      ],
      "metadata": {
        "id": "8kA9wLPhE484"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### max_samples\n",
        "\n",
        "* This is used to set the number of samples to draw from the dataset to train each of the base learners\n",
        "* This is nothing but the row sampling which we studied earlier and represented as $m$\n",
        "* We can give both int and float values, if we give a int value it takes as the number of samples to be considered and float is for percentage of the rows to be considered   "
      ],
      "metadata": {
        "id": "C4e3t0bQ-asP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1aF4Sn4hUjbhoUAmuPPe2ThGhfwilxUz0'>\n"
      ],
      "metadata": {
        "id": "5-ffrHTpFDIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ccp_alpha\n",
        "\n",
        "* ccp is cost complexity pruning\n",
        "* This is basicaly used for pruning the base learners\n",
        "* We can control the overfitting and undefitting of the base learners using the value $α$, this is almost similar tp $λ$ whixh we used in linear and logistic regression\n",
        "* so the idea here is to minimise the loss associated with the decision tree and $α$ times the number of terminal nodes (leaves) which controls overfitting\n",
        " * min(loss + $α$ * number of leaves in the tree)\n",
        "* As the depth of tree increases we know the loss decreases, where the number of leaf nodes in increases, this trade-off between the loss and number of leaves can be controlled using $α$ like regularisation."
      ],
      "metadata": {
        "id": "A2hfjQz__RNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1OS3oMsWLqUUzuKEO--J_5dlyDNknjMRl'>\n"
      ],
      "metadata": {
        "id": "h0Sub3dMFMWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1jMy3PfJkOZWRDvLHd2D6fh8FQEcgSpjy'>\n"
      ],
      "metadata": {
        "id": "WpG9Or-GFWQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1JsoH0fNs82BYPKOStZDT4uN2bygFaQbE'>\n"
      ],
      "metadata": {
        "id": "DTdCbMutFl--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Trees or Extremely Randomised Trees"
      ],
      "metadata": {
        "id": "0uxMpWPiGAUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's compare these with Random Forest\n",
        "\n",
        "**What do we do in random forest?**\n",
        "* We do random row sampling and column sampling and then we do aggregation, in which this randomisation and aggregation play a key role in reducing the variace keeping the bias similar, further avoiding overfitting\n",
        "\n",
        "**Randomisation is a great, useful and very powerful strategy for Regularization**\n",
        ""
      ],
      "metadata": {
        "id": "t83SEKWqGKK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1DSnJpwOzFCuyQdF5KsUDktJPYZ_szZdv'>\n"
      ],
      "metadata": {
        "id": "imZY7_qTfMoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**what do extra trees do?**\n",
        "\n",
        "In extra tree does random row and column sampling and aggregation just like decision trees but it also randomly picks the threshold (τ) to split for numerical features."
      ],
      "metadata": {
        "id": "RwdjbC8ET8mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In decsision trees we saw using the features values as threshold we calculate information gain and then choose the thresold of the split , basing on the values of information gain\n",
        "\n",
        "* In an extra tree we choose this threshold (τ) also randomly ,adding one more randomisation on top of random forest,\n",
        " * say there are $m$ rows in a feature we randomly select few rows and set the threshold basing on their IG values\n",
        "*This is useful when the $m$ is large, but these require more base laerners as the trees are not perfect\n",
        "* Hence these are not widely used"
      ],
      "metadata": {
        "id": "fpwzb_KgUXJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1hLXJNCgbUqp3emjQineZvLf8W6dKfwnV'>\n"
      ],
      "metadata": {
        "id": "FqtL0oSgfXxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1GncvBjGn0p0ZdEYl49SaZxNSgZMIgY1Y'>\n",
        "\n"
      ],
      "metadata": {
        "id": "Ur5-An-Lf0EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n"
      ],
      "metadata": {
        "id": "h8RJAMXGcntv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Random forest works well when  Dimensionality is not too large\n",
        "* As it is trivially parallelizable, it works very well even when the $n$ is large\n",
        "* The hyper parameters in Random forest are\n",
        " * $k$ : the number of trees\n",
        " * $\\frac{m}{n}$ ratio and\n",
        "\n",
        " * $\\frac{d'}{d}$ ratio\n"
      ],
      "metadata": {
        "id": "Y2TUDmYPcxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Tm-8-_Fnp_V0fpCaqe7CzXjMFRfJ7QY5'>\n"
      ],
      "metadata": {
        "id": "XcOOLaMnf_Cn"
      }
    }
  ]
}