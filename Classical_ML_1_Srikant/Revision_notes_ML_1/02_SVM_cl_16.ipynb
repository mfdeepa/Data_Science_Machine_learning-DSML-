{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b><font style=\"Times New Roman\">Lecture | DSML Advanced : Naive Bayes</h1></b></font>\n",
        "\n",
        "Lecture Link : https://www.scaler.com/meetings/i/dsml-advanced-naive-bayes/archive\n",
        "\n",
        "<h2><b>Content</h2></b>\n",
        "\n",
        "1. <h3><b>Kernal SVM's</b></h3>\n",
        "\n",
        "> - <b>Quick Recap of Linear SVM's</b> ( 00:04:15 )\n",
        "> - <b>Loss minimization</b> ( 00:10:35 )\n",
        "> > - Effect of C on model performance ( 00:14:42 )\n",
        "> > - Hinge loss ( 00:35:00 )\n",
        "> > - Comparison with Logistic regression ( 00:56:24 )\n",
        "\n",
        "> - <b>Dual form of SVM</b>\n",
        "> > - Primal form ( 01:09:21 )\n",
        "> > - Dual form ( 01:11:10 )\n",
        "> > - Primal-Dual equivalence ( 01:25:32 )\n",
        "\n",
        "\n",
        "> - <b>Kernel SVM</b> ( 01:42:00 )\n",
        "> > - Kernel function and different types of Kernel ( 01:58:15 )\n",
        "> > - Kernel trick ( 02:04:45 )\n",
        "> > - Logistic regression Vs Kernel SVM ( 02:20:46 )\n"
      ],
      "metadata": {
        "id": "HvcJcBJv7Qma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Quick Recap of Linear SVM's</b></h2>\n",
        "<p>Linear SVM is a classifier that is used for linearly separable data, which implies that if a dataset can be categorised into two classes using a single straight line, it is called linearly separable data.</p><br>\n",
        "<p>They are of two types: </p>\n",
        "<ol>\n",
        "<li> <b>Hard Margin SVM</b> : When data is completely linearly seperable by maximum margin hyperplane, objective function of it is defined as:\n",
        "\n",
        "\n",
        ">  <font size=5 color=\"green\"> max<sub>(w,b)</sub> &nbsp; $\\frac{2}{||w||}$</font>&nbsp;&nbsp; or  equivalent to &nbsp;&nbsp; <font size=5 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$</font><br><br>\n",
        "with a constraint that <font color='green' size=4> y<sub>i</sub> (w<sup>t</sup> x<sub>i</sub> + b) >= 1</font> &#8704; i = 0..n\n",
        "</l1>\n",
        "<li>\n",
        "<b>Soft Margin SVM</b>: When data is almost linearly seperable but there are few instances which are on wrong side of their respective support vectors.This is based on premise that allow SVM to make a fixed amount of mistakes while keeping the margin as large as feasible to ensure that other points are correctly identified.The objective function is defined as:\n",
        "\n",
        "\n",
        "> <font size=5 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$ + $\\frac{C}{N}$$\\sum_{i=1}^N \\zeta_i$</font><br><br>\n",
        "with a constraint that <font color='green' size=4> y<sub>i</sub> (w<sup>t</sup> x<sub>i</sub> + b) >= 1 - &zeta;<sub>i</sub> </font> &#8704; i = 0..n\n",
        "\n",
        "Where C is hyperparamter,&zeta; is error value for incorrectly placed points and we need to minimize mean of all incorrectly place points error to get best classification.\n",
        "\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "<img src='https://drive.google.com/uc?id=1t-F-kEUcs5_WPTY-DLdmBkKNAgEfuzVx'/>\n",
        "\n"
      ],
      "metadata": {
        "id": "h4tNBiIm7PTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Loss Minimisation</b></h2>\n",
        "\n",
        "If you closely look at the equation:&nbsp;&nbsp;<font size=4 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$ + $\\frac{C}{N}$$\\sum_{i=1}^N \\zeta_i$</font><br><br>\n",
        "The first part &nbsp;&nbsp;<font size=4 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$</font> is nothing but <font size=4>$\\frac{L2}{2}$ </font> norm and <font color=\"green\">$\\frac{C}{N}$$\\sum_{i=1}^N \\zeta_i$</font> is Hinge loss.<br>\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=15g-jYm6tuM7Bq9c6BPjhR8Dhdi19zlxu'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1qGn6RnzM9FNjzF842t8otRBmxV6GmYrn'/> </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "<h4> <b>What is the effect of C paramter on model performance ?</b></h4>\n",
        "\n",
        "\n",
        "```\n",
        "1) Higher the C => High penalty for Hinge loss => Model try to minimize loss as much as possible => Model Overfit\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "2) Lower the C => More focus maximizing margin => Traning performance decrease => Model underfit\n",
        "```\n",
        "<table>\n",
        "    <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=10AgFcLSVMYBI7UJoc5AZQ-OkdjNH9xLu'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1XrLzjqIwelN4s9Al46ehZCgN9VWZnOl9'/> </td>\n",
        "     <td> <img src='https://drive.google.com/uc?id=1oj3RW5lXdXMlV4aSwLhCtbrw3g7aIM6P'/> </td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "ZWVpeKMzEEhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  How the constraints are getting included in the  optimization. We are not explicitly checking that in the minimization. We once used lagrange multipliers earlier. Could we do that here as well to convert it to unconstrained ?\n",
        "---\n",
        "Yes, we would use the lagrange multipliers to include the constraints.\n",
        "\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1pqSgIq19JPP-9UAQ39H8t9ZCxJZcd03X'/>"
      ],
      "metadata": {
        "id": "1hdgFFChDuHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Hinge Loss</b></h2>\n",
        "<ul>\n",
        "<li>Hinge loss is one of the types of Loss function, mainly used for maximum margin classification models.</li>\n",
        "\n",
        "<li>The hinge loss is a form of cost function that penalises not only misclassified instances but also properly classified samples that are within a certain margin of the decision boundary.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "Hinge loss incorporates a margin or distance from the classification boundary into the loss calculation. Even if new observations are classified correctly, they can incur a penalty if the margin from the decision boundary is not large enough.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1zMC1g8oa8DdZ9RhPPktFa2IQ_B8b_nUQ'/>\n",
        "\n",
        "The x-axis shows the distance from any particular instance's border, while the y-axis reflects the magnitude of the loss.\n",
        "\n",
        "Observations:\n",
        "<ol>\n",
        "<li>If the distance of the point from boundary is greater than equal to 1, then this represents correctly classified points and are on correct side of their respective hyperplanes, distance from the boundary is greater than or at 1, our loss size is 0.<br>\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1tTElSpWOmVwSrzvXW7RIfuwuVUSRxcWn'/>\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>If the distance from the boundary is 0 (meaning that the instance is literally on the boundary), then we incur a loss size of 1.<br>\n",
        "<img width=60% src='https://drive.google.com/uc?id=1NN8dYHWWo2JsesBS6i3noopYvoAPEhqC'/>\n",
        "</li>\n",
        "\n",
        "<li>Correctly classified points will have a small (or none) loss size, while incorrectly classified instances will have a high loss size.\n",
        "If the distance of point is negative from the boundary then it incurs a high hinge loss. This essentially means that we are on the wrong side of the boundary, and that the instance will be classified incorrectly.<br>\n",
        "<img width=60% src='https://drive.google.com/uc?id=1RWHQ6Nr4Uxi1I3L6ucTfTYjs7sqLBT9A'/><br>\n",
        "Acc to the above graph, observations that falls on the proper side of the decision boundary (hyperplane) but are inside the margin incur a cost between 0 and 1. <br>All observations that fall on the incorrect side of the hyperplane will suffer a loss bigger than one that grows linearly.\n",
        "</li>\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "5IzOVQHRLIcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any two particular classes say { +1,-1 }, let's say <font color='green' size=4>z<sub>i</sub> = y<sub>i</sub> (w<sup>t</sup> x<sub>i</sub> + b)</font><br>\n",
        "Hinge loss can be defined like this:\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1fvaq6xftQ347uXaq1oyEmQXGSxNpOfAN'/><br>\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1AwJAQsYohyId06gifFOBriOy8pAE1_FF'/><br>\n",
        "\n",
        "and if we were to draw it<br>\n",
        "\n",
        "<img width=60% src='https://drive.google.com/uc?id=1UY1HSMNJhwn4fgJTXK2F2Qy0KblzHS1Z'/><br>\n"
      ],
      "metadata": {
        "id": "7AZ_JDGg4ZCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Comparison with Logistic Regression</h3></b>\n",
        "\n",
        "If we were to calculate loss function of Logistic regression considering two classes <b>{+1,-1}</b> instead of <b>{0,1}</b>, then loss function would look like this:\n",
        "\n",
        "\n",
        "> Loss :<font color='green' size=4> Log ( 1 + exp( -y<sub>i</sub> (w<sup>t</sup> x<sub>i</sub> + b)) )</font>  called as log-loss function\n",
        "\n",
        "<b> If we draw both Hinge-loss and log-loss together, we will observe that they are almost similar except for the curvature around margin region points. That's why we observe that both SVM and Logistic regression models gives similar results.</b>\n",
        "\n",
        "\n",
        "The following figure show Hinge-loss, Log-loss, 0-1 Loss together.\n",
        "( 0-1 loss is nothing but 0 for correct classification and 1 for incorrect )\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=1O9vH9k8kyHsOmvFYENd3cIWevdn-w5fs'/><br>\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=1BvfVu46jeGJiWGYgIag-sN_BDQs7Bu-A'/><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "unnn8R_9-WcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Dual form of SVM</b></h2>\n",
        "\n",
        "<p> The minimization function we studied above &nbsp;<font size=4 color=\"green\"> min<sub>(w,b)</sub> &nbsp; $\\frac{||w||}{2}$ + $\\frac{C}{N}$$\\sum_{i=1}^N \\zeta_i$</font><br><br> is also called as primal form of SVM.<br>\n",
        "<ul>\n",
        "<li>In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints.</li>\n",
        "\n",
        "<li>To find the local maxima and minima of function subjected to constraint, we generally use Lagrange multipliers.Lagrange problem is typically solved using dual form. </li>\n",
        "</ul>\n",
        "\n",
        "> ## Question:  Is there a relation between primal and dual forms? why do we need same hyperparameter C in both equations ?\n",
        "> <p>The duality principle says that the optimization can be viewed from 2 different perspectives. The 1st one is the primal form which is minimization problem and other one is dual problem which is maximization problem. So every paramter present in primal form will be used in dual form also, called as <b>Primal-Dual equivalance</b>. </p>\n",
        "\n",
        "The dual form can be represented as shown below such that for x<sub>i</sub> there exist a <font size=4>&alpha;<sub>i</sub> </font> and x<sub>i</sub> only exist in the form of x<sub>i</sub><sup>T</sup> x<sub>j</sub>:\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=1Rsrdvu8c6uVV3TcC8pohefdszApITzA9'/><br>\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=1KRKiIaEzbV8qwZuiUtUMhZ4jOhcO7fuk'/><br>\n"
      ],
      "metadata": {
        "id": "MmftVBMpIS1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  How would it correctly classify a data point based on various support vectors calculated for runtime ?\n",
        "---\n",
        "So as <font size=4>&alpha;<sub>i</sub></font> is zero for all non support vectors, we only need varibales for all our support vectors.\n",
        "<table>\n",
        " <tr>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1t1dZiXWMsFWT-hQGXXVoMYiHmAqrSoWe'/> </td>\n",
        "    <td> <img src='https://drive.google.com/uc?id=1gNo9OLdj6xBUk0FhJ-LgppxSZvi4Ze-R'/> </td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "GeNjveY9Lg6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  In real time do we retrain the margins only with new data points .. or we revise the whole model and margins for retraining with new points? Also, can SVM be used for regression ?\n",
        "---\n",
        "- Generally, We train retrain model with new data but some hacks are there which will be discussed later in lectures.\n",
        "- Yes, SVM can be used for regression, we call them SVR's ( support vector regressors).\n"
      ],
      "metadata": {
        "id": "LS98qOgpOve6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Kernel SVM</b></h2>\n",
        "<ul>\n",
        "<li>In the dual form we studied, <font size=4>x<sub>i</sub> </font>only exist in the form of <font size=4>x<sub>i</sub><sup>T</sup> x<sub>j</sub></font> which is nothing but dot product of <font size=4>x<sub>i</sub> </font> and <font size=4>x<sub>j</sub> </font> and which represents <b>similary</b> between them. </li>\n",
        "<li>So we can replace <font size=4>x<sub>i</sub><sup>T</sup> x<sub>j</sub></font> can be replaced with any other function denoting similarity between two, called as <b>kernal function, K( <font size=4>x<sub>i</sub> , x<sub>j</sub></font>).</b></li>\n",
        "</ul>\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=1OuQ-84w7ZEJe2VUA9I5f83iogFGonO74'/><br>\n",
        "\n",
        "<img width=90% src='https://drive.google.com/uc?id=16--iJ7DEHn-nszBQeXAlLjzA6ywG1QD9'/><br>\n"
      ],
      "metadata": {
        "id": "0IzQ8odHcual"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li> <b>Kernel function could be quadratic, polynomial, trignometric, RBF( if we don't know the shape of data), etc..</b></li><br>\n",
        "<img width=80% src='https://drive.google.com/uc?id=1nka17jj9QpXXxvaurmOY1Cm0IP7nB8YU'/><br>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "ouYG6YoXf_lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Kernel Trick</b></h2>\n",
        "<p>A Kernel Trick is a simple way for projecting non-linear data into a higher-dimensional space to make it simpler to classify the data where it may be split linearly by a plane.<br>\n",
        "<b>In Kernalization, data of dimension ( D ) is projected to dimension ( D' ) implicitly such that D' >> D .</b>\n",
        "</p><br>\n",
        "<img width=80% src='https://drive.google.com/uc?id=15FIeSFhFegnuRN-wCXexhag17OZ8W1oo'/>"
      ],
      "metadata": {
        "id": "di-3wRWftfvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  Are we actually finding a hyperplane in D' space, or it is just about finding a similarity score ?\n",
        "---\n",
        "We are not actually finding hyperplane in D' but solving kernel function is equivalent to finding hyperplane in D'. By the function, we can assume that the time complexity of this function is still O(n<sup>2</sup>) ."
      ],
      "metadata": {
        "id": "bvilfebSvJWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***:  So does that mean we can use kernelization with other models also? I mean what is the diff bw using polynomial features and regularization Vs kernelization?\n",
        "---\n",
        "Yes, we can use kernalisation with other models also. <br>\n",
        "If we were to comapre <b>Logistic regression + polynomial feature + regularisation</b> with  <b> SVM + polynomial kernel + regularisation </b>, there is not much difference.But there are other kernels too, the most important out of them is <b>RBF</b>.<br>\n",
        "RBF transform the data from dimension D to infinitley many possible dimension.\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=14C08jSFL2OC6DeX57DOI8VOIjbmqLaA5'/>\n"
      ],
      "metadata": {
        "id": "lTOTgpxb10Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: Give a mathematical example of kernalization.\n",
        "---\n",
        "Below is a example of Quadratic kernel converting a 2D data to 6D data implicitly. But remember computation still is in 2D.<br>\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1baG5sswPFzo-WMZZFyQKtxpTnb_EQ_YN'/>\n",
        "\n",
        "<img width=80% src='https://drive.google.com/uc?id=1KhBvjLDaS7ogtday2HqEP2lItiDe4LR6'/>\n",
        "*italicised text*"
      ],
      "metadata": {
        "id": "lBuZeaaS30IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h3>Logistic regression Vs Kernel SVM</h3></b>\n",
        "<p>In case of Logistic regression (or also Primal form of SVM ), transformation will be explicitly which means computation will be in 6D if we were to use quadratic features but that is not in Kernel or Dual form of SVM in which computaion is implicit.</p><br>\n",
        "\n",
        "> This is the importance of kernel trick that we are computing non linear models in D' dimension without explicitly converting them in D'</li><br>\n",
        "\n",
        "\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=125SaJHJYtn4Pz6kqZ6O5ah0VS3_WBvak'/>\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1S7o_rOjdCp-iSr5zCcfZAMm4IhRb9q3r'/>\n"
      ],
      "metadata": {
        "id": "DHtiJ_uL7nnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: If kernels are so powerful then why they are not used widely.\n",
        "---\n",
        "- Designing special purpose kernels are too difficult and complex\n",
        "- Time complexity to train and deploy is large comparsion to various boosting algorithms like GBDT/RF etc. It depends on our number of support vectors which is difficult to control.\n",
        "- RBF kernel is very similar to KNN.\n",
        "- If we observe, we are just replacing feature engineering in GBDT/Random forest with kernel design in SVM.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1H2EfbN8Kb9fBpaCvCX1fE6pbV-faGETv'/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j1odvn2bAGak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Question***: If we increase accuracy, we might be prone to overfitting. How can we know that the model is generalizing well?\n",
        "---\n",
        "- In the graph shown below, x-axis represents hyperparamters ( say &lambda; of logistic regression ) and y-axis represents error ( which is inversly proportional to accuracy ).\n",
        "- When &lambda; = 0, represents overfit model where train error is very low or zero and cross-validation error will be very high.\n",
        "- As we increase &lambda; along x-axis, training error will increase and cross-validation error decrease.\n",
        "- There will be a &lambda;' where difference between training and cross-validation error will be very small called as trade-off model point.\n",
        "- We can use this &lambda;' as our selected hyperparamter to train a generalized model.\n",
        "\n",
        "<img width=70% src='https://drive.google.com/uc?id=1kDLJ_kd3o7YO7Wrar0CErGvGwOhvjyGF'/>\n"
      ],
      "metadata": {
        "id": "yCKEyizuDlFN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKzo61b_-oQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}